{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#unity-ml-agents-toolkit","title":"Unity ML-Agents Toolkit","text":"<p>(latest release) (all releases)</p> <p>The Unity Machine Learning Agents Toolkit (ML-Agents) is an open-source project that enables games and simulations to serve as environments for training intelligent agents. We provide implementations (based on PyTorch) of state-of-the-art algorithms to enable game developers and hobbyists to easily train intelligent agents for 2D, 3D and VR/AR games. Researchers can also use the provided simple-to-use Python API to train Agents using reinforcement learning, imitation learning, neuroevolution, or any other methods. These trained agents can be used for multiple purposes, including controlling NPC behavior (in a variety of settings such as multi-agent and adversarial), automated testing of game builds and evaluating different game design decisions pre-release. The ML-Agents Toolkit is mutually beneficial for both game developers and AI researchers as it provides a central platform where advances in AI can be evaluated on Unity\u2019s rich environments and then made accessible to the wider research and game developer communities.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>17+ example Unity environments</li> <li>Support for multiple environment configurations and training scenarios</li> <li>Flexible Unity SDK that can be integrated into your game or custom Unity scene</li> <li>Support for training single-agent, multi-agent cooperative, and multi-agent   competitive scenarios via several Deep Reinforcement Learning algorithms (PPO, SAC, MA-POCA, self-play).</li> <li>Support for learning from demonstrations through two Imitation Learning algorithms (BC and GAIL).</li> <li>Quickly and easily add your own custom training algorithm and/or components.</li> <li>Easily definable Curriculum Learning scenarios for complex tasks</li> <li>Train robust agents using environment randomization</li> <li>Flexible agent control with On Demand Decision Making</li> <li>Train using multiple concurrent Unity environment instances</li> <li>Utilizes the Sentis to   provide native cross-platform support</li> <li>Unity environment control from Python</li> <li>Wrap Unity learning environments as a gym environment</li> <li>Wrap Unity learning environments as a PettingZoo environment</li> </ul> <p>See our ML-Agents Overview page for detailed descriptions of all these features. Or go straight to our web docs.</p>"},{"location":"#releases-documentation","title":"Releases &amp; Documentation","text":"<p>Our latest, stable release is <code>Release 21</code>. Click here to get started with the latest release of ML-Agents.</p> <p>You can also check out our new web docs!</p> <p>The table below lists all our releases, including our <code>main</code> branch which is under active development and may be unstable. A few helpful guidelines: - The Versioning page overviews how we manage our GitHub   releases and the versioning process for each of the ML-Agents components. - The Releases page   contains details of the changes between releases. - The Migration page contains details on how to upgrade   from earlier releases of the ML-Agents Toolkit. - The Documentation links in the table below include installation and usage   instructions specific to each release. Remember to always use the   documentation that corresponds to the release version you're using. - The <code>com.unity.ml-agents</code> package is verified   for Unity 2020.1 and later. Verified packages releases are numbered 1.0.x.</p> Version Release Date Source Documentation Download Python Package Unity Package develop (unstable) -- source docs download -- -- Release 21 October 9, 2023 source docs download 1.0.0 3.0.0 <p>If you are a researcher interested in a discussion of Unity as an AI platform, see a pre-print of our reference paper on Unity and the ML-Agents Toolkit.</p> <p>If you use Unity or the ML-Agents Toolkit to conduct research, we ask that you cite the following paper as a reference:</p> <pre><code>@article{juliani2020,\n  title={Unity: A general platform for intelligent agents},\n  author={Juliani, Arthur and Berges, Vincent-Pierre and Teng, Ervin and Cohen, Andrew and Harper, Jonathan and Elion, Chris and Goy, Chris and Gao, Yuan and Henry, Hunter and Mattar, Marwan and Lange, Danny},\n  journal={arXiv preprint arXiv:1809.02627},\n  url={https://arxiv.org/pdf/1809.02627.pdf},\n  year={2020}\n}\n</code></pre> <p>Additionally, if you use the MA-POCA trainer in your research, we ask that you cite the following paper as a reference:</p> <pre><code>@article{cohen2022,\n  title={On the Use and Misuse of Absorbing States in Multi-agent Reinforcement Learning},\n  author={Cohen, Andrew and Teng, Ervin and Berges, Vincent-Pierre and Dong, Ruo-Ping and Henry, Hunter and Mattar, Marwan and Zook, Alexander and Ganguly, Sujoy},\n  journal={RL in Games Workshop AAAI 2022},\n  url={http://aaai-rlg.mlanctot.info/papers/AAAI22-RLG_paper_32.pdf},\n  year={2022}\n}\n</code></pre>"},{"location":"#additional-resources","title":"Additional Resources","text":"<p>We have a Unity Learn course, ML-Agents: Hummingbirds, that provides a gentle introduction to Unity and the ML-Agents Toolkit.</p> <p>We've also partnered with CodeMonkeyUnity to create a series of tutorial videos on how to implement and use the ML-Agents Toolkit.</p> <p>We have also published a series of blog posts that are relevant for ML-Agents:</p> <ul> <li>(July 12, 2021)   ML-Agents plays Dodgeball</li> <li>(May 5, 2021)   ML-Agents v2.0 release: Now supports training complex cooperative behaviors</li> <li>(December 28, 2020)   Happy holidays from the Unity ML-Agents team!</li> <li>(November 20, 2020)   How Eidos-Montr\u00e9al created Grid Sensors to improve observations for training agents</li> <li>(November 11, 2020)   2020 AI@Unity interns shoutout</li> <li>(May 12, 2020)   Announcing ML-Agents Unity Package v1.0!</li> <li>(February 28, 2020)   Training intelligent adversaries using self-play with ML-Agents</li> <li>(November 11, 2019)   Training your agents 7 times faster with ML-Agents</li> <li>(October 21, 2019)   The AI@Unity interns help shape the world</li> <li>(April 15, 2019)   Unity ML-Agents Toolkit v0.8: Faster training on real games</li> <li>(March 1, 2019)   Unity ML-Agents Toolkit v0.7: A leap towards cross-platform inference</li> <li>(December 17, 2018)   ML-Agents Toolkit v0.6: Improved usability of Brains and Imitation Learning</li> <li>(October 2, 2018)   Puppo, The Corgi: Cuteness Overload with the Unity ML-Agents Toolkit</li> <li>(September 11, 2018)   ML-Agents Toolkit v0.5, new resources for AI researchers available now</li> <li>(June 26, 2018)   Solving sparse-reward tasks with Curiosity</li> <li>(June 19, 2018)   Unity ML-Agents Toolkit v0.4 and Udacity Deep Reinforcement Learning Nanodegree</li> <li>(May 24, 2018)   Imitation Learning in Unity: The Workflow</li> <li>(March 15, 2018)   ML-Agents Toolkit v0.3 Beta released: Imitation Learning, feedback-driven features, and more</li> <li>(December 11, 2017)   Using Machine Learning Agents in a real game: a beginner\u2019s guide</li> <li>(December 8, 2017)   Introducing ML-Agents Toolkit v0.2: Curriculum Learning, new environments, and more</li> <li>(September 19, 2017)   Introducing: Unity Machine Learning Agents Toolkit</li> <li>Overviewing reinforcement learning concepts   (multi-armed bandit   and   Q-learning)</li> </ul>"},{"location":"#more-from-unity","title":"More from Unity","text":"<ul> <li>Unity Sentis</li> <li>Introductin Unity Muse and Sentis</li> </ul>"},{"location":"#community-and-feedback","title":"Community and Feedback","text":"<p>The ML-Agents Toolkit is an open-source project and we encourage and welcome contributions. If you wish to contribute, be sure to review our contribution guidelines and code of conduct.</p> <p>For problems with the installation and setup of the ML-Agents Toolkit, or discussions about how to best setup or train your agents, please create a new thread on the Unity ML-Agents forum and make sure to include as much detail as possible. If you run into any other problems using the ML-Agents Toolkit or have a specific feature request, please submit a GitHub issue.</p> <p>Please tell us which samples you would like to see shipped with the ML-Agents Unity package by replying to this forum thread.</p> <p>Your opinion matters a great deal to us. Only by hearing your thoughts on the Unity ML-Agents Toolkit can we continue to improve and grow. Please take a few minutes to let us know about it.</p> <p>For any other questions or feedback, connect directly with the ML-Agents team at ml-agents@unity3d.com.</p>"},{"location":"#privacy","title":"Privacy","text":"<p>In order to improve the developer experience for Unity ML-Agents Toolkit, we have added in-editor analytics. Please refer to \"Information that is passively collected by Unity\" in the Unity Privacy Policy.</p>"},{"location":"API-Reference/","title":"API Reference","text":"<p>Our developer-facing C# classes have been documented to be compatible with Doxygen for auto-generating HTML documentation.</p> <p>To generate the API reference, download Doxygen and run the following command within the <code>docs/</code> directory:</p> <pre><code>doxygen dox-ml-agents.conf\n</code></pre> <p><code>dox-ml-agents.conf</code> is a Doxygen configuration file for the ML-Agents Toolkit that includes the classes that have been properly formatted. The generated HTML files will be placed in the <code>html/</code> subdirectory. Open <code>index.html</code> within that subdirectory to navigate to the API reference home. Note that <code>html/</code> is already included in the repository's <code>.gitignore</code> file.</p> <p>In the near future, we aim to expand our documentation to include the Python classes.</p>"},{"location":"Background-Machine-Learning/","title":"Background: Machine Learning","text":"<p>Given that a number of users of the ML-Agents Toolkit might not have a formal machine learning background, this page provides an overview to facilitate the understanding of the ML-Agents Toolkit. However, we will not attempt to provide a thorough treatment of machine learning as there are fantastic resources online.</p> <p>Machine learning, a branch of artificial intelligence, focuses on learning patterns from data. The three main classes of machine learning algorithms include: unsupervised learning, supervised learning and reinforcement learning. Each class of algorithm learns from a different type of data. The following paragraphs provide an overview for each of these classes of machine learning, as well as introductory examples.</p>"},{"location":"Background-Machine-Learning/#unsupervised-learning","title":"Unsupervised Learning","text":"<p>The goal of unsupervised learning is to group or cluster similar items in a data set. For example, consider the players of a game. We may want to group the players depending on how engaged they are with the game. This would enable us to target different groups (e.g. for highly-engaged players we might invite them to be beta testers for new features, while for unengaged players we might email them helpful tutorials). Say that we wish to split our players into two groups. We would first define basic attributes of the players, such as the number of hours played, total money spent on in-app purchases and number of levels completed. We can then feed this data set (three attributes for every player) to an unsupervised learning algorithm where we specify the number of groups to be two. The algorithm would then split the data set of players into two groups where the players within each group would be similar to each other. Given the attributes we used to describe each player, in this case, the output would be a split of all the players into two groups, where one group would semantically represent the engaged players and the second group would semantically represent the unengaged players.</p> <p>With unsupervised learning, we did not provide specific examples of which players are considered engaged and which are considered unengaged. We just defined the appropriate attributes and relied on the algorithm to uncover the two groups on its own. This type of data set is typically called an unlabeled data set as it is lacking these direct labels. Consequently, unsupervised learning can be helpful in situations where these labels can be expensive or hard to produce. In the next paragraph, we overview supervised learning algorithms which accept input labels in addition to attributes.</p>"},{"location":"Background-Machine-Learning/#supervised-learning","title":"Supervised Learning","text":"<p>In supervised learning, we do not want to just group similar items but directly learn a mapping from each item to the group (or class) that it belongs to. Returning to our earlier example of clustering players, let's say we now wish to predict which of our players are about to churn (that is stop playing the game for the next 30 days). We can look into our historical records and create a data set that contains attributes of our players in addition to a label indicating whether they have churned or not. Note that the player attributes we use for this churn prediction task may be different from the ones we used for our earlier clustering task. We can then feed this data set (attributes and label for each player) into a supervised learning algorithm which would learn a mapping from the player attributes to a label indicating whether that player will churn or not. The intuition is that the supervised learning algorithm will learn which values of these attributes typically correspond to players who have churned and not churned (for example, it may learn that players who spend very little and play for very short periods will most likely churn). Now given this learned model, we can provide it the attributes of a new player (one that recently started playing the game) and it would output a predicted label for that player. This prediction is the algorithms expectation of whether the player will churn or not. We can now use these predictions to target the players who are expected to churn and entice them to continue playing the game.</p> <p>As you may have noticed, for both supervised and unsupervised learning, there are two tasks that need to be performed: attribute selection and model selection. Attribute selection (also called feature selection) pertains to selecting how we wish to represent the entity of interest, in this case, the player. Model selection, on the other hand, pertains to selecting the algorithm (and its parameters) that perform the task well. Both of these tasks are active areas of machine learning research and, in practice, require several iterations to achieve good performance.</p> <p>We now switch to reinforcement learning, the third class of machine learning algorithms, and arguably the one most relevant for the ML-Agents Toolkit.</p>"},{"location":"Background-Machine-Learning/#reinforcement-learning","title":"Reinforcement Learning","text":"<p>Reinforcement learning can be viewed as a form of learning for sequential decision making that is commonly associated with controlling robots (but is, in fact, much more general). Consider an autonomous firefighting robot that is tasked with navigating into an area, finding the fire and neutralizing it. At any given moment, the robot perceives the environment through its sensors (e.g. camera, heat, touch), processes this information and produces an action (e.g. move to the left, rotate the water hose, turn on the water). In other words, it is continuously making decisions about how to interact in this environment given its view of the world (i.e. sensors input) and objective (i.e. neutralizing the fire). Teaching a robot to be a successful firefighting machine is precisely what reinforcement learning is designed to do.</p> <p>More specifically, the goal of reinforcement learning is to learn a policy, which is essentially a mapping from observations to actions. An observation is what the robot can measure from its environment (in this case, all its sensory inputs) and an action, in its most raw form, is a change to the configuration of the robot (e.g. position of its base, position of its water hose and whether the hose is on or off).</p> <p>The last remaining piece of the reinforcement learning task is the reward signal. The robot is trained to learn a policy that maximizes its overall rewards. When training a robot to be a mean firefighting machine, we provide it with rewards (positive and negative) indicating how well it is doing on completing the task. Note that the robot does not know how to put out fires before it is trained. It learns the objective because it receives a large positive reward when it puts out the fire and a small negative reward for every passing second. The fact that rewards are sparse (i.e. may not be provided at every step, but only when a robot arrives at a success or failure situation), is a defining characteristic of reinforcement learning and precisely why learning good policies can be difficult (and/or time-consuming) for complex environments.</p> <p>Learning a policy usually requires many trials and iterative policy updates. More specifically, the robot is placed in several fire situations and over time learns an optimal policy which allows it to put out fires more effectively. Obviously, we cannot expect to train a robot repeatedly in the real world, particularly when fires are involved. This is precisely why the use of Unity as a simulator serves as the perfect training grounds for learning such behaviors. While our discussion of reinforcement learning has centered around robots, there are strong parallels between robots and characters in a game. In fact, in many ways, one can view a non-playable character (NPC) as a virtual robot, with its own observations about the environment, its own set of actions and a specific objective. Thus it is natural to explore how we can train behaviors within Unity using reinforcement learning. This is precisely what the ML-Agents Toolkit offers. The video linked below includes a reinforcement learning demo showcasing training character behaviors using the ML-Agents Toolkit.</p> <p> </p> <p>Similar to both unsupervised and supervised learning, reinforcement learning also involves two tasks: attribute selection and model selection. Attribute selection is defining the set of observations for the robot that best help it complete its objective, while model selection is defining the form of the policy (mapping from observations to actions) and its parameters. In practice, training behaviors is an iterative process that may require changing the attribute and model choices.</p>"},{"location":"Background-Machine-Learning/#training-and-inference","title":"Training and Inference","text":"<p>One common aspect of all three branches of machine learning is that they all involve a training phase and an inference phase. While the details of the training and inference phases are different for each of the three, at a high-level, the training phase involves building a model using the provided data, while the inference phase involves applying this model to new, previously unseen, data. More specifically:</p> <ul> <li>For our unsupervised learning example, the training phase learns the optimal   two clusters based on the data describing existing players, while the   inference phase assigns a new player to one of these two clusters.</li> <li>For our supervised learning example, the training phase learns the mapping   from player attributes to player label (whether they churned or not), and the   inference phase predicts whether a new player will churn or not based on that   learned mapping.</li> <li>For our reinforcement learning example, the training phase learns the optimal   policy through guided trials, and in the inference phase, the agent observes   and takes actions in the wild using its learned policy.</li> </ul> <p>To briefly summarize: all three classes of algorithms involve training and inference phases in addition to attribute and model selections. What ultimately separates them is the type of data available to learn from. In unsupervised learning our data set was a collection of attributes, in supervised learning our data set was a collection of attribute-label pairs, and, lastly, in reinforcement learning our data set was a collection of observation-action-reward tuples.</p>"},{"location":"Background-Machine-Learning/#deep-learning","title":"Deep Learning","text":"<p>Deep learning is a family of algorithms that can be used to address any of the problems introduced above. More specifically, they can be used to solve both attribute and model selection tasks. Deep learning has gained popularity in recent years due to its outstanding performance on several challenging machine learning tasks. One example is AlphaGo, a computer Go program, that leverages deep learning, that was able to beat Lee Sedol (a Go world champion).</p> <p>A key characteristic of deep learning algorithms is their ability to learn very complex functions from large amounts of training data. This makes them a natural choice for reinforcement learning tasks when a large amount of data can be generated, say through the use of a simulator or engine such as Unity. By generating hundreds of thousands of simulations of the environment within Unity, we can learn policies for very complex environments (a complex environment is one where the number of observations an agent perceives and the number of actions they can take are large). Many of the algorithms we provide in ML-Agents use some form of deep learning, built on top of the open-source library, PyTorch.</p>"},{"location":"Background-PyTorch/","title":"Background: PyTorch","text":"<p>As discussed in our machine learning background page, many of the algorithms we provide in the ML-Agents Toolkit leverage some form of deep learning. More specifically, our implementations are built on top of the open-source library PyTorch. In this page we provide a brief overview of PyTorch and TensorBoard that we leverage within the ML-Agents Toolkit.</p>"},{"location":"Background-PyTorch/#pytorch","title":"PyTorch","text":"<p>PyTorch is an open source library for performing computations using data flow graphs, the underlying representation of deep learning models. It facilitates training and inference on CPUs and GPUs in a desktop, server, or mobile device. Within the ML-Agents Toolkit, when you train the behavior of an agent, the output is a model (.onnx) file that you can then associate with an Agent. Unless you implement a new algorithm, the use of PyTorch is mostly abstracted away and behind the scenes.</p>"},{"location":"Background-PyTorch/#tensorboard","title":"TensorBoard","text":"<p>One component of training models with PyTorch is setting the values of certain model attributes (called hyperparameters). Finding the right values of these hyperparameters can require a few iterations. Consequently, we leverage a visualization tool called TensorBoard. It allows the visualization of certain agent attributes (e.g. reward) throughout training which can be helpful in both building intuitions for the different hyperparameters and setting the optimal values for your Unity environment. We provide more details on setting the hyperparameters in the Training ML-Agents page. If you are unfamiliar with TensorBoard we recommend our guide on using TensorBoard with ML-Agents or this tutorial.</p>"},{"location":"Background-Unity/","title":"Background: Unity","text":"<p>If you are not familiar with the Unity Engine, we highly recommend the Unity Manual and Tutorials page. The Roll-a-ball tutorial is a fantastic resource to learn all the basic concepts of Unity to get started with the ML-Agents Toolkit:</p> <ul> <li>Editor</li> <li>Scene</li> <li>GameObject</li> <li>Rigidbody</li> <li>Camera</li> <li>Scripting</li> <li>Physics</li> <li>Ordering of event functions   (e.g. FixedUpdate, Update)</li> <li>Prefabs</li> </ul>"},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or   advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic   address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at ml-agents@unity3d.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct/</p>"},{"location":"CONTRIBUTING/","title":"How to Contribute to ML-Agents","text":""},{"location":"CONTRIBUTING/#1fork-the-repository","title":"1.Fork the repository","text":"<p>Fork the ML-Agents repository by clicking on the \"Fork\" button in the top right corner of the GitHub page. This creates a copy of the repository under your GitHub account.</p>"},{"location":"CONTRIBUTING/#2-set-up-your-development-environment","title":"2. Set up your development environment","text":"<p>Clone the forked repository to your local machine using Git. Install the necessary dependencies and follow the instructions provided in the project's documentation to set up your development environment properly.</p>"},{"location":"CONTRIBUTING/#3-choose-an-issue-or-feature","title":"3. Choose an issue or feature","text":"<p>Browse the project's issue tracker or discussions to find an open issue or feature that you would like to contribute to. Read the guidelines and comments associated with the issue to understand the requirements and constraints.</p>"},{"location":"CONTRIBUTING/#4-make-your-changes","title":"4. Make your changes","text":"<p>Create a new branch for your changes based on the main branch of the ML-Agents repository. Implement your code changes or add new features as necessary. Ensure that your code follows the project's coding style and conventions.</p> <ul> <li>Example: Let's say you want to add support for a new type of reward function in the ML-Agents framework. You can create a new branch named feature/reward-function to implement this feature.</li> </ul>"},{"location":"CONTRIBUTING/#5-test-your-changes","title":"5. Test your changes","text":"<p>Run the appropriate tests to ensure your changes work as intended. If necessary, add new tests to cover your code and verify that it doesn't introduce regressions.</p> <ul> <li>Example: For the reward function feature, you would write tests to check different scenarios and expected outcomes of the new reward function.</li> </ul>"},{"location":"CONTRIBUTING/#6-submit-a-pull-request","title":"6. Submit a pull request","text":"<p>Push your branch to your forked repository and submit a pull request (PR) to the ML-Agents main repository. Provide a clear and concise description of your changes, explaining the problem you solved or the feature you added.</p> <ul> <li>Example: In the pull request description, you would explain how the new reward function works, its benefits, and any relevant implementation details.</li> </ul>"},{"location":"CONTRIBUTING/#7-respond-to-feedback","title":"7. Respond to feedback","text":"<p>Be responsive to any feedback or comments provided by the project maintainers. Address the feedback by making necessary revisions to your code and continue the discussion if required.</p>"},{"location":"CONTRIBUTING/#8-continuous-integration-and-code-review","title":"8. Continuous integration and code review","text":"<p>The ML-Agents project utilizes automated continuous integration (CI) systems to run tests on pull requests. Address any issues flagged by the CI system and actively participate in the code review process by addressing comments from reviewers.</p>"},{"location":"CONTRIBUTING/#9-merge-your-changes","title":"9. Merge your changes","text":"<p>Once your pull request has been approved and meets all the project's requirements, a project maintainer will merge your changes into the main repository. Congratulations, your contribution has been successfully integrated!</p> <p>Remember to always adhere to the project's code of conduct, be respectful, and follow any specific contribution guidelines provided by the ML-Agents project. Happy contributing!</p>"},{"location":"Custom-SideChannels/","title":"Custom Side Channels","text":"<p>You can create your own side channel in C# and Python and use it to communicate custom data structures between the two. This can be useful for situations in which the data to be sent is too complex or structured for the built-in <code>EnvironmentParameters</code>, or is not related to any specific agent, and therefore inappropriate as an agent observation.</p>"},{"location":"Custom-SideChannels/#overview","title":"Overview","text":"<p>In order to use a side channel, it must be implemented as both Unity and Python classes.</p>"},{"location":"Custom-SideChannels/#unity-side","title":"Unity side","text":"<p>The side channel will have to implement the <code>SideChannel</code> abstract class and the following method.</p> <ul> <li><code>OnMessageReceived(IncomingMessage msg)</code> : You must implement this method and   read the data from IncomingMessage. The data must be read in the order that it   was written.</li> </ul> <p>The side channel must also assign a <code>ChannelId</code> property in the constructor. The <code>ChannelId</code> is a Guid (or UUID in Python) used to uniquely identify a side channel. This Guid must be the same on C# and Python. There can only be one side channel of a certain id during communication.</p> <p>To send data from C# to Python, create an <code>OutgoingMessage</code> instance, add data to it, call the <code>base.QueueMessageToSend(msg)</code> method inside the side channel, and call the <code>OutgoingMessage.Dispose()</code> method.</p> <p>To register a side channel on the Unity side, call <code>SideChannelManager.RegisterSideChannel</code> with the side channel as only argument.</p>"},{"location":"Custom-SideChannels/#python-side","title":"Python side","text":"<p>The side channel will have to implement the <code>SideChannel</code> abstract class. You must implement :</p> <ul> <li><code>on_message_received(self, msg: \"IncomingMessage\") -&gt; None</code> : You must   implement this method and read the data from IncomingMessage. The data must be   read in the order that it was written.</li> </ul> <p>The side channel must also assign a <code>channel_id</code> property in the constructor. The <code>channel_id</code> is a UUID (referred in C# as Guid) used to uniquely identify a side channel. This number must be the same on C# and Python. There can only be one side channel of a certain id during communication.</p> <p>To assign the <code>channel_id</code> call the abstract class constructor with the appropriate <code>channel_id</code> as follows:</p> <pre><code>super().__init__(my_channel_id)\n</code></pre> <p>To send a byte array from Python to C#, create an <code>OutgoingMessage</code> instance, add data to it, and call the <code>super().queue_message_to_send(msg)</code> method inside the side channel.</p> <p>To register a side channel on the Python side, pass the side channel as argument when creating the <code>UnityEnvironment</code> object. One of the arguments of the constructor (<code>side_channels</code>) is a list of side channels.</p>"},{"location":"Custom-SideChannels/#example-implementation","title":"Example implementation","text":"<p>Below is a simple implementation of a side channel that will exchange ASCII encoded strings between a Unity environment and Python.</p>"},{"location":"Custom-SideChannels/#example-unity-c-code","title":"Example Unity C# code","text":"<p>The first step is to create the <code>StringLogSideChannel</code> class within the Unity project. Here is an implementation of a <code>StringLogSideChannel</code> that will listen for messages from python and print them to the Unity debug log, as well as send error messages from Unity to python.</p> <pre><code>using UnityEngine;\nusing Unity.MLAgents;\nusing Unity.MLAgents.SideChannels;\nusing System.Text;\nusing System;\n\npublic class StringLogSideChannel : SideChannel\n{\n    public StringLogSideChannel()\n    {\n        ChannelId = new Guid(\"621f0a70-4f87-11ea-a6bf-784f4387d1f7\");\n    }\n\n    protected override void OnMessageReceived(IncomingMessage msg)\n    {\n        var receivedString = msg.ReadString();\n        Debug.Log(\"From Python : \" + receivedString);\n    }\n\n    public void SendDebugStatementToPython(string logString, string stackTrace, LogType type)\n    {\n        if (type == LogType.Error)\n        {\n            var stringToSend = type.ToString() + \": \" + logString + \"\\n\" + stackTrace;\n            using (var msgOut = new OutgoingMessage())\n            {\n                msgOut.WriteString(stringToSend);\n                QueueMessageToSend(msgOut);\n            }\n        }\n    }\n}\n</code></pre> <p>Once we have defined our custom side channel class, we need to ensure that it is instantiated and registered. This can typically be done wherever the logic of the side channel makes sense to be associated, for example on a MonoBehaviour object that might need to access data from the side channel. Here we show a simple MonoBehaviour object which instantiates and registers the new side channel. If you have not done it already, make sure that the MonoBehaviour which registers the side channel is attached to a GameObject which will be live in your Unity scene.</p> <pre><code>using UnityEngine;\nusing Unity.MLAgents;\n\n\npublic class RegisterStringLogSideChannel : MonoBehaviour\n{\n\n    StringLogSideChannel stringChannel;\n    public void Awake()\n    {\n        // We create the Side Channel\n        stringChannel = new StringLogSideChannel();\n\n        // When a Debug.Log message is created, we send it to the stringChannel\n        Application.logMessageReceived += stringChannel.SendDebugStatementToPython;\n\n        // The channel must be registered with the SideChannelManager class\n        SideChannelManager.RegisterSideChannel(stringChannel);\n    }\n\n    public void OnDestroy()\n    {\n        // De-register the Debug.Log callback\n        Application.logMessageReceived -= stringChannel.SendDebugStatementToPython;\n        if (Academy.IsInitialized){\n            SideChannelManager.UnregisterSideChannel(stringChannel);\n        }\n    }\n\n    public void Update()\n    {\n        // Optional : If the space bar is pressed, raise an error !\n        if (Input.GetKeyDown(KeyCode.Space))\n        {\n            Debug.LogError(\"This is a fake error. Space bar was pressed in Unity.\");\n        }\n    }\n}\n</code></pre>"},{"location":"Custom-SideChannels/#example-python-code","title":"Example Python code","text":"<p>Now that we have created the necessary Unity C# classes, we can create their Python counterparts.</p> <pre><code>from mlagents_envs.environment import UnityEnvironment\nfrom mlagents_envs.side_channel.side_channel import (\n    SideChannel,\n    IncomingMessage,\n    OutgoingMessage,\n)\nimport numpy as np\nimport uuid\n\n\n# Create the StringLogChannel class\nclass StringLogChannel(SideChannel):\n\n    def __init__(self) -&gt; None:\n        super().__init__(uuid.UUID(\"621f0a70-4f87-11ea-a6bf-784f4387d1f7\"))\n\n    def on_message_received(self, msg: IncomingMessage) -&gt; None:\n        \"\"\"\n        Note: We must implement this method of the SideChannel interface to\n        receive messages from Unity\n        \"\"\"\n        # We simply read a string from the message and print it.\n        print(msg.read_string())\n\n    def send_string(self, data: str) -&gt; None:\n        # Add the string to an OutgoingMessage\n        msg = OutgoingMessage()\n        msg.write_string(data)\n        # We call this method to queue the data we want to send\n        super().queue_message_to_send(msg)\n</code></pre> <p>We can then instantiate the new side channel, launch a <code>UnityEnvironment</code> with that side channel active, and send a series of messages to the Unity environment from Python using it.</p> <pre><code># Create the channel\nstring_log = StringLogChannel()\n\n# We start the communication with the Unity Editor and pass the string_log side channel as input\nenv = UnityEnvironment(side_channels=[string_log])\nenv.reset()\nstring_log.send_string(\"The environment was reset\")\n\ngroup_name = list(env.behavior_specs.keys())[0]  # Get the first group_name\ngroup_spec = env.behavior_specs[group_name]\nfor i in range(1000):\n    decision_steps, terminal_steps = env.get_steps(group_name)\n    # We send data to Unity : A string with the number of Agent at each\n    string_log.send_string(\n        f\"Step {i} occurred with {len(decision_steps)} deciding agents and \"\n        f\"{len(terminal_steps)} terminal agents\"\n    )\n    env.step()  # Move the simulation forward\n\nenv.close()\n</code></pre> <p>Now, if you run this script and press <code>Play</code> the Unity Editor when prompted, the console in the Unity Editor will display a message at every Python step. Additionally, if you press the Space Bar in the Unity Engine, a message will appear in the terminal.</p>"},{"location":"ELO-Rating-System/","title":"ELO Rating System","text":"<p>In adversarial games, the cumulative environment reward may not be a meaningful metric by which to track learning progress.</p> <p>This is because the cumulative reward is entirely dependent on the skill of the opponent.</p> <p>An agent at a particular skill level will get more or less reward against a worse or better agent, respectively.</p> <p>Instead, it's better to use ELO rating system, a method to calculate the relative skill level between two players in a zero-sum game.</p> <p>If the training performs correctly, this value should steadily increase.</p>"},{"location":"ELO-Rating-System/#what-is-a-zero-sum-game","title":"What is a zero-sum game?","text":"<p>A zero-sum game is a game where each player's gain or loss of utility is exactly balanced by the gain or loss of the utility of the opponent.</p> <p>Simply explained, we face a zero-sum game when one agent gets +1.0, its opponent gets -1.0 reward.</p> <p>For instance, Tennis is a zero-sum game: if you win the point you get +1.0 and your opponent gets -1.0 reward.</p>"},{"location":"ELO-Rating-System/#how-works-the-elo-rating-system","title":"How works the ELO Rating System","text":"<ul> <li> <p>Each player has an initial ELO score. It's defined in the <code>initial_elo</code> trainer config hyperparameter.</p> </li> <li> <p>The difference in rating between the two players serves as the predictor of the outcomes of a match.</p> </li> </ul> <p> For instance, if player A has an Elo score of 2100 and player B has an ELO score of 1800 the chance that player A wins is 85% against 15% for player b.</p> <ul> <li>We calculate the expected score of each player using this formula:</li> </ul> <p></p> <ul> <li>At the end of the game, based on the outcome we update the player\u2019s actual Elo score, we use a linear adjustment proportional to the amount by which the player over-performed or under-performed. The winning player takes points from the losing one:</li> <li>If the higher-rated player wins \u2192 a few points will be taken from the lower-rated player.</li> <li>If the lower-rated player wins \u2192 a lot of points will be taken from the high-rated player.</li> <li> <p>If it\u2019s a draw \u2192 the lower-rated player gains a few points from higher.</p> </li> <li> <p>We update players rating using this formula:</p> </li> </ul> <p></p>"},{"location":"ELO-Rating-System/#the-tennis-example","title":"The Tennis example","text":"<ul> <li>We start to train our agents.</li> <li>Both of them have the same skills. So ELO score for each of them that we defined using parameter <code>initial_elo = 1200.0</code>.</li> </ul> <p>We calculate the expected score E: Ea = 0.5 Eb = 0.5</p> <p>So it means that each player has 50% chances of winning the point.</p> <p>If A wins, the new rating R would be:</p> <p>Ra = 1200 + 16 * (1 - 0.5) \u2192 1208</p> <p>Rb = 1200 + 16 * (0 - 0.5) \u2192 1192</p> <p>Player A has now an ELO score of 1208 and Player B an ELO score of 1192. Therefore, Player A is now a little bit better than Player B.</p>"},{"location":"FAQ/","title":"Frequently Asked Questions","text":""},{"location":"FAQ/#installation-problems","title":"Installation problems","text":""},{"location":"FAQ/#environment-permission-error","title":"Environment Permission Error","text":"<p>If you directly import your Unity environment without building it in the editor, you might need to give it additional permissions to execute it.</p> <p>If you receive such a permission error on macOS, run:</p> <pre><code>chmod -R 755 *.app\n</code></pre> <p>or on Linux:</p> <pre><code>chmod -R 755 *.x86_64\n</code></pre> <p>On Windows, you can find instructions.</p>"},{"location":"FAQ/#environment-connection-timeout","title":"Environment Connection Timeout","text":"<p>If you are able to launch the environment from <code>UnityEnvironment</code> but then receive a timeout error like this:</p> <pre><code>UnityAgentsException: The Communicator was unable to connect. Please make sure the External process is ready to accept communication with Unity.\n</code></pre> <p>There may be a number of possible causes:</p> <ul> <li>Cause: There may be no agent in the scene</li> <li>Cause: On OSX, the firewall may be preventing communication with the   environment. Solution: Add the built environment binary to the list of   exceptions on the firewall by following   instructions.</li> <li>Cause: An error happened in the Unity Environment preventing communication.   Solution: Look into the   log files generated by the   Unity Environment to figure what error happened.</li> <li>Cause: You have assigned <code>HTTP_PROXY</code> and <code>HTTPS_PROXY</code> values in your   environment variables. Solution: Remove these values and try again.</li> <li>Cause: You are running in a headless environment (e.g. remotely connected   to a server). Solution: Pass <code>--no-graphics</code> to <code>mlagents-learn</code>, or   <code>no_graphics=True</code> to <code>RemoteRegistryEntry.make()</code> or the <code>UnityEnvironment</code>   initializer. If you need graphics for visual observations, you will need to   set up <code>xvfb</code> (or equivalent).</li> </ul>"},{"location":"FAQ/#communication-port-still-in-use","title":"Communication port {} still in use","text":"<p>If you receive an exception <code>\"Couldn't launch new environment because communication port {} is still in use. \"</code>, you can change the worker number in the Python script when calling</p> <pre><code>UnityEnvironment(file_name=filename, worker_id=X)\n</code></pre>"},{"location":"FAQ/#mean-reward-nan","title":"Mean reward : nan","text":"<p>If you receive a message <code>Mean reward : nan</code> when attempting to train a model using PPO, this is due to the episodes of the Learning Environment not terminating. In order to address this, set <code>Max Steps</code> for the Agents within the Scene Inspector to a value greater than 0. Alternatively, it is possible to manually set <code>done</code> conditions for episodes from within scripts for custom episode-terminating events.</p>"},{"location":"FAQ/#file-name-cannot-be-opened-because-the-developer-cannot-be-verified","title":"\"File name\" cannot be opened because the developer cannot be verified.","text":"<p>If you have downloaded the repository using the github website on macOS 10.15 (Catalina) or later, you may see this error when attempting to play scenes in the Unity project. Workarounds include installing the package using the Unity Package Manager (this is the officially supported approach - see here), or following the instructions here to verify the relevant files on your machine on a file-by-file basis.</p>"},{"location":"Getting-Started/","title":"Getting Started Guide","text":"<p>This guide walks through the end-to-end process of opening one of our example environments in Unity, training an Agent in it, and embedding the trained model into the Unity environment. After reading this tutorial, you should be able to train any of the example environments. If you are not familiar with the Unity Engine, view our Background: Unity page for helpful pointers. Additionally, if you're not familiar with machine learning, view our Background: Machine Learning page for a brief overview and helpful pointers.</p> <p></p> <p>For this guide, we'll use the 3D Balance Ball environment which contains a number of agent cubes and balls (which are all copies of each other). Each agent cube tries to keep its ball from falling by rotating either horizontally or vertically. In this environment, an agent cube is an Agent that receives a reward for every step that it balances the ball. An agent is also penalized with a negative reward for dropping the ball. The goal of the training process is to have the agents learn to balance the ball on their head.</p> <p>Let's get started!</p>"},{"location":"Getting-Started/#installation","title":"Installation","text":"<p>If you haven't already, follow the installation instructions. Afterwards, open the Unity Project that contains all the example environments:</p> <ol> <li>Open the Package Manager Window by navigating to <code>Window -&gt; Package Manager</code>    in the menu.</li> <li>Navigate to the ML-Agents Package and click on it.</li> <li>Find the <code>3D Ball</code> sample and click <code>Import</code>.</li> <li>In the Project window, go to the    <code>Assets/ML-Agents/Examples/3DBall/Scenes</code> folder and open the <code>3DBall</code> scene    file.</li> </ol>"},{"location":"Getting-Started/#understanding-a-unity-environment","title":"Understanding a Unity Environment","text":"<p>An agent is an autonomous actor that observes and interacts with an environment. In the context of Unity, an environment is a scene containing one or more Agent objects, and, of course, the other entities that an agent interacts with.</p> <p></p> <p>Note: In Unity, the base object of everything in a scene is the GameObject. The GameObject is essentially a container for everything else, including behaviors, graphics, physics, etc. To see the components that make up a GameObject, select the GameObject in the Scene window, and open the Inspector window. The Inspector shows every component on a GameObject.</p> <p>The first thing you may notice after opening the 3D Balance Ball scene is that it contains not one, but several agent cubes. Each agent cube in the scene is an independent agent, but they all share the same Behavior. 3D Balance Ball does this to speed up training since all twelve agents contribute to training in parallel.</p>"},{"location":"Getting-Started/#agent","title":"Agent","text":"<p>The Agent is the actor that observes and takes actions in the environment. In the 3D Balance Ball environment, the Agent components are placed on the twelve \"Agent\" GameObjects. The base Agent object has a few properties that affect its behavior:</p> <ul> <li>Behavior Parameters \u2014 Every Agent must have a Behavior. The Behavior   determines how an Agent makes decisions.</li> <li>Max Step \u2014 Defines how many simulation steps can occur before the Agent's   episode ends. In 3D Balance Ball, an Agent restarts after 5000 steps.</li> </ul>"},{"location":"Getting-Started/#behavior-parameters-vector-observation-space","title":"Behavior Parameters : Vector Observation Space","text":"<p>Before making a decision, an agent collects its observation about its state in the world. The vector observation is a vector of floating point numbers which contain relevant information for the agent to make decisions.</p> <p>The Behavior Parameters of the 3D Balance Ball example uses a <code>Space Size</code> of 8. This means that the feature vector containing the Agent's observations contains eight elements: the <code>x</code> and <code>z</code> components of the agent cube's rotation and the <code>x</code>, <code>y</code>, and <code>z</code> components of the ball's relative position and velocity.</p>"},{"location":"Getting-Started/#behavior-parameters-actions","title":"Behavior Parameters : Actions","text":"<p>An Agent is given instructions in the form of actions. ML-Agents Toolkit classifies actions into two types: continuous and discrete. The 3D Balance Ball example is programmed to use continuous actions, which are a vector of floating-point numbers that can vary continuously. More specifically, it uses a <code>Space Size</code> of 2 to control the amount of <code>x</code> and <code>z</code> rotations to apply to itself to keep the ball balanced on its head.</p>"},{"location":"Getting-Started/#running-a-pre-trained-model","title":"Running a pre-trained model","text":"<p>We include pre-trained models for our agents (<code>.onnx</code> files) and we use the Sentis to run these models inside Unity. In this section, we will use the pre-trained model for the 3D Ball example.</p> <ol> <li>In the Project window, go to the    <code>Assets/ML-Agents/Examples/3DBall/Prefabs</code> folder. Expand <code>3DBall</code> and click    on the <code>Agent</code> prefab. You should see the <code>Agent</code> prefab in the Inspector    window.</li> </ol> <p>Note: The platforms in the <code>3DBall</code> scene were created using the <code>3DBall</code>    prefab. Instead of updating all 12 platforms individually, you can update the    <code>3DBall</code> prefab instead.</p> <p></p> <ol> <li>In the Project window, drag the 3DBall Model located in    <code>Assets/ML-Agents/Examples/3DBall/TFModels</code> into the <code>Model</code> property under    <code>Behavior Parameters (Script)</code> component in the Agent GameObject    Inspector window.</li> </ol> <p></p> <ol> <li>You should notice that each <code>Agent</code> under each <code>3DBall</code> in the Hierarchy    windows now contains 3DBall as <code>Model</code> on the <code>Behavior Parameters</code>.    Note : You can modify multiple game objects in a scene by selecting them    all at once using the search bar in the Scene Hierarchy.</li> <li>Set the Inference Device to use for this model as <code>CPU</code>.</li> <li>Click the Play button in the Unity Editor and you will see the platforms    balance the balls using the pre-trained model.</li> </ol>"},{"location":"Getting-Started/#training-a-new-model-with-reinforcement-learning","title":"Training a new model with Reinforcement Learning","text":"<p>While we provide pre-trained models for the agents in this environment, any environment you make yourself will require training agents from scratch to generate a new model file. In this section we will demonstrate how to use the reinforcement learning algorithms that are part of the ML-Agents Python package to accomplish this. We have provided a convenient command <code>mlagents-learn</code> which accepts arguments used to configure both training and inference phases.</p>"},{"location":"Getting-Started/#training-the-environment","title":"Training the environment","text":"<ol> <li>Open a command or terminal window.</li> <li>Navigate to the folder where you cloned the <code>ml-agents</code> repository. Note:    If you followed the default installation, then you should    be able to run <code>mlagents-learn</code> from any directory.</li> <li>Run <code>mlagents-learn config/ppo/3DBall.yaml --run-id=first3DBallRun</code>.</li> <li><code>config/ppo/3DBall.yaml</code> is the path to a default training      configuration file that we provide. The <code>config/ppo</code> folder includes training configuration      files for all our example environments, including 3DBall.</li> <li><code>run-id</code> is a unique name for this training session.</li> <li>When the message \"Start training by pressing the Play button in the Unity    Editor\" is displayed on the screen, you can press the Play button in    Unity to start training in the Editor.</li> </ol> <p>If <code>mlagents-learn</code> runs correctly and starts training, you should see something like this:</p> <pre><code>INFO:mlagents_envs:\n'Ball3DAcademy' started successfully!\nUnity Academy name: Ball3DAcademy\n\nINFO:mlagents_envs:Connected new brain:\nUnity brain name: 3DBallLearning\n        Number of Visual Observations (per agent): 0\n        Vector Observation space size (per agent): 8\n        Number of stacked Vector Observation: 1\nINFO:mlagents_envs:Hyperparameters for the PPO Trainer of brain 3DBallLearning:\n        batch_size:          64\n        beta:                0.001\n        buffer_size:         12000\n        epsilon:             0.2\n        gamma:               0.995\n        hidden_units:        128\n        lambd:               0.99\n        learning_rate:       0.0003\n        max_steps:           5.0e4\n        normalize:           True\n        num_epoch:           3\n        num_layers:          2\n        time_horizon:        1000\n        sequence_length:     64\n        summary_freq:        1000\n        use_recurrent:       False\n        memory_size:         256\n        use_curiosity:       False\n        curiosity_strength:  0.01\n        curiosity_enc_size:  128\n        output_path: ./results/first3DBallRun/3DBallLearning\nINFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 1000. Mean Reward: 1.242. Std of Reward: 0.746. Training.\nINFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 2000. Mean Reward: 1.319. Std of Reward: 0.693. Training.\nINFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 3000. Mean Reward: 1.804. Std of Reward: 1.056. Training.\nINFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 4000. Mean Reward: 2.151. Std of Reward: 1.432. Training.\nINFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 5000. Mean Reward: 3.175. Std of Reward: 2.250. Training.\nINFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 6000. Mean Reward: 4.898. Std of Reward: 4.019. Training.\nINFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 7000. Mean Reward: 6.716. Std of Reward: 5.125. Training.\nINFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 8000. Mean Reward: 12.124. Std of Reward: 11.929. Training.\nINFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 9000. Mean Reward: 18.151. Std of Reward: 16.871. Training.\nINFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 10000. Mean Reward: 27.284. Std of Reward: 28.667. Training.\n</code></pre> <p>Note how the <code>Mean Reward</code> value printed to the screen increases as training progresses. This is a positive sign that training is succeeding.</p> <p>Note: You can train using an executable rather than the Editor. To do so, follow the instructions in Using an Executable.</p>"},{"location":"Getting-Started/#observing-training-progress","title":"Observing Training Progress","text":"<p>Once you start training using <code>mlagents-learn</code> in the way described in the previous section, the <code>ml-agents</code> directory will contain a <code>results</code> directory. In order to observe the training process in more detail, you can use TensorBoard. From the command line run:</p> <pre><code>tensorboard --logdir results\n</code></pre> <p>Then navigate to <code>localhost:6006</code> in your browser to view the TensorBoard summary statistics as shown below. For the purposes of this section, the most important statistic is <code>Environment/Cumulative Reward</code> which should increase throughout training, eventually converging close to <code>100</code> which is the maximum reward the agent can accumulate.</p> <p></p>"},{"location":"Getting-Started/#embedding-the-model-into-the-unity-environment","title":"Embedding the model into the Unity Environment","text":"<p>Once the training process completes, and the training process saves the model (denoted by the <code>Saved Model</code> message) you can add it to the Unity project and use it with compatible Agents (the Agents that generated the model). Note: Do not just close the Unity Window once the <code>Saved Model</code> message appears. Either wait for the training process to close the window or press <code>Ctrl+C</code> at the command-line prompt. If you close the window manually, the <code>.onnx</code> file containing the trained model is not exported into the ml-agents folder.</p> <p>If you've quit the training early using <code>Ctrl+C</code> and want to resume training, run the same command again, appending the <code>--resume</code> flag:</p> <pre><code>mlagents-learn config/ppo/3DBall.yaml --run-id=first3DBallRun --resume\n</code></pre> <p>Your trained model will be at <code>results/&lt;run-identifier&gt;/&lt;behavior_name&gt;.onnx</code> where <code>&lt;behavior_name&gt;</code> is the name of the <code>Behavior Name</code> of the agents corresponding to the model. This file corresponds to your model's latest checkpoint. You can now embed this trained model into your Agents by following the steps below, which is similar to the steps described above.</p> <ol> <li>Move your model file into    <code>Project/Assets/ML-Agents/Examples/3DBall/TFModels/</code>.</li> <li>Open the Unity Editor, and select the 3DBall scene as described above.</li> <li>Select the 3DBall prefab Agent object.</li> <li>Drag the <code>&lt;behavior_name&gt;.onnx</code> file from the Project window of the Editor to    the Model placeholder in the Ball3DAgent inspector window.</li> <li>Press the Play button at the top of the Editor.</li> </ol>"},{"location":"Getting-Started/#next-steps","title":"Next Steps","text":"<ul> <li>For more information on the ML-Agents Toolkit, in addition to helpful   background, check out the ML-Agents Toolkit Overview   page.</li> <li>For a \"Hello World\" introduction to creating your own Learning Environment,   check out the   Making a New Learning Environment page.</li> <li>For an overview on the more complex example environments that are provided in   this toolkit, check out the   Example Environments page.</li> <li>For more information on the various training options available, check out the   Training ML-Agents page.</li> </ul>"},{"location":"Glossary/","title":"ML-Agents Toolkit Glossary","text":"<ul> <li>Academy - Singleton object which controls timing, reset, and   training/inference settings of the environment.</li> <li>Action - The carrying-out of a decision on the part of an agent within the   environment.</li> <li>Agent - Unity Component which produces observations and takes actions in   the environment. Agents actions are determined by decisions produced by a   Policy.</li> <li>Decision - The specification produced by a Policy for an action to be   carried out given an observation.</li> <li>Editor - The Unity Editor, which may include any pane (e.g. Hierarchy,   Scene, Inspector).</li> <li>Environment - The Unity scene which contains Agents.</li> <li>Experience - Corresponds to a tuple of [Agent observations, actions,   rewards] of a single Agent obtained after a Step.</li> <li>External Coordinator - ML-Agents class responsible for communication with   outside processes (in this case, the Python API).</li> <li>FixedUpdate - Unity method called each time the game engine is stepped.   ML-Agents logic should be placed here.</li> <li>Frame - An instance of rendering the main camera for the display.   Corresponds to each <code>Update</code> call of the game engine.</li> <li>Observation - Partial information describing the state of the environment   available to a given agent. (e.g. Vector, Visual)</li> <li>Policy - The decision making mechanism for producing decisions from   observations, typically a neural network model.</li> <li>Reward - Signal provided at every step used to indicate desirability of an   agent\u2019s action within the current state of the environment.</li> <li>State - The underlying properties of the environment (including all agents   within it) at a given time.</li> <li>Step - Corresponds to an atomic change of the engine that happens between   Agent decisions.</li> <li>Trainer - Python class which is responsible for training a given group of   Agents.</li> <li>Update - Unity function called each time a frame is rendered. ML-Agents   logic should not be placed here.</li> </ul>"},{"location":"Hugging-Face-Integration/","title":"The Hugging Face Integration","text":"<p>The Hugging Face Hub \ud83e\udd17 is a central place where anyone can share and download models.</p> <p>It allows you to: - Host your trained models. - Download trained models from the community. - Visualize your agents playing directly on your browser.</p> <p>You can see the list of ml-agents models here.</p> <p>We wrote a complete tutorial to learn to train your first agent using ML-Agents and publish it to the Hub:</p> <ul> <li>A short tutorial where you teach Huggy the Dog to fetch the stick and then play with him directly in your browser</li> <li>A more in-depth tutorial</li> </ul>"},{"location":"Hugging-Face-Integration/#download-a-model-from-the-hub","title":"Download a model from the Hub","text":"<p>You can simply download a model from the Hub using <code>mlagents-load-from-hf</code>.</p> <p>You need to define two parameters:</p> <ul> <li><code>--repo-id</code>: the name of the Hugging Face repo you want to download.</li> <li><code>--local-dir</code>: the path to download the model.</li> </ul> <p>For instance, I want to load the model with model-id \"ThomasSimonini/MLAgents-Pyramids\" and put it in the downloads directory:</p> <pre><code>mlagents-load-from-hf --repo-id=\"ThomasSimonini/MLAgents-Pyramids\" --local-dir=\"./downloads\"\n</code></pre>"},{"location":"Hugging-Face-Integration/#upload-a-model-to-the-hub","title":"Upload a model to the Hub","text":"<p>You can simply upload a model to the Hub using <code>mlagents-push-to-hf</code></p> <p>You need to define four parameters:</p> <ul> <li><code>--run-id</code>: the name of the training run id.</li> <li><code>--local-dir</code>: where the model was saved</li> <li><code>--repo-id</code>: the name of the Hugging Face repo you want to create or update. It\u2019s always / If the repo does not exist it will be created automatically <li><code>--commit-message</code>: since HF repos are git repositories you need to give a commit message.</li> <p>For instance, I want to upload my model trained with run-id \"SnowballTarget1\" to the repo-id: ThomasSimonini/ppo-SnowballTarget:</p> <pre><code>  mlagents-push-to-hf --run-id=\"SnowballTarget1\" --local-dir=\"./results/SnowballTarget1\" --repo-id=\"ThomasSimonini/ppo-SnowballTarget\" --commit-message=\"First Push\"\n</code></pre>"},{"location":"Hugging-Face-Integration/#visualize-an-agent-playing","title":"Visualize an agent playing","text":"<p>You can watch your agent playing directly in your browser (if the environment is from the ML-Agents official environments)</p> <ul> <li>Step 1: Go to https://huggingface.co/unity and select the environment demo.</li> <li>Step 2: Find your model_id in the list.</li> <li>Step 3: Select your .nn /.onnx file.</li> <li>Step 4: Click on Watch the agent play</li> </ul>"},{"location":"Installation-Anaconda-Windows/","title":"Installing ML-Agents Toolkit for Windows (Deprecated)","text":"<p>:warning: Note: We no longer use this guide ourselves and so it may not work correctly. We've decided to keep it up just in case it is helpful to you.</p> <p>The ML-Agents Toolkit supports Windows 10. While it might be possible to run the ML-Agents Toolkit using other versions of Windows, it has not been tested on other versions. Furthermore, the ML-Agents Toolkit has not been tested on a Windows VM such as Bootcamp or Parallels.</p> <p>To use the ML-Agents Toolkit, you install Python and the required Python packages as outlined below. This guide also covers how set up GPU-based training (for advanced users). GPU-based training is not currently required for the ML-Agents Toolkit. However, training on a GPU might be required by future versions and features.</p>"},{"location":"Installation-Anaconda-Windows/#step-1-install-python-via-anaconda","title":"Step 1: Install Python via Anaconda","text":"<p>Download and install Anaconda for Windows. By using Anaconda, you can manage separate environments for different distributions of Python. Python 3.7.2 or higher is required as we no longer support Python 2. In this guide, we are using Python version 3.7 and Anaconda version 5.1 (64-bit or 32-bit direct links).</p> <p> </p> <p>We recommend the default advanced installation options. However, select the options appropriate for your specific situation.</p> <p> </p> <p>After installation, you must open Anaconda Navigator to finish the setup. From the Windows search bar, type anaconda navigator. You can close Anaconda Navigator after it opens.</p> <p>If environment variables were not created, you will see error \"conda is not recognized as internal or external command\" when you type <code>conda</code> into the command line. To solve this you will need to set the environment variable correctly.</p> <p>Type <code>environment variables</code> in the search bar (this can be reached by hitting the Windows key or the bottom left Windows button). You should see an option called Edit the system environment variables.</p> <p> </p> <p>From here, click the Environment Variables button. Double click \"Path\" under System variable to edit the \"Path\" variable, click New to add the following new paths.</p> <pre><code>%UserProfile%\\Anaconda3\\Scripts\n%UserProfile%\\Anaconda3\\Scripts\\conda.exe\n%UserProfile%\\Anaconda3\n%UserProfile%\\Anaconda3\\python.exe\n</code></pre>"},{"location":"Installation-Anaconda-Windows/#step-2-setup-and-activate-a-new-conda-environment","title":"Step 2: Setup and Activate a New Conda Environment","text":"<p>You will create a new Conda environment to be used with the ML-Agents Toolkit. This means that all the packages that you install are localized to just this environment. It will not affect any other installation of Python or other environments. Whenever you want to run ML-Agents, you will need activate this Conda environment.</p> <p>To create a new Conda environment, open a new Anaconda Prompt (Anaconda Prompt in the search bar) and type in the following command:</p> <pre><code>conda create -n ml-agents python=3.7\n</code></pre> <p>You may be asked to install new packages. Type <code>y</code> and press enter (make sure you are connected to the Internet). You must install these required packages. The new Conda environment is called ml-agents and uses Python version 3.7.</p> <p> </p> <p>To use this environment, you must activate it. (To use this environment In the future, you can run the same command). In the same Anaconda Prompt, type in the following command:</p> <pre><code>activate ml-agents\n</code></pre> <p>You should see <code>(ml-agents)</code> prepended on the last line.</p> <p>Next, install <code>tensorflow</code>. Install this package using <code>pip</code> - which is a package management system used to install Python packages. Latest versions of TensorFlow won't work, so you will need to make sure that you install version 1.7.1. In the same Anaconda Prompt, type in the following command (make sure you are connected to the Internet):</p> <pre><code>pip install tensorflow==1.7.1\n</code></pre>"},{"location":"Installation-Anaconda-Windows/#step-3-install-required-python-packages","title":"Step 3: Install Required Python Packages","text":"<p>The ML-Agents Toolkit depends on a number of Python packages. Use <code>pip</code> to install these Python dependencies.</p> <p>If you haven't already, clone the ML-Agents Toolkit Github repository to your local computer. You can do this using Git (download here) and running the following commands in an Anaconda Prompt (if you open a new prompt, be sure to activate the ml-agents Conda environment by typing <code>activate ml-agents</code>):</p> <pre><code>git clone --branch release_21 https://github.com/Unity-Technologies/ml-agents.git\n</code></pre> <p>The <code>--branch release_21</code> option will switch to the tag of the latest stable release. Omitting that will get the <code>main</code> branch which is potentially unstable.</p> <p>If you don't want to use Git, you can find download links on the releases page.</p> <p>The <code>com.unity.ml-agents</code> subdirectory contains the core code to add to your projects. The <code>Project</code> subdirectory contains many example environments to help you get started.</p> <p>The <code>ml-agents</code> subdirectory contains a Python package which provides deep reinforcement learning trainers to use with Unity environments.</p> <p>The <code>ml-agents-envs</code> subdirectory contains a Python API to interface with Unity, which the <code>ml-agents</code> package depends on.</p> <p>The <code>gym-unity</code> subdirectory contains a package to interface with OpenAI Gym.</p> <p>Keep in mind where the files were downloaded, as you will need the trainer config files in this directory when running <code>mlagents-learn</code>. Make sure you are connected to the Internet and then type in the Anaconda Prompt:</p> <pre><code>python -m pip install mlagents==1.0.0\n</code></pre> <p>This will complete the installation of all the required Python packages to run the ML-Agents Toolkit.</p> <p>Sometimes on Windows, when you use pip to install certain Python packages, the pip will get stuck when trying to read the cache of the package. If you see this, you can try:</p> <pre><code>python -m pip install mlagents==1.0.0 --no-cache-dir\n</code></pre> <p>This <code>--no-cache-dir</code> tells the pip to disable the cache.</p>"},{"location":"Installation-Anaconda-Windows/#installing-for-development","title":"Installing for Development","text":"<p>If you intend to make modifications to <code>ml-agents</code> or <code>ml-agents-envs</code>, you should install the packages from the cloned repo rather than from PyPi. To do this, you will need to install <code>ml-agents</code> and <code>ml-agents-envs</code> separately.</p> <p>In our example, the files are located in <code>C:\\Downloads</code>. After you have either cloned or downloaded the files, from the Anaconda Prompt, change to the ml-agents subdirectory inside the ml-agents directory:</p> <pre><code>cd C:\\Downloads\\ml-agents\n</code></pre> <p>From the repo's main directory, now run:</p> <pre><code>cd ml-agents-envs\npip install -e .\ncd ..\ncd ml-agents\npip install -e .\n</code></pre> <p>Running pip with the <code>-e</code> flag will let you make changes to the Python files directly and have those reflected when you run <code>mlagents-learn</code>. It is important to install these packages in this order as the <code>mlagents</code> package depends on <code>mlagents_envs</code>, and installing it in the other order will download <code>mlagents_envs</code> from PyPi.</p>"},{"location":"Installation-Anaconda-Windows/#optional-step-4-gpu-training-using-the-ml-agents-toolkit","title":"(Optional) Step 4: GPU Training using The ML-Agents Toolkit","text":"<p>GPU is not required for the ML-Agents Toolkit and won't speed up the PPO algorithm a lot during training(but something in the future will benefit from GPU). This is a guide for advanced users who want to train using GPUs. Additionally, you will need to check if your GPU is CUDA compatible. Please check Nvidia's page here.</p> <p>Currently for the ML-Agents Toolkit, only CUDA v9.0 and cuDNN v7.0.5 is supported.</p>"},{"location":"Installation-Anaconda-Windows/#install-nvidia-cuda-toolkit","title":"Install Nvidia CUDA toolkit","text":"<p>Download and install the CUDA toolkit 9.0 from Nvidia's archive. The toolkit includes GPU-accelerated libraries, debugging and optimization tools, a C/C++ (Step Visual Studio 2017) compiler and a runtime library and is needed to run the ML-Agents Toolkit. In this guide, we are using version 9.0.176).</p> <p>Before installing, please make sure you close any running instances of Unity or Visual Studio.</p> <p>Run the installer and select the Express option. Note the directory where you installed the CUDA toolkit. In this guide, we installed in the directory <code>C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0</code></p>"},{"location":"Installation-Anaconda-Windows/#install-nvidia-cudnn-library","title":"Install Nvidia cuDNN library","text":"<p>Download and install the cuDNN library from Nvidia. cuDNN is a GPU-accelerated library of primitives for deep neural networks. Before you can download, you will need to sign up for free to the Nvidia Developer Program.</p> <p> </p> <p>Once you've signed up, go back to the cuDNN downloads page. You may or may not be asked to fill out a short survey. When you get to the list cuDNN releases, make sure you are downloading the right version for the CUDA toolkit you installed in Step 1. In this guide, we are using version 7.0.5 for CUDA toolkit version 9.0 (direct link).</p> <p>After you have downloaded the cuDNN files, you will need to extract the files into the CUDA toolkit directory. In the cuDNN zip file, there are three folders called <code>bin</code>, <code>include</code>, and <code>lib</code>.</p> <p> </p> <p>Copy these three folders into the CUDA toolkit directory. The CUDA toolkit directory is located at <code>C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0</code></p> <p> </p>"},{"location":"Installation-Anaconda-Windows/#set-environment-variables","title":"Set Environment Variables","text":"<p>You will need to add one environment variable and two path variables.</p> <p>To set the environment variable, type <code>environment variables</code> in the search bar (this can be reached by hitting the Windows key or the bottom left Windows button). You should see an option called Edit the system environment variables.</p> <p> </p> <p>From here, click the Environment Variables button. Click New to add a new system variable (make sure you do this under System variables and not User variables.</p> <p> </p> <p>For Variable Name, enter <code>CUDA_HOME</code>. For the variable value, put the directory location for the CUDA toolkit. In this guide, the directory location is <code>C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0</code>. Press OK once.</p> <p> </p> <p>To set the two path variables, inside the same Environment Variables window and under the second box called System Variables, find a variable called <code>Path</code> and click Edit. You will add two directories to the list. For this guide, the two entries would look like:</p> <pre><code>C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\lib\\x64\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\extras\\CUPTI\\libx64\n</code></pre> <p>Make sure to replace the relevant directory location with the one you have installed. Please note that case sensitivity matters.</p> <p> </p>"},{"location":"Installation-Anaconda-Windows/#install-tensorflow-gpu","title":"Install TensorFlow GPU","text":"<p>Next, install <code>tensorflow-gpu</code> using <code>pip</code>. You'll need version 1.7.1. In an Anaconda Prompt with the Conda environment ml-agents activated, type in the following command to uninstall TensorFlow for cpu and install TensorFlow for gpu (make sure you are connected to the Internet):</p> <pre><code>pip uninstall tensorflow\npip install tensorflow-gpu==1.7.1\n</code></pre> <p>Lastly, you should test to see if everything installed properly and that TensorFlow can identify your GPU. In the same Anaconda Prompt, open Python in the Prompt by calling:</p> <pre><code>python\n</code></pre> <p>And then type the following commands:</p> <pre><code>import tensorflow as tf\n\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n</code></pre> <p>You should see something similar to:</p> <pre><code>Found device 0 with properties ...\n</code></pre>"},{"location":"Installation-Anaconda-Windows/#acknowledgments","title":"Acknowledgments","text":"<p>We would like to thank Jason Weimann and Nitish S. Mutha for writing the original articles which were used to create this guide.</p>"},{"location":"Installation/","title":"Installation","text":"<p>The ML-Agents Toolkit contains several components:</p> <ul> <li>Unity package (<code>com.unity.ml-agents</code>) contains the   Unity C# SDK that will be integrated into your Unity project.  This package contains   a sample to help you get started with ML-Agents.</li> <li>Unity package   (<code>com.unity.ml-agents.extensions</code>)   contains experimental C#/Unity components that are not yet ready to be part   of the base <code>com.unity.ml-agents</code> package. <code>com.unity.ml-agents.extensions</code>   has a direct dependency on <code>com.unity.ml-agents</code>.</li> <li>Two Python packages:</li> <li><code>mlagents</code> contains the machine learning algorithms that     enables you to train behaviors in your Unity scene. Most users of ML-Agents     will only need to directly install <code>mlagents</code>.</li> <li><code>mlagents_envs</code> contains a set of Python APIs to interact with     a Unity scene. It is a foundational layer that facilitates data messaging     between Unity scene and the Python machine learning algorithms.     Consequently, <code>mlagents</code> depends on <code>mlagents_envs</code>.</li> <li>Unity Project that contains several   example environments that highlight the   various features of the toolkit to help you get started.</li> </ul> <p>Consequently, to install and use the ML-Agents Toolkit you will need to:</p> <ul> <li>Install Unity (2022.3 or later)</li> <li>Install Python (3.10.12 or higher)</li> <li>Clone this repository (Recommended for the latest version and bug fixes)</li> <li>Note: If you do not clone the repository, then you will not be   able to access the example environments and training configurations or the   <code>com.unity.ml-agents.extensions</code> package. Additionally, the   Getting Started Guide assumes that you have cloned the   repository.</li> <li>Install the <code>com.unity.ml-agents</code> Unity package</li> <li>Install the <code>com.unity.ml-agents.extensions</code> Unity package (Optional)</li> <li>Install the <code>mlagents-envs</code></li> <li>Install the <code>mlagents</code> Python package</li> </ul>"},{"location":"Installation/#install-unity-20223-or-later","title":"Install Unity 2022.3 or Later","text":"<p>Download and install Unity. We strongly recommend that you install Unity through the Unity Hub as it will enable you to manage multiple Unity versions.</p>"},{"location":"Installation/#install-python-31012","title":"Install Python 3.10.12","text":"<p>We recommend installing Python 3.10.12. If you are using Windows, please install the x86-64 version and not x86. If your Python environment doesn't include <code>pip3</code>, see these instructions on installing it. We also recommend using conda or mamba to manage your python virtual environments.</p>"},{"location":"Installation/#conda-python-setup","title":"Conda python setup","text":"<p>Once conda has been installed in your system, open a terminal and execute the following commands to setup a python 3.10.12 virtual environment and activate it.</p> <pre><code>conda create -n mlagents python=3.10.12 &amp;&amp; conda activate mlagents\n</code></pre>"},{"location":"Installation/#clone-the-ml-agents-toolkit-repository-recommended","title":"Clone the ML-Agents Toolkit Repository (Recommended)","text":"<p>Now that you have installed Unity and Python, you can now install the Unity and Python packages. You do not need to clone the repository to install those packages, but you may choose to clone the repository if you'd like download our example environments and training configurations to experiment with them (some of our tutorials / guides assume you have access to our example environments).</p> <p>NOTE: There are samples shipped with the Unity Package.  You only need to clone the repository if you would like to explore more examples.</p> <pre><code>git clone --branch release_21 https://github.com/Unity-Technologies/ml-agents.git\n</code></pre> <p>The <code>--branch release_21</code> option will switch to the tag of the latest stable release. Omitting that will get the <code>main</code> branch which is potentially unstable.</p>"},{"location":"Installation/#advanced-local-installation-for-development","title":"Advanced: Local Installation for Development","text":"<p>You will need to clone the repository if you plan to modify or extend the ML-Agents Toolkit for your purposes. If you plan to contribute those changes back, make sure to clone the <code>develop</code> branch (by omitting <code>--branch release_21</code> from the command above). See our Contributions Guidelines for more information on contributing to the ML-Agents Toolkit.</p>"},{"location":"Installation/#install-the-comunityml-agents-unity-package","title":"Install the <code>com.unity.ml-agents</code> Unity package","text":"<p>The Unity ML-Agents C# SDK is a Unity Package. You can install the <code>com.unity.ml-agents</code> package directly from the Package Manager registry. Please make sure you enable 'Preview Packages' in the 'Advanced' dropdown in order to find the latest Preview release of the package.</p> <p>NOTE: If you do not see the ML-Agents package listed in the Package Manager please follow the advanced installation instructions below.</p>"},{"location":"Installation/#advanced-local-installation-for-development_1","title":"Advanced: Local Installation for Development","text":"<p>You can add the local <code>com.unity.ml-agents</code> package (from the repository that you just cloned) to your project by:</p> <ol> <li>navigating to the menu <code>Window</code> -&gt; <code>Package Manager</code>.</li> <li>In the package manager window click on the <code>+</code> button on the top left of the packages list).</li> <li>Select <code>Add package from disk...</code></li> <li>Navigate into the <code>com.unity.ml-agents</code> folder.</li> <li>Select the <code>package.json</code> file.</li> </ol> <p> </p> <p>If you are going to follow the examples from our documentation, you can open the <code>Project</code> folder in Unity and start tinkering immediately.</p>"},{"location":"Installation/#install-the-comunityml-agentsextensions-unity-package-optional","title":"Install the <code>com.unity.ml-agents.extensions</code> Unity package (Optional)","text":"<p>To install the <code>com.unity.ml-agents.extensions</code> package, you need to first clone the repo and then complete a local installation similar to what was outlined in the previous Advanced: Local Installation for Development section. Complete installation steps can be found in the package documentation.</p>"},{"location":"Installation/#install-the-mlagents-python-package","title":"Install the <code>mlagents</code> Python package","text":"<p>Installing the <code>mlagents</code> Python package involves installing other Python packages that <code>mlagents</code> depends on. So you may run into installation issues if your machine has older versions of any of those dependencies already installed. Consequently, our supported path for installing <code>mlagents</code> is to leverage Python Virtual Environments. Virtual Environments provide a mechanism for isolating the dependencies for each project and are supported on Mac / Windows / Linux. We offer a dedicated guide on Virtual Environments.</p>"},{"location":"Installation/#windows-installing-pytorch","title":"(Windows) Installing PyTorch","text":"<p>On Windows, you'll have to install the PyTorch package separately prior to installing ML-Agents. Activate your virtual environment and run from the command line:</p> <pre><code>pip3 install torch~=1.13.1 -f https://download.pytorch.org/whl/torch_stable.html\n</code></pre> <p>Note that on Windows, you may also need Microsoft's Visual C++ Redistributable if you don't have it already. See the PyTorch installation guide for more installation options and versions.</p>"},{"location":"Installation/#installing-mlagents","title":"Installing <code>mlagents</code>","text":"<p>To install the <code>mlagents</code> Python package, activate your virtual environment and run from the command line:</p> <pre><code>cd /path/to/ml-agents\npython -m pip install ./ml-agents-envs\npython -m pip install ./ml-agents\n</code></pre> <p>Note that this will install <code>mlagents</code> from the cloned repository, not from the PyPi repository. If you installed this correctly, you should be able to run <code>mlagents-learn --help</code>, after which you will see the command line parameters you can use with <code>mlagents-learn</code>.</p> <p>NOTE: Since ML-Agents development has slowed, PyPi releases will be less frequent. However, you can install from PyPi by executing the following command:</p> <pre><code>python -m pip install mlagents==1.0.0\n</code></pre> <p>which will install the latest version of ML-Agents and associated dependencies available on PyPi. Note, you need to have the matching version of the Unity packages with the particular release of the python packages. You can find the release history here</p> <p>By installing the <code>mlagents</code> package, the dependencies listed in the setup.py file are also installed. These include PyTorch.</p>"},{"location":"Installation/#advanced-local-installation-for-development_2","title":"Advanced: Local Installation for Development","text":"<p>If you intend to make modifications to <code>mlagents</code> or <code>mlagents_envs</code>, you should install the packages from the cloned repository rather than from PyPi. To do this, you will need to install <code>mlagents</code> and <code>mlagents_envs</code> separately. From the repository's root directory, run:</p> <pre><code>pip3 install torch -f https://download.pytorch.org/whl/torch_stable.html\npip3 install -e ./ml-agents-envs\npip3 install -e ./ml-agents\n</code></pre> <p>Running pip with the <code>-e</code> flag will let you make changes to the Python files directly and have those reflected when you run <code>mlagents-learn</code>. It is important to install these packages in this order as the <code>mlagents</code> package depends on <code>mlagents_envs</code>, and installing it in the other order will download <code>mlagents_envs</code> from PyPi.</p>"},{"location":"Installation/#next-steps","title":"Next Steps","text":"<p>The Getting Started guide contains several short tutorials on setting up the ML-Agents Toolkit within Unity, running a pre-trained model, in addition to building and training environments.</p>"},{"location":"Installation/#help","title":"Help","text":"<p>If you run into any problems regarding ML-Agents, refer to our FAQ and our Limitations pages. If you can't find anything please submit an issue and make sure to cite relevant information on OS, Python version, and exact error message (whenever possible).</p>"},{"location":"Integrations-Match3/","title":"Match-3 with ML-Agents","text":""},{"location":"Integrations-Match3/#getting-started","title":"Getting started","text":"<p>The C# code for Match-3 exists inside of the Unity package (<code>com.unity.ml-agents</code>). The good first step would be to take a look at how we have implemented the C# code in the example Match-3 scene (located under /Project/Assets/ML-Agents/Examples/match3). Once you have some familiarity, then the next step would be to implement the C# code for Match-3 from the extensions package.</p> <p>Additionally, see below for additional technical specifications on the C# code for Match-3. Please note the Match-3 game isn't human playable as implemented and can be only played via training.</p>"},{"location":"Integrations-Match3/#technical-specifications-for-match-3-with-ml-agents","title":"Technical specifications for Match-3 with ML-Agents","text":""},{"location":"Integrations-Match3/#abstractboard-class","title":"AbstractBoard class","text":"<p>The <code>AbstractBoard</code> is the bridge between ML-Agents and your game. It allows ML-Agents to * ask your game what the current and maximum sizes (rows, columns, and potential piece types) of the board are * ask your game what the \"color\" of a cell is * ask whether the cell is a \"special\" piece type or not * ask your game whether a move is allowed * request that your game make a move</p> <p>These are handled by implementing the abstract methods of <code>AbstractBoard</code>.</p>"},{"location":"Integrations-Match3/#public-abstract-boardsize-getmaxboardsize","title":"<code>public abstract BoardSize GetMaxBoardSize()</code>","text":"<p>Returns the largest <code>BoardSize</code> that the game can use. This is used to determine the sizes of observations and sensors, so don't make it larger than necessary.</p>"},{"location":"Integrations-Match3/#public-virtual-boardsize-getcurrentboardsize","title":"<code>public virtual BoardSize GetCurrentBoardSize()</code>","text":"<p>Returns the current size of the board. Each field on this BoardSize must be less than or equal to the corresponding field returned by <code>GetMaxBoardSize()</code>. This method is optional; if your always use the same size board, you don't need to override it.</p> <p>If the current board size is smaller than the maximum board size, <code>GetCellType()</code> and <code>GetSpecialType()</code> will not be called for cells outside the current board size, and <code>IsValidMove</code> won't be called for moves that would go outside of the current board size.</p>"},{"location":"Integrations-Match3/#public-abstract-int-getcelltypeint-row-int-col","title":"<code>public abstract int GetCellType(int row, int col)</code>","text":"<p>Returns the \"color\" of piece at the given row and column. This should be between 0 and BoardSize.NumCellTypes-1 (inclusive). The actual order of the values doesn't matter.</p>"},{"location":"Integrations-Match3/#public-abstract-int-getspecialtypeint-row-int-col","title":"<code>public abstract int GetSpecialType(int row, int col)</code>","text":"<p>Returns the special type of the piece at the given row and column. This should be between 0 and BoardSize.NumSpecialTypes (inclusive). The actual order of the values doesn't matter.</p>"},{"location":"Integrations-Match3/#public-abstract-bool-ismovevalidmove-m","title":"<code>public abstract bool IsMoveValid(Move m)</code>","text":"<p>Check whether the particular <code>Move</code> is valid for the game. The actual results will depend on the rules of the game, but we provide the <code>SimpleIsMoveValid()</code> method that handles basic match3 rules with no special or immovable pieces.</p>"},{"location":"Integrations-Match3/#public-abstract-bool-makemovemove-m","title":"<code>public abstract bool MakeMove(Move m)</code>","text":"<p>Instruct the game to make the given move. Returns true if the move was made. Note that during training, a move that was marked as invalid may occasionally still be requested. If this happens, it is safe to do nothing and request another move.</p>"},{"location":"Integrations-Match3/#move-struct","title":"<code>Move</code> struct","text":"<p>The Move struct encapsulates a swap of two adjacent cells. You can get the number of potential moves for a board of a given size with. <code>Move.NumPotentialMoves(maxBoardSize)</code>. There are two helper functions to create a new <code>Move</code>: * <code>public static Move FromMoveIndex(int moveIndex, BoardSize maxBoardSize)</code> can be used to iterate over all potential moves for the board by looping from 0 to <code>Move.NumPotentialMoves()</code> * <code>public static Move FromPositionAndDirection(int row, int col, Direction dir, BoardSize maxBoardSize)</code> creates a <code>Move</code> from a row, column, and direction (and board size).</p>"},{"location":"Integrations-Match3/#boardsize-struct","title":"<code>BoardSize</code> struct","text":"<p>Describes the \"size\" of the board, including the number of potential piece types that the board can have. This is returned by the AbstractBoard.GetMaxBoardSize() and GetCurrentBoardSize() methods.</p>"},{"location":"Integrations-Match3/#match3sensor-and-match3sensorcomponent-classes","title":"<code>Match3Sensor</code> and <code>Match3SensorComponent</code> classes","text":"<p>The <code>Match3Sensor</code> generates observations about the state using the <code>AbstractBoard</code> interface. You can choose whether to use vector or \"visual\" observations; in theory, visual observations should perform better because they are 2-dimensional like the board, but we need to experiment more on this.</p> <p>A <code>Match3SensorComponent</code> generates <code>Match3Sensor</code>s (the exact number of sensors depends on your configuration) at runtime, and should be added to the same GameObject as your <code>Agent</code> implementation. You do not need to write any additional code to use them.</p>"},{"location":"Integrations-Match3/#match3actuator-and-match3actuatorcomponent-classes","title":"<code>Match3Actuator</code> and <code>Match3ActuatorComponent</code> classes","text":"<p>The <code>Match3Actuator</code> converts actions from training or inference into a <code>Move</code> that is sent to<code>AbstractBoard.MakeMove()</code> It also checks <code>AbstractBoard.IsMoveValid</code> for each potential move and uses this to set the action mask for Agent.</p> <p>A <code>Match3ActuatorComponent</code> generates a <code>Match3Actuator</code> at runtime, and should be added to the same GameObject as your <code>Agent</code> implementation.  You do not need to write any additional code to use them.</p>"},{"location":"Integrations-Match3/#setting-up-match-3-simulation","title":"Setting up Match-3 simulation","text":"<ul> <li>Implement the <code>AbstractBoard</code> methods to integrate with your game.</li> <li>Give the <code>Agent</code> rewards when it does what you want it to (match multiple pieces in a row, clears pieces of a certain type, etc).</li> <li>Add the <code>Agent</code>, <code>AbstractBoard</code> implementation, <code>Match3SensorComponent</code>, and <code>Match3ActuatorComponent</code> to the same <code>GameObject</code>.</li> <li>Call <code>Agent.RequestDecision()</code> when you're ready for the <code>Agent</code> to make a move on the next <code>Academy</code> step. During the next <code>Academy</code> step, the <code>MakeMove()</code> method on the board will be called.</li> </ul>"},{"location":"Integrations-Match3/#implementation-details","title":"Implementation Details","text":""},{"location":"Integrations-Match3/#action-space","title":"Action Space","text":"<p>The indexing for actions is the same as described in Human Like Playtesting with Deep Learning (for example, Figure 2b). The horizontal moves are enumerated first, then the vertical ones. </p>"},{"location":"Integrations/","title":"Game Integrations","text":"<p>ML-Agents provides some utilities to make it easier to integrate with some common genres of games.</p>"},{"location":"Integrations/#match-3","title":"Match-3","text":"<p>The Match-3 integration provides an abstraction of a match-3 game board and moves, along with a sensor to observe the game state, and an actuator to translate the ML-Agent actions into game moves.</p>"},{"location":"Integrations/#interested-in-more-game-templates","title":"Interested in more game templates?","text":"<p>Do you have a type of game you are interested for ML-Agents? If so, please post a forum issue with <code>[GAME TEMPLATE]</code> in the title.</p>"},{"location":"LICENSE/","title":"LICENSE","text":"<p>Copyright 2017-2021 Unity Technologies</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"{}\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre>"},{"location":"Learning-Environment-Create-New/","title":"Making a New Learning Environment","text":"<p>This tutorial walks through the process of creating a Unity Environment from scratch. We recommend first reading the Getting Started guide to understand the concepts presented here first in an already-built environment.</p> <p></p> <p>In this example, we will create an agent capable of controlling a ball on a platform. We will then train the agent to roll the ball toward the cube while avoiding falling off the platform.</p>"},{"location":"Learning-Environment-Create-New/#overview","title":"Overview","text":"<p>Using the ML-Agents Toolkit in a Unity project involves the following basic steps:</p> <ol> <li>Create an environment for your agents to live in. An environment can range    from a simple physical simulation containing a few objects to an entire game    or ecosystem.</li> <li>Implement your Agent subclasses. An Agent subclass defines the code an Agent    uses to observe its environment, to carry out assigned actions, and to    calculate the rewards used for reinforcement training. You can also implement    optional methods to reset the Agent when it has finished or failed its task.</li> <li>Add your Agent subclasses to appropriate GameObjects, typically, the object    in the scene that represents the Agent in the simulation.</li> </ol> <p>Note: If you are unfamiliar with Unity, refer to the Unity manual if an Editor task isn't explained sufficiently in this tutorial.</p> <p>If you haven't already, follow the installation instructions.</p>"},{"location":"Learning-Environment-Create-New/#set-up-the-unity-project","title":"Set Up the Unity Project","text":"<p>The first task to accomplish is simply creating a new Unity project and importing the ML-Agents assets into it:</p> <ol> <li>Launch Unity Hub and create a new 3D project named \"RollerBall\".</li> <li>Add the ML-Agents Unity package    to your project.</li> </ol> <p>Your Unity Project window should contain the following assets:</p> <p></p>"},{"location":"Learning-Environment-Create-New/#create-the-environment","title":"Create the Environment","text":"<p>Next, we will create a very simple scene to act as our learning environment. The \"physical\" components of the environment include a Plane to act as the floor for the Agent to move around on, a Cube to act as the goal or target for the agent to seek, and a Sphere to represent the Agent itself.</p>"},{"location":"Learning-Environment-Create-New/#create-the-floor-plane","title":"Create the Floor Plane","text":"<ol> <li>Right click in Hierarchy window, select 3D Object &gt; Plane.</li> <li>Name the GameObject \"Floor\".</li> <li>Select the Floor Plane to view its properties in the Inspector window.</li> <li>Set Transform to Position = <code>(0, 0, 0)</code>, Rotation = <code>(0, 0, 0)</code>, Scale =    <code>(1, 1, 1)</code>.</li> </ol>"},{"location":"Learning-Environment-Create-New/#add-the-target-cube","title":"Add the Target Cube","text":"<ol> <li>Right click in Hierarchy window, select 3D Object &gt; Cube.</li> <li>Name the GameObject \"Target\".</li> <li>Select the Target Cube to view its properties in the Inspector window.</li> <li>Set Transform to Position = <code>(3, 0.5, 3)</code>, Rotation = <code>(0, 0, 0)</code>, Scale =    <code>(1, 1, 1)</code>.</li> </ol>"},{"location":"Learning-Environment-Create-New/#add-the-agent-sphere","title":"Add the Agent Sphere","text":"<ol> <li>Right click in Hierarchy window, select 3D Object &gt; Sphere.</li> <li>Name the GameObject \"RollerAgent\".</li> <li>Select the RollerAgent Sphere to view its properties in the Inspector window.</li> <li>Set Transform to Position = <code>(0, 0.5, 0)</code>, Rotation = <code>(0, 0, 0)</code>, Scale =    <code>(1, 1, 1)</code>.</li> <li>Click Add Component.</li> <li>Add the <code>Rigidbody</code> component to the Sphere.</li> </ol>"},{"location":"Learning-Environment-Create-New/#group-into-training-area","title":"Group into Training Area","text":"<p>Group the floor, target and agent under a single, empty, GameObject. This will simplify some of our subsequent steps.</p> <p>To do so:</p> <ol> <li>Right-click on your Project Hierarchy and create a new empty GameObject. Name    it TrainingArea.</li> <li>Reset the TrainingArea\u2019s Transform so that it is at <code>(0,0,0)</code> with Rotation    <code>(0,0,0)</code> and Scale <code>(1,1,1)</code>.</li> <li>Drag the Floor, Target, and RollerAgent GameObjects in the Hierarchy into the    TrainingArea GameObject.</li> </ol> <p></p>"},{"location":"Learning-Environment-Create-New/#implement-an-agent","title":"Implement an Agent","text":"<p>To create the Agent Script:</p> <ol> <li>Select the RollerAgent GameObject to view it in the Inspector window.</li> <li>Click Add Component.</li> <li>Click New Script in the list of components (at the bottom).</li> <li>Name the script \"RollerAgent\".</li> <li>Click Create and Add.</li> </ol> <p>Then, edit the new <code>RollerAgent</code> script:</p> <ol> <li>In the Unity Project window, double-click the <code>RollerAgent</code> script to open it    in your code editor.</li> <li>Import ML-Agent package by adding</li> </ol> <p><code>csharp    using Unity.MLAgents;    using Unity.MLAgents.Sensors;    using Unity.MLAgents.Actuators;</code>    then change the base class from <code>MonoBehaviour</code> to <code>Agent</code>. 1. Delete <code>Update()</code> since we are not using it, but keep <code>Start()</code>.</p> <p>So far, these are the basic steps that you would use to add ML-Agents to any Unity project. Next, we will add the logic that will let our Agent learn to roll to the cube using reinforcement learning. More specifically, we will need to extend three methods from the <code>Agent</code> base class:</p> <ul> <li><code>OnEpisodeBegin()</code></li> <li><code>CollectObservations(VectorSensor sensor)</code></li> <li><code>OnActionReceived(ActionBuffers actionBuffers)</code></li> </ul> <p>We overview each of these in more detail in the dedicated subsections below.</p>"},{"location":"Learning-Environment-Create-New/#initialization-and-resetting-the-agent","title":"Initialization and Resetting the Agent","text":"<p>The process of training in the ML-Agents Toolkit involves running episodes where the Agent (Sphere) attempts to solve the task. Each episode lasts until the Agents solves the task (i.e. reaches the cube), fails (rolls off the platform) or times out (takes too long to solve or fail at the task). At the start of each episode, <code>OnEpisodeBegin()</code> is called to set-up the environment for a new episode. Typically the scene is initialized in a random manner to enable the agent to learn to solve the task under a variety of conditions.</p> <p>In this example, each time the Agent (Sphere) reaches its target (Cube), the episode ends and the target (Cube) is moved to a new random location; and if the Agent rolls off the platform, it will be put back onto the floor. These are all handled in <code>OnEpisodeBegin()</code>.</p> <p>To move the target (Cube), we need a reference to its Transform (which stores a GameObject's position, orientation and scale in the 3D world). To get this reference, add a public field of type <code>Transform</code> to the RollerAgent class. Public fields of a component in Unity get displayed in the Inspector window, allowing you to choose which GameObject to use as the target in the Unity Editor.</p> <p>To reset the Agent's velocity (and later to apply force to move the agent) we need a reference to the Rigidbody component. A Rigidbody is Unity's primary element for physics simulation. (See Physics for full documentation of Unity physics.) Since the Rigidbody component is on the same GameObject as our Agent script, the best way to get this reference is using <code>GameObject.GetComponent&lt;T&gt;()</code>, which we can call in our script's <code>Start()</code> method.</p> <p>So far, our RollerAgent script looks like:</p> <pre><code>using System.Collections.Generic;\nusing UnityEngine;\nusing Unity.MLAgents;\nusing Unity.MLAgents.Sensors;\n\npublic class RollerAgent : Agent\n{\n    Rigidbody rBody;\n    void Start () {\n        rBody = GetComponent&lt;Rigidbody&gt;();\n    }\n\n    public Transform Target;\n    public override void OnEpisodeBegin()\n    {\n       // If the Agent fell, zero its momentum\n        if (this.transform.localPosition.y &lt; 0)\n        {\n            this.rBody.angularVelocity = Vector3.zero;\n            this.rBody.velocity = Vector3.zero;\n            this.transform.localPosition = new Vector3( 0, 0.5f, 0);\n        }\n\n        // Move the target to a new spot\n        Target.localPosition = new Vector3(Random.value * 8 - 4,\n                                           0.5f,\n                                           Random.value * 8 - 4);\n    }\n}\n</code></pre> <p>Next, let's implement the <code>Agent.CollectObservations(VectorSensor sensor)</code> method.</p>"},{"location":"Learning-Environment-Create-New/#observing-the-environment","title":"Observing the Environment","text":"<p>The Agent sends the information we collect to the Brain, which uses it to make a decision. When you train the Agent (or use a trained model), the data is fed into a neural network as a feature vector. For an Agent to successfully learn a task, we need to provide the correct information. A good rule of thumb for deciding what information to collect is to consider what you would need to calculate an analytical solution to the problem.</p> <p>In our case, the information our Agent collects includes the position of the target, the position of the agent itself, and the velocity of the agent. This helps the Agent learn to control its speed so it doesn't overshoot the target and roll off the platform. In total, the agent observation contains 8 values as implemented below:</p> <pre><code>public override void CollectObservations(VectorSensor sensor)\n{\n    // Target and Agent positions\n    sensor.AddObservation(Target.localPosition);\n    sensor.AddObservation(this.transform.localPosition);\n\n    // Agent velocity\n    sensor.AddObservation(rBody.velocity.x);\n    sensor.AddObservation(rBody.velocity.z);\n}\n</code></pre>"},{"location":"Learning-Environment-Create-New/#taking-actions-and-assigning-rewards","title":"Taking Actions and Assigning Rewards","text":"<p>The final part of the Agent code is the <code>Agent.OnActionReceived()</code> method, which receives actions and assigns the reward.</p>"},{"location":"Learning-Environment-Create-New/#actions","title":"Actions","text":"<p>To solve the task of moving towards the target, the Agent (Sphere) needs to be able to move in the <code>x</code> and <code>z</code> directions. As such, the agent needs 2 actions: the first determines the force applied along the x-axis; and the second determines the force applied along the z-axis. (If we allowed the Agent to move in three dimensions, then we would need a third action.)</p> <p>The RollerAgent applies the values from the <code>action[]</code> array to its Rigidbody component <code>rBody</code>, using <code>Rigidbody.AddForce()</code>:</p> <pre><code>Vector3 controlSignal = Vector3.zero;\ncontrolSignal.x = action[0];\ncontrolSignal.z = action[1];\nrBody.AddForce(controlSignal * forceMultiplier);\n</code></pre>"},{"location":"Learning-Environment-Create-New/#rewards","title":"Rewards","text":"<p>Reinforcement learning requires rewards to signal which decisions are good and which are bad. The learning algorithm uses the rewards to determine whether it is giving the Agent the optimal actions. You want to reward an Agent for completing the assigned task. In this case, the Agent is given a reward of 1.0 for reaching the Target cube.</p> <p>Rewards are assigned in <code>OnActionReceived()</code>. The RollerAgent calculates the distance to detect when it reaches the target. When it does, the code calls <code>Agent.SetReward()</code> to assign a reward of 1.0 and marks the agent as finished by calling <code>EndEpisode()</code> on the Agent.</p> <pre><code>float distanceToTarget = Vector3.Distance(this.transform.localPosition, Target.localPosition);\n// Reached target\nif (distanceToTarget &lt; 1.42f)\n{\n    SetReward(1.0f);\n    EndEpisode();\n}\n</code></pre> <p>Finally, if the Agent falls off the platform, end the episode so that it can reset itself:</p> <pre><code>// Fell off platform\nif (this.transform.localPosition.y &lt; 0)\n{\n    EndEpisode();\n}\n</code></pre>"},{"location":"Learning-Environment-Create-New/#onactionreceived","title":"OnActionReceived()","text":"<p>With the action and reward logic outlined above, the final version of <code>OnActionReceived()</code> looks like:</p> <pre><code>public float forceMultiplier = 10;\npublic override void OnActionReceived(ActionBuffers actionBuffers)\n{\n    // Actions, size = 2\n    Vector3 controlSignal = Vector3.zero;\n    controlSignal.x = actionBuffers.ContinuousActions[0];\n    controlSignal.z = actionBuffers.ContinuousActions[1];\n    rBody.AddForce(controlSignal * forceMultiplier);\n\n    // Rewards\n    float distanceToTarget = Vector3.Distance(this.transform.localPosition, Target.localPosition);\n\n    // Reached target\n    if (distanceToTarget &lt; 1.42f)\n    {\n        SetReward(1.0f);\n        EndEpisode();\n    }\n\n    // Fell off platform\n    else if (this.transform.localPosition.y &lt; 0)\n    {\n        EndEpisode();\n    }\n}\n</code></pre> <p>Note the <code>forceMultiplier</code> class variable is defined before the method definition. Since <code>forceMultiplier</code> is public, you can set the value from the Inspector window.</p>"},{"location":"Learning-Environment-Create-New/#final-agent-setup-in-editor","title":"Final Agent Setup in Editor","text":"<p>Now that all the GameObjects and ML-Agent components are in place, it is time to connect everything together in the Unity Editor. This involves adding and setting some of the Agent Component's properties so that they are compatible with our Agent script.</p> <ol> <li>Select the RollerAgent GameObject to show its properties in the Inspector    window.</li> <li>Drag the Target GameObject in the Hierarchy into the <code>Target</code> field in RollerAgent Script.</li> <li>Add a <code>Decision Requester</code> script with the Add Component button.    Set the Decision Period to <code>10</code>. For more information on decisions,    see the Agent documentation</li> <li>Add a <code>Behavior Parameters</code> script with the Add Component button.    Set the Behavior Parameters of the Agent to the following:</li> <li><code>Behavior Name</code>: RollerBall</li> <li><code>Vector Observation</code> &gt; <code>Space Size</code> = 8</li> <li><code>Actions</code> &gt; <code>Continuous Actions</code> = 2</li> </ol> <p>In the inspector, the <code>RollerAgent</code> should look like this now:</p> <p></p> <p>Now you are ready to test the environment before training.</p>"},{"location":"Learning-Environment-Create-New/#testing-the-environment","title":"Testing the Environment","text":"<p>It is always a good idea to first test your environment by controlling the Agent using the keyboard. To do so, you will need to extend the <code>Heuristic()</code> method in the <code>RollerAgent</code> class. For our example, the heuristic will generate an action corresponding to the values of the \"Horizontal\" and \"Vertical\" input axis (which correspond to the keyboard arrow keys):</p> <pre><code>public override void Heuristic(in ActionBuffers actionsOut)\n{\n    var continuousActionsOut = actionsOut.ContinuousActions;\n    continuousActionsOut[0] = Input.GetAxis(\"Horizontal\");\n    continuousActionsOut[1] = Input.GetAxis(\"Vertical\");\n}\n</code></pre> <p>In order for the Agent to use the Heuristic, You will need to set the <code>Behavior Type</code> to <code>Heuristic Only</code> in the <code>Behavior Parameters</code> of the RollerAgent.</p> <p>Press Play to run the scene and use the arrows keys to move the Agent around the platform. Make sure that there are no errors displayed in the Unity Editor Console window and that the Agent resets when it reaches its target or falls from the platform.</p>"},{"location":"Learning-Environment-Create-New/#training-the-environment","title":"Training the Environment","text":"<p>The process is the same as described in the Getting Started Guide.</p> <p>The hyperparameters for training are specified in a configuration file that you pass to the <code>mlagents-learn</code> program. Create a new <code>rollerball_config.yaml</code> file under <code>config/</code> and include the following hyperparameter values:</p> <pre><code>behaviors:\n  RollerBall:\n    trainer_type: ppo\n    hyperparameters:\n      batch_size: 10\n      buffer_size: 100\n      learning_rate: 3.0e-4\n      beta: 5.0e-4\n      epsilon: 0.2\n      lambd: 0.99\n      num_epoch: 3\n      learning_rate_schedule: linear\n      beta_schedule: constant\n      epsilon_schedule: linear\n    network_settings:\n      normalize: false\n      hidden_units: 128\n      num_layers: 2\n    reward_signals:\n      extrinsic:\n        gamma: 0.99\n        strength: 1.0\n    max_steps: 500000\n    time_horizon: 64\n    summary_freq: 10000\n</code></pre> <p>Hyperparameters are explained in the training configuration file documentation</p> <p>Since this example creates a very simple training environment with only a few inputs and outputs, using small batch and buffer sizes speeds up the training considerably. However, if you add more complexity to the environment or change the reward or observation functions, you might also find that training performs better with different hyperparameter values. In addition to setting these hyperparameter values, the Agent DecisionFrequency parameter has a large effect on training time and success. A larger value reduces the number of decisions the training algorithm has to consider and, in this simple environment, speeds up training.</p> <p>To train your agent, run the following command before pressing Play in the Editor:</p> <pre><code>mlagents-learn config/rollerball_config.yaml --run-id=RollerBall\n</code></pre> <p>To monitor the statistics of Agent performance during training, use TensorBoard.</p> <p></p> <p>In particular, the cumulative_reward and value_estimate statistics show how well the Agent is achieving the task. In this example, the maximum reward an Agent can earn is 1.0, so these statistics approach that value when the Agent has successfully solved the problem.</p>"},{"location":"Learning-Environment-Create-New/#optional-multiple-training-areas-within-the-same-scene","title":"Optional: Multiple Training Areas within the Same Scene","text":"<p>In many of the example environments, many copies of the training area are instantiated in the scene. This generally speeds up training, allowing the environment to gather many experiences in parallel. This can be achieved simply by instantiating many Agents with the same <code>Behavior Name</code>. Note that we've already simplified our transition to using multiple areas by creating the <code>TrainingArea</code> GameObject and relying on local positions in <code>RollerAgent.cs</code>. Use the following steps to parallelize your RollerBall environment:</p> <ol> <li>Drag the TrainingArea GameObject, along with its attached GameObjects, into    your Assets browser, turning it into a prefab.</li> <li>You can now instantiate copies of the TrainingArea prefab. Drag them into    your scene, positioning them so that they do not overlap.</li> </ol> <p>Alternatively, you can use the <code>TrainingAreaReplicator</code> to replicate training areas. Use the following steps:</p> <ol> <li>Create a new empty Game Object in the scene.</li> <li>Click on the new object and add a TrainingAreaReplicator component to the empty Game Object through the inspector.</li> <li>Drag the training area to <code>Base Area</code> in the Training Area Replicator.</li> <li>Specify the number of areas to replicate and the separation between areas.</li> <li>Hit play and the areas will be replicated automatically!</li> </ol>"},{"location":"Learning-Environment-Create-New/#optional-training-using-concurrent-unity-instances","title":"Optional: Training Using Concurrent Unity Instances","text":"<p>Another level of parallelization comes by training using concurrent Unity instances. For example,</p> <pre><code>mlagents-learn config/rollerball_config.yaml --run-id=RollerBall --num-envs=2\n</code></pre> <p>will start ML Agents training with two environment instances. Combining multiple training areas within the same scene, with concurrent Unity instances, effectively gives you two levels of parallelism to speed up training. The command line option <code>--num-envs=&lt;n&gt;</code> controls the number of concurrent Unity instances that are executed in parallel during training.</p>"},{"location":"Learning-Environment-Design-Agents/","title":"Agents","text":"<p>Table of Contents:</p> <ul> <li>Decisions</li> <li>Observations and Sensors</li> <li>Generating Observations<ul> <li>Agent.CollectObservations()</li> <li>Observable Fields and Properties</li> <li>ISensor interface and SensorComponents</li> </ul> </li> <li>Vector Observations<ul> <li>One-hot encoding categorical information</li> <li>Normalization</li> <li>Stacking</li> <li>Vector Observation Summary &amp; Best Practices</li> </ul> </li> <li>Visual Observations<ul> <li>Visual Observation Summary &amp; Best Practices</li> </ul> </li> <li>Raycast Observations<ul> <li>RayCast Observation Summary &amp; Best Practices</li> </ul> </li> <li>Variable Length Observations<ul> <li>Variable Length Observation Summary &amp; Best Practices</li> </ul> </li> <li>Goal Signal<ul> <li>Goal Signal Summary &amp; Best Practices</li> </ul> </li> <li>Actions and Actuators</li> <li>Continuous Actions</li> <li>Discrete Actions<ul> <li>Masking Discrete Actions</li> </ul> </li> <li>Actions Summary &amp; Best Practices</li> <li>Rewards</li> <li>Examples</li> <li>Rewards Summary &amp; Best Practices</li> <li>Agent Properties</li> <li>Destroying an Agent</li> <li>Defining Multi-agent Scenarios</li> <li>Teams for Adversarial Scenarios</li> <li>Groups for Cooperative Scenarios</li> <li>Recording Demonstrations</li> </ul> <p>An agent is an entity that can observe its environment, decide on the best course of action using those observations, and execute those actions within its environment. Agents can be created in Unity by extending the <code>Agent</code> class. The most important aspects of creating agents that can successfully learn are the observations the agent collects, and the reward you assign to estimate the value of the agent's current state toward accomplishing its tasks.</p> <p>An Agent passes its observations to its Policy. The Policy then makes a decision and passes the chosen action back to the agent. Your agent code must execute the action, for example, move the agent in one direction or another. In order to train an agent using reinforcement learning, your agent must calculate a reward value at each action. The reward is used to discover the optimal decision-making policy.</p> <p>The <code>Policy</code> class abstracts out the decision making logic from the Agent itself so that you can use the same Policy in multiple Agents. How a Policy makes its decisions depends on the <code>Behavior Parameters</code> associated with the agent. If you set <code>Behavior Type</code> to <code>Heuristic Only</code>, the Agent will use its <code>Heuristic()</code> method to make decisions which can allow you to control the Agent manually or write your own Policy. If the Agent has a <code>Model</code> file, its Policy will use the neural network <code>Model</code> to take decisions.</p> <p>When you create an Agent, you should usually extend the base Agent class. This includes implementing the following methods:</p> <ul> <li><code>Agent.OnEpisodeBegin()</code> \u2014 Called at the beginning of an Agent's episode,   including at the beginning of the simulation.</li> <li><code>Agent.CollectObservations(VectorSensor sensor)</code> \u2014 Called every step that the Agent   requests a decision. This is one possible way for collecting the Agent's   observations of the environment; see Generating Observations   below for more options.</li> <li><code>Agent.OnActionReceived()</code> \u2014 Called every time the Agent receives an action to   take. Receives the action chosen by the Agent. It is also common to assign a   reward in this method.</li> <li><code>Agent.Heuristic()</code> - When the <code>Behavior Type</code> is set to <code>Heuristic Only</code> in   the Behavior Parameters of the Agent, the Agent will use the <code>Heuristic()</code>   method to generate the actions of the Agent. As such, the <code>Heuristic()</code> method   writes to the array of floats provided to the Heuristic method as argument.   Note: Do not create a new float array of action in the <code>Heuristic()</code> method,   as this will prevent writing floats to the original action array.</li> </ul> <p>As a concrete example, here is how the Ball3DAgent class implements these methods:</p> <ul> <li><code>Agent.OnEpisodeBegin()</code> \u2014 Resets the agent cube and ball to their starting   positions. The function randomizes the reset values so that the training   generalizes to more than a specific starting position and agent cube   orientation.</li> <li><code>Agent.CollectObservations(VectorSensor sensor)</code> \u2014 Adds information about the   orientation of the agent cube, the ball velocity, and the relative position   between the ball and the cube. Since the  <code>CollectObservations()</code>   method calls <code>VectorSensor.AddObservation()</code> such that vector size adds up to 8,   the Behavior Parameters of the Agent are set with vector observation space   with a state size of 8.</li> <li><code>Agent.OnActionReceived()</code> \u2014 The action results   in a small change in the agent cube's rotation at each step. In this example,   an Agent receives a small positive reward for each step it keeps the ball on the   agent cube's head and a larger, negative reward for dropping the ball. An   Agent's episode is also ended when it drops the ball so that it will reset   with a new ball for the next simulation step.</li> <li><code>Agent.Heuristic()</code> - Converts the keyboard inputs into actions.</li> </ul>"},{"location":"Learning-Environment-Design-Agents/#decisions","title":"Decisions","text":"<p>The observation-decision-action-reward cycle repeats each time the Agent request a decision. Agents will request a decision when <code>Agent.RequestDecision()</code> is called. If you need the Agent to request decisions on its own at regular intervals, add a <code>Decision Requester</code> component to the Agent's GameObject. Making decisions at regular step intervals is generally most appropriate for physics-based simulations. For example, an agent in a robotic simulator that must provide fine-control of joint torques should make its decisions every step of the simulation. In games such as real-time strategy, where many agents make their decisions at regular intervals, the decision timing for each agent can be staggered by setting the <code>DecisionStep</code> parameter in the <code>Decision Requester</code> component for each agent. On the other hand, an agent that only needs to make decisions when certain game or simulation events occur, such as in a turn-based game, should call <code>Agent.RequestDecision()</code> manually.</p>"},{"location":"Learning-Environment-Design-Agents/#observations-and-sensors","title":"Observations and Sensors","text":"<p>In order for an agent to learn, the observations should include all the information an agent needs to accomplish its task. Without sufficient and relevant information, an agent may learn poorly or may not learn at all. A reasonable approach for determining what information should be included is to consider what you would need to calculate an analytical solution to the problem, or what you would expect a human to be able to use to solve the problem.</p>"},{"location":"Learning-Environment-Design-Agents/#generating-observations","title":"Generating Observations","text":"<p>ML-Agents provides multiple ways for an Agent to make observations:   1. Overriding the <code>Agent.CollectObservations()</code> method and passing the     observations to the provided <code>VectorSensor</code>.   1. Adding the <code>[Observable]</code> attribute to fields and properties on the Agent.   1. Implementing the <code>ISensor</code> interface, using a <code>SensorComponent</code> attached to   the Agent to create the <code>ISensor</code>.</p>"},{"location":"Learning-Environment-Design-Agents/#agentcollectobservations","title":"Agent.CollectObservations()","text":"<p>Agent.CollectObservations() is best used for aspects of the environment which are numerical and non-visual. The Policy class calls the <code>CollectObservations(VectorSensor sensor)</code> method of each Agent. Your implementation of this function must call <code>VectorSensor.AddObservation</code> to add vector observations.</p> <p>The <code>VectorSensor.AddObservation</code> method provides a number of overloads for adding common types of data to your observation vector. You can add Integers and booleans directly to the observation vector, as well as some common Unity data types such as <code>Vector2</code>, <code>Vector3</code>, and <code>Quaternion</code>.</p> <p>For examples of various state observation functions, you can look at the example environments included in the ML-Agents SDK. For instance, the 3DBall example uses the rotation of the platform, the relative position of the ball, and the velocity of the ball as its state observation.</p> <pre><code>public GameObject ball;\n\npublic override void CollectObservations(VectorSensor sensor)\n{\n    // Orientation of the cube (2 floats)\n    sensor.AddObservation(gameObject.transform.rotation.z);\n    sensor.AddObservation(gameObject.transform.rotation.x);\n    // Relative position of the ball to the cube (3 floats)\n    sensor.AddObservation(ball.transform.position - gameObject.transform.position);\n    // Velocity of the ball (3 floats)\n    sensor.AddObservation(m_BallRb.velocity);\n    // 8 floats total\n}\n</code></pre> <p>As an experiment, you can remove the velocity components from the observation and retrain the 3DBall agent. While it will learn to balance the ball reasonably well, the performance of the agent without using velocity is noticeably worse.</p> <p>The observations passed to <code>VectorSensor.AddObservation()</code> must always contain the same number of elements must always be in the same order. If the number of observed entities in an environment can vary, you can pad the calls with zeros for any missing entities in a specific observation, or you can limit an agent's observations to a fixed subset. For example, instead of observing every enemy in an environment, you could only observe the closest five.</p> <p>Additionally, when you set up an Agent's <code>Behavior Parameters</code> in the Unity Editor, you must set the Vector Observations &gt; Space Size to equal the number of floats that are written by <code>CollectObservations()</code>.</p>"},{"location":"Learning-Environment-Design-Agents/#observable-fields-and-properties","title":"Observable Fields and Properties","text":"<p>Another approach is to define the relevant observations as fields or properties on your Agent class, and annotate them with an <code>ObservableAttribute</code>. For example, in the Ball3DHardAgent, the difference between positions could be observed by adding a property to the Agent:</p> <pre><code>using Unity.MLAgents.Sensors.Reflection;\n\npublic class Ball3DHardAgent : Agent {\n\n    [Observable(numStackedObservations: 9)]\n    Vector3 PositionDelta\n    {\n        get\n        {\n            return ball.transform.position - gameObject.transform.position;\n        }\n    }\n}\n</code></pre> <p><code>ObservableAttribute</code> currently supports most basic types (e.g. floats, ints, bools), as well as <code>Vector2</code>, <code>Vector3</code>, <code>Vector4</code>, <code>Quaternion</code>, and enums.</p> <p>The behavior of <code>ObservableAttribute</code>s are controlled by the \"Observable Attribute Handling\" in the Agent's <code>Behavior Parameters</code>. The possible values for this are:  * Ignore (default) - All ObservableAttributes on the Agent will be ignored.   If there are no ObservableAttributes on the Agent, this will result in the   fastest  initialization time.  * Exclude Inherited - Only members on the declared class will be examined;   members that are inherited are ignored. This is a reasonable tradeoff between   performance and flexibility.  * Examine All All members on the class will be examined. This can lead to   slower startup times.</p> <p>\"Exclude Inherited\" is generally sufficient, but if your Agent inherits from another Agent implementation that has Observable members, you will need to use \"Examine All\".</p> <p>Internally, ObservableAttribute uses reflection to determine which members of the Agent have ObservableAttributes, and also uses reflection to access the fields or invoke the properties at runtime. This may be slower than using CollectObservations or an ISensor, although this might not be enough to noticeably affect performance.</p> <p>NOTE: you do not need to adjust the Space Size in the Agent's <code>Behavior Parameters</code> when you add <code>[Observable]</code> fields or properties to an Agent, since their size can be computed before they are used.</p>"},{"location":"Learning-Environment-Design-Agents/#isensor-interface-and-sensorcomponents","title":"ISensor interface and SensorComponents","text":"<p>The <code>ISensor</code> interface is generally intended for advanced users. The <code>Write()</code> method is used to actually generate the observation, but some other methods such as returning the shape of the observations must also be implemented.</p> <p>The <code>SensorComponent</code> abstract class is used to create the actual <code>ISensor</code> at runtime. It must be attached to the same <code>GameObject</code> as the <code>Agent</code>, or to a child <code>GameObject</code>.</p> <p>There are several SensorComponents provided in the API, including: - <code>CameraSensorComponent</code> - Uses images from a <code>Camera</code> as observations. - <code>RenderTextureSensorComponent</code> - Uses the content of a <code>RenderTexture</code> as observations. - <code>RayPerceptionSensorComponent</code> - Uses the information from set of ray casts as observations. - <code>Match3SensorComponent</code> - Uses the board of a Match-3 game as observations. - <code>GridSensorComponent</code> - Uses a set of box queries in a grid shape as observations.</p> <p>NOTE: you do not need to adjust the Space Size in the Agent's <code>Behavior Parameters</code> when using <code>SensorComponents</code>s.</p> <p>Internally, both <code>Agent.CollectObservations</code> and <code>[Observable]</code> attribute use an ISensors to write observations, although this is mostly abstracted from the user.</p>"},{"location":"Learning-Environment-Design-Agents/#vector-observations","title":"Vector Observations","text":"<p>Both <code>Agent.CollectObservations()</code> and <code>ObservableAttribute</code>s produce vector observations, which are represented at lists of <code>float</code>s. <code>ISensor</code>s can produce both vector observations and visual observations, which are multi-dimensional arrays of floats.</p> <p>Below are some additional considerations when dealing with vector observations:</p>"},{"location":"Learning-Environment-Design-Agents/#one-hot-encoding-categorical-information","title":"One-hot encoding categorical information","text":"<p>Type enumerations should be encoded in the one-hot style. That is, add an element to the feature vector for each element of enumeration, setting the element representing the observed member to one and set the rest to zero. For example, if your enumeration contains [Sword, Shield, Bow] and the agent observes that the current item is a Bow, you would add the elements: 0, 0, 1 to the feature vector. The following code example illustrates how to add.</p> <pre><code>enum ItemType { Sword, Shield, Bow, LastItem }\npublic override void CollectObservations(VectorSensor sensor)\n{\n    for (int ci = 0; ci &lt; (int)ItemType.LastItem; ci++)\n    {\n        sensor.AddObservation((int)currentItem == ci ? 1.0f : 0.0f);\n    }\n}\n</code></pre> <p><code>VectorSensor</code> also provides a two-argument function <code>AddOneHotObservation()</code> as a shortcut for one-hot style observations. The following example is identical to the previous one.</p> <pre><code>enum ItemType { Sword, Shield, Bow, LastItem }\nconst int NUM_ITEM_TYPES = (int)ItemType.LastItem + 1;\n\npublic override void CollectObservations(VectorSensor sensor)\n{\n    // The first argument is the selection index; the second is the\n    // number of possibilities\n    sensor.AddOneHotObservation((int)currentItem, NUM_ITEM_TYPES);\n}\n</code></pre> <p><code>ObservableAttribute</code> has built-in support for enums. Note that you don't need the <code>LastItem</code> placeholder in this case:</p> <pre><code>enum ItemType { Sword, Shield, Bow }\n\npublic class HeroAgent : Agent\n{\n    [Observable]\n    ItemType m_CurrentItem;\n}\n</code></pre>"},{"location":"Learning-Environment-Design-Agents/#normalization","title":"Normalization","text":"<p>For the best results when training, you should normalize the components of your feature vector to the range [-1, +1] or [0, 1]. When you normalize the values, the PPO neural network can often converge to a solution faster. Note that it isn't always necessary to normalize to these recommended ranges, but it is considered a best practice when using neural networks. The greater the variation in ranges between the components of your observation, the more likely that training will be affected.</p> <p>To normalize a value to [0, 1], you can use the following formula:</p> <pre><code>normalizedValue = (currentValue - minValue)/(maxValue - minValue)\n</code></pre> <p>:warning: For vectors, you should apply the above formula to each component (x, y, and z). Note that this is not the same as using the <code>Vector3.normalized</code> property or <code>Vector3.Normalize()</code> method in Unity (and similar for <code>Vector2</code>).</p> <p>Rotations and angles should also be normalized. For angles between 0 and 360 degrees, you can use the following formulas:</p> <pre><code>Quaternion rotation = transform.rotation;\nVector3 normalized = rotation.eulerAngles / 180.0f - Vector3.one;  // [-1,1]\nVector3 normalized = rotation.eulerAngles / 360.0f;  // [0,1]\n</code></pre> <p>For angles that can be outside the range [0,360], you can either reduce the angle, or, if the number of turns is significant, increase the maximum value used in your normalization formula.</p>"},{"location":"Learning-Environment-Design-Agents/#stacking","title":"Stacking","text":"<p>Stacking refers to repeating observations from previous steps as part of a larger observation. For example, consider an Agent that generates these observations in four steps</p> <pre><code>step 1: [0.1]\nstep 2: [0.2]\nstep 3: [0.3]\nstep 4: [0.4]\n</code></pre> <p>If we use a stack size of 3, the observations would instead be:</p> <pre><code>step 1: [0.1, 0.0, 0.0]\nstep 2: [0.2, 0.1, 0.0]\nstep 3: [0.3, 0.2, 0.1]\nstep 4: [0.4, 0.3, 0.2]\n</code></pre> <p>(The observations are padded with zeroes for the first <code>stackSize-1</code> steps). This is a simple way to give an Agent limited \"memory\" without the complexity of adding a recurrent neural network (RNN).</p> <p>The steps for enabling stacking depends on how you generate observations: * For Agent.CollectObservations(), set \"Stacked Vectors\" on the Agent's   <code>Behavior Parameters</code> to a value greater than 1. * For ObservableAttribute, set the <code>numStackedObservations</code> parameter in the   constructor, e.g. <code>[Observable(numStackedObservations: 2)]</code>. * For <code>ISensor</code>s, wrap them in a <code>StackingSensor</code> (which is also an <code>ISensor</code>).   Generally, this should happen in the <code>CreateSensor()</code> method of your   <code>SensorComponent</code>.</p>"},{"location":"Learning-Environment-Design-Agents/#vector-observation-summary-best-practices","title":"Vector Observation Summary &amp; Best Practices","text":"<ul> <li>Vector Observations should include all variables relevant for allowing the   agent to take the optimally informed decision, and ideally no extraneous   information.</li> <li>In cases where Vector Observations need to be remembered or compared over   time, either an RNN should be used in the model, or the <code>Stacked Vectors</code>   value in the agent GameObject's <code>Behavior Parameters</code> should be changed.</li> <li>Categorical variables such as type of object (Sword, Shield, Bow) should be   encoded in one-hot fashion (i.e. <code>3</code> -&gt; <code>0, 0, 1</code>). This can be done   automatically using the <code>AddOneHotObservation()</code> method of the <code>VectorSensor</code>,   or using <code>[Observable]</code> on an enum field or property of the Agent.</li> <li>In general, all inputs should be normalized to be in the range 0 to +1 (or -1   to 1). For example, the <code>x</code> position information of an agent where the maximum   possible value is <code>maxValue</code> should be recorded as   <code>VectorSensor.AddObservation(transform.position.x / maxValue);</code> rather than   <code>VectorSensor.AddObservation(transform.position.x);</code>.</li> <li>Positional information of relevant GameObjects should be encoded in relative   coordinates wherever possible. This is often relative to the agent position.</li> </ul>"},{"location":"Learning-Environment-Design-Agents/#visual-observations","title":"Visual Observations","text":"<p>Visual observations are generally provided to agent via either a <code>CameraSensor</code> or <code>RenderTextureSensor</code>. These collect image information and transforms it into a 3D Tensor which can be fed into the convolutional neural network (CNN) of the agent policy. For more information on CNNs, see this guide. This allows agents to learn from spatial regularities in the observation images. It is possible to use visual and vector observations with the same agent.</p> <p>Agents using visual observations can capture state of arbitrary complexity and are useful when the state is difficult to describe numerically. However, they are also typically less efficient and slower to train, and sometimes don't succeed at all as compared to vector observations. As such, they should only be used when it is not possible to properly define the problem using vector or ray-cast observations.</p> <p>Visual observations can be derived from Cameras or RenderTextures within your scene. To add a visual observation to an Agent, add either a Camera Sensor Component or RenderTextures Sensor Component to the Agent. Then drag the camera or render texture you want to add to the <code>Camera</code> or <code>RenderTexture</code> field. You can have more than one camera or render texture and even use a combination of both attached to an Agent. For each visual observation, set the width and height of the image (in pixels) and whether or not the observation is color or grayscale.</p> <p></p> <p>or</p> <p></p> <p>Each Agent that uses the same Policy must have the same number of visual observations, and they must all have the same resolutions (including whether or not they are grayscale). Additionally, each Sensor Component on an Agent must have a unique name so that they can be sorted deterministically (the name must be unique for that Agent, but multiple Agents can have a Sensor Component with the same name).</p> <p>Visual observations also support stacking, by specifying <code>Observation Stacks</code> to a value greater than 1. The visual observations from the last <code>stackSize</code> steps will be stacked on the last dimension (channel dimension).</p> <p>When using <code>RenderTexture</code> visual observations, a handy feature for debugging is adding a <code>Canvas</code>, then adding a <code>Raw Image</code> with it's texture set to the Agent's <code>RenderTexture</code>. This will render the agent observation on the game screen.</p> <p></p> <p>The GridWorld environment is an example on how to use a RenderTexture for both debugging and observation. Note that in this example, a Camera is rendered to a RenderTexture, which is then used for observations and debugging. To update the RenderTexture, the Camera must be asked to render every time a decision is requested within the game code. When using Cameras as observations directly, this is done automatically by the Agent.</p> <p></p>"},{"location":"Learning-Environment-Design-Agents/#visual-observation-summary-best-practices","title":"Visual Observation Summary &amp; Best Practices","text":"<ul> <li>To collect visual observations, attach <code>CameraSensor</code> or <code>RenderTextureSensor</code>   components to the agent GameObject.</li> <li>Visual observations should generally only be used when vector observations are   not sufficient.</li> <li>Image size should be kept as small as possible, without the loss of needed   details for decision making.</li> <li>Images should be made grayscale in situations where color information is not   needed for making informed decisions.</li> </ul>"},{"location":"Learning-Environment-Design-Agents/#raycast-observations","title":"Raycast Observations","text":"<p>Raycasts are another possible method for providing observations to an agent. This can be easily implemented by adding a <code>RayPerceptionSensorComponent3D</code> (or <code>RayPerceptionSensorComponent2D</code>) to the Agent GameObject.</p> <p>During observations, several rays (or spheres, depending on settings) are cast into the physics world, and the objects that are hit determine the observation vector that is produced.</p> <p></p> <p>Both sensor components have several settings:</p> <ul> <li>Detectable Tags A list of strings corresponding to the types of objects that   the Agent should be able to distinguish between. For example, in the WallJump   example, we use \"wall\", \"goal\", and \"block\" as the list of objects to detect.</li> <li>Rays Per Direction Determines the number of rays that are cast. One ray is   always cast forward, and this many rays are cast to the left and right.</li> <li>Max Ray Degrees The angle (in degrees) for the outermost rays. 90 degrees   corresponds to the left and right of the agent.</li> <li>Sphere Cast Radius The size of the sphere used for sphere casting. If set to   0, rays will be used instead of spheres. Rays may be more efficient,   especially in complex scenes.</li> <li>Ray Length The length of the casts</li> <li>Ray Layer Mask The LayerMask   passed to the raycast or spherecast. This can be used to ignore certain types   of objects when casting.</li> <li>Observation Stacks The number of previous results to \"stack\" with the cast   results. Note that this can be independent of the \"Stacked Vectors\" setting in   <code>Behavior Parameters</code>.</li> <li>Start Vertical Offset (3D only) The vertical offset of the ray start point.</li> <li>End Vertical Offset (3D only) The vertical offset of the ray end point.</li> <li>Alternating Ray Order Alternating is the default, it gives an order of (0,   -delta, delta, -2delta, 2delta, ..., -ndelta, ndelta). If alternating is   disabled the order is left to right (-ndelta, -(n-1)delta, ..., -delta, 0,   delta, ..., (n-1)delta, ndelta). For general usage there is no difference   but if using custom models the left-to-right layout that matches the spatial   structuring can be preferred (e.g. for processing with conv nets).</li> <li>Use Batched Raycasts (3D only) Whether to use batched raycasts. Enable to use batched raycasts and the jobs system.</li> </ul> <p>In the example image above, the Agent has two <code>RayPerceptionSensorComponent3D</code>s. Both use 3 Rays Per Direction and 90 Max Ray Degrees. One of the components had a vertical offset, so the Agent can tell whether it's clear to jump over the wall.</p> <p>The total size of the created observations is</p> <pre><code>(Observation Stacks) * (1 + 2 * Rays Per Direction) * (Num Detectable Tags + 2)\n</code></pre> <p>so the number of rays and tags should be kept as small as possible to reduce the amount of data used. Note that this is separate from the State Size defined in <code>Behavior Parameters</code>, so you don't need to worry about the formula above when setting the State Size.</p>"},{"location":"Learning-Environment-Design-Agents/#raycast-observation-summary-best-practices","title":"RayCast Observation Summary &amp; Best Practices","text":"<ul> <li>Attach <code>RayPerceptionSensorComponent3D</code> or <code>RayPerceptionSensorComponent2D</code> to   use.</li> <li>This observation type is best used when there is relevant spatial information   for the agent that doesn't require a fully rendered image to convey.</li> <li>Use as few rays and tags as necessary to solve the problem in order to improve   learning stability and agent performance.</li> <li>If you run into performance issues, try using batched raycasts by enabling the Use Batched Raycast setting.   (Only available for 3D ray perception sensors.)</li> </ul>"},{"location":"Learning-Environment-Design-Agents/#grid-observations","title":"Grid Observations","text":"<p>Grid-base observations combine the advantages of 2D spatial representation in visual observations, and the flexibility of defining detectable objects in RayCast observations. The sensor uses a set of box queries in a grid shape and gives a top-down 2D view around the agent. This can be implemented by adding a <code>GridSensorComponent</code> to the Agent GameObject.</p> <p>During observations, the sensor detects the presence of detectable objects in each cell and encode that into one-hot representation. The collected information from each cell forms a 3D tensor observation and will be fed into the convolutional neural network (CNN) of the agent policy just like visual observations.</p> <p></p> <p>The sensor component has the following settings: - Cell Scale The scale of each cell in the grid. - Grid Size Number of cells on each side of the grid. - Agent Game Object The Agent that holds the grid sensor. This is used to   disambiguate objects with the same tag as the agent so that the agent doesn't   detect itself. - Rotate With Agent Whether the grid rotates with the Agent. - Detectable Tags A list of strings corresponding to the types of objects that   the Agent should be able to distinguish between. - Collider Mask The LayerMask   passed to the collider detection. This can be used to ignore certain types   of objects. - Initial Collider Buffer Size The initial size of the Collider buffer used   in the non-allocating Physics calls for each cell. - Max Collider Buffer Size The max size of the Collider buffer used in the   non-allocating Physics calls for each cell.</p> <p>The observation for each grid cell is a one-hot encoding of the detected object. The total size of the created observations is</p> <pre><code>GridSize.x * GridSize.z * Num Detectable Tags\n</code></pre> <p>so the number of detectable tags and size of the grid should be kept as small as possible to reduce the amount of data used. This makes a trade-off between the granularity of the observation and training speed.</p> <p>To allow more variety of observations that grid sensor can capture, the <code>GridSensorComponent</code> and the underlying <code>GridSensorBase</code> also provides interfaces that can be overridden to collect customized observation from detected objects. See the doc on extending grid Sensors for more details on custom grid sensors.</p> <p>Note: The <code>GridSensor</code> only works in 3D environments and will not behave properly in 2D environments.</p>"},{"location":"Learning-Environment-Design-Agents/#grid-observation-summary-best-practices","title":"Grid Observation Summary &amp; Best Practices","text":"<ul> <li>Attach <code>GridSensorComponent</code> to use.</li> <li>This observation type is best used when there is relevant non-visual spatial information that   can be best captured in 2D representations.</li> <li>Use as small grid size and as few tags as necessary to solve the problem in order to improve   learning stability and agent performance.</li> <li>Do not use <code>GridSensor</code> in a 2D game.</li> </ul>"},{"location":"Learning-Environment-Design-Agents/#variable-length-observations","title":"Variable Length Observations","text":"<p>It is possible for agents to collect observations from a varying number of GameObjects by using a <code>BufferSensor</code>. You can add a <code>BufferSensor</code> to your Agent by adding a <code>BufferSensorComponent</code> to its GameObject. The <code>BufferSensor</code> can be useful in situations in which the Agent must pay attention to a varying number of entities (for example, a varying number of enemies or projectiles). On the trainer side, the <code>BufferSensor</code> is processed using an attention module. More information about attention mechanisms can be found here. Training or doing inference with variable length observations can be slower than using a flat vector observation. However, attention mechanisms enable solving problems that require comparative reasoning between entities in a scene such as our Sorter environment. Note that even though the <code>BufferSensor</code> can process a variable number of entities, you still need to define a maximum number of entities. This is because our network architecture requires to know what the shape of the observations will be. If fewer entities are observed than the maximum, the observation will be padded with zeros and the trainer will ignore the padded observations. Note that attention layers are invariant to the order of the entities, so there is no need to properly \"order\" the entities before feeding them into the <code>BufferSensor</code>.</p> <p>The <code>BufferSensorComponent</code> Editor inspector has two arguments:  - <code>Observation Size</code> : This is how many floats each entities will be  represented with. This number is fixed and all entities must  have the same representation. For example, if the entities you want to  put into the <code>BufferSensor</code> have for relevant information position and  speed, then the <code>Observation Size</code> should be 6 floats.  - <code>Maximum Number of Entities</code> : This is the maximum number of entities  the <code>BufferSensor</code> will be able to collect.</p> <p>To add an entity's observations to a <code>BufferSensorComponent</code>, you need to call <code>BufferSensorComponent.AppendObservation()</code> in the Agent.CollectObservations() method with a float array of size <code>Observation Size</code> as argument.</p> <p>Note: Currently, the observations put into the <code>BufferSensor</code> are not normalized, you will need to normalize your observations manually between -1 and 1.</p>"},{"location":"Learning-Environment-Design-Agents/#variable-length-observation-summary-best-practices","title":"Variable Length Observation Summary &amp; Best Practices","text":"<ul> <li>Attach <code>BufferSensorComponent</code> to use.</li> <li>Call <code>BufferSensorComponent.AppendObservation()</code> in the  Agent.CollectObservations() methodto add the observations  of an entity to the <code>BufferSensor</code>.</li> <li>Normalize the entities observations before feeding them into the <code>BufferSensor</code>.</li> </ul>"},{"location":"Learning-Environment-Design-Agents/#goal-signal","title":"Goal Signal","text":"<p>It is possible for agents to collect observations that will be treated as \"goal signal\". A goal signal is used to condition the policy of the agent, meaning that if the goal changes, the policy (i.e. the mapping from observations to actions) will change as well. Note that this is true for any observation since all observations influence the policy of the Agent to some degree. But by specifying a goal signal explicitly, we can make this conditioning more important to the agent. This feature can be used in settings where an agent must learn to solve different tasks that are similar by some aspects because the agent will learn to reuse learnings from different tasks to generalize better. In Unity, you can specify that a <code>VectorSensor</code> or a <code>CameraSensor</code> is a goal by attaching a <code>VectorSensorComponent</code> or a <code>CameraSensorComponent</code> to the Agent and selecting <code>Goal Signal</code> as <code>Observation Type</code>. On the trainer side, there are two different ways to condition the policy. This setting is determined by the conditioning_type parameter. If set to <code>hyper</code> (default) a HyperNetwork will be used to generate some of the weights of the policy using the goal observations as input. Note that using a HyperNetwork requires a lot of computations, it is recommended to use a smaller number of hidden units in the policy to alleviate this. If set to <code>none</code> the goal signal will be considered as regular observations. For an example on how to use a goal signal, see the GridWorld example.</p>"},{"location":"Learning-Environment-Design-Agents/#goal-signal-summary-best-practices","title":"Goal Signal Summary &amp; Best Practices","text":"<ul> <li>Attach a <code>VectorSensorComponent</code> or <code>CameraSensorComponent</code> to an agent and  set the observation type to goal to use the feature.</li> <li>Set the conditioning_type parameter in the training configuration.</li> <li>Reduce the number of hidden units in the network when using the HyperNetwork  conditioning type.</li> </ul>"},{"location":"Learning-Environment-Design-Agents/#actions-and-actuators","title":"Actions and Actuators","text":"<p>An action is an instruction from the Policy that the agent carries out. The action is passed to the an <code>IActionReceiver</code> (either an <code>Agent</code> or an <code>IActuator</code>) as the <code>ActionBuffers</code> parameter when the Academy invokes the <code>IActionReciever.OnActionReceived()</code> function. There are two types of actions supported: Continuous and Discrete.</p> <p>Neither the Policy nor the training algorithm know anything about what the action values themselves mean. The training algorithm simply tries different values for the action list and observes the affect on the accumulated rewards over time and many training episodes. Thus, the only place actions are defined for an Agent is in the <code>OnActionReceived()</code> function.</p> <p>For example, if you designed an agent to move in two dimensions, you could use either continuous or the discrete actions. In the continuous case, you would set the action size to two (one for each dimension), and the agent's Policy would output an action with two floating point values. In the discrete case, you would use one Branch with a size of four (one for each direction), and the Policy would create an action array containing a single element with a value ranging from zero to three. Alternatively, you could create two branches of size two (one for horizontal movement and one for vertical movement), and the Policy would output an action array containing two elements with values ranging from zero to one. You could alternatively use a combination of continuous and discrete actions e.g., using one continuous action for horizontal movement and a discrete branch of size two for the vertical movement.</p> <p>Note that when you are programming actions for an agent, it is often helpful to test your action logic using the <code>Heuristic()</code> method of the Agent, which lets you map keyboard commands to actions.</p>"},{"location":"Learning-Environment-Design-Agents/#continuous-actions","title":"Continuous Actions","text":"<p>When an Agent's Policy has Continuous actions, the <code>ActionBuffers.ContinuousActions</code> passed to the Agent's <code>OnActionReceived()</code> function is an array with length equal to the <code>Continuous Action Size</code> property value. The individual values in the array have whatever meanings that you ascribe to them. If you assign an element in the array as the speed of an Agent, for example, the training process learns to control the speed of the Agent through this parameter.</p> <p>The 3DBall example uses continuous actions with two control values.</p> <p></p> <p>These control values are applied as rotation to the cube:</p> <pre><code>    public override void OnActionReceived(ActionBuffers actionBuffers)\n    {\n        var actionZ = 2f * Mathf.Clamp(actionBuffers.ContinuousActions[0], -1f, 1f);\n        var actionX = 2f * Mathf.Clamp(actionBuffers.ContinuousActions[1], -1f, 1f);\n\n        gameObject.transform.Rotate(new Vector3(0, 0, 1), actionZ);\n        gameObject.transform.Rotate(new Vector3(1, 0, 0), actionX);\n    }\n</code></pre> <p>By default the output from our provided PPO algorithm pre-clamps the values of <code>ActionBuffers.ContinuousActions</code> into the [-1, 1] range. It is a best practice to manually clip these as well, if you plan to use a 3rd party algorithm with your environment. As shown above, you can scale the control values as needed after clamping them.</p>"},{"location":"Learning-Environment-Design-Agents/#discrete-actions","title":"Discrete Actions","text":"<p>When an Agent's Policy uses discrete actions, the <code>ActionBuffers.DiscreteActions</code> passed to the Agent's <code>OnActionReceived()</code> function is an array of integers with length equal to <code>Discrete Branch Size</code>. When defining the discrete actions, <code>Branches</code> is an array of integers, each value corresponds to the number of possibilities for each branch.</p> <p>For example, if we wanted an Agent that can move in a plane and jump, we could define two branches (one for motion and one for jumping) because we want our agent be able to move and jump concurrently. We define the first branch to have 5 possible actions (don't move, go left, go right, go backward, go forward) and the second one to have 2 possible actions (don't jump, jump). The <code>OnActionReceived()</code> method would look something like:</p> <pre><code>// Get the action index for movement\nint movement = actionBuffers.DiscreteActions[0];\n// Get the action index for jumping\nint jump = actionBuffers.DiscreteActions[1];\n\n// Look up the index in the movement action list:\nif (movement == 1) { directionX = -1; }\nif (movement == 2) { directionX = 1; }\nif (movement == 3) { directionZ = -1; }\nif (movement == 4) { directionZ = 1; }\n// Look up the index in the jump action list:\nif (jump == 1 &amp;&amp; IsGrounded()) { directionY = 1; }\n\n// Apply the action results to move the Agent\ngameObject.GetComponent&lt;Rigidbody&gt;().AddForce(\n    new Vector3(\n        directionX * 40f, directionY * 300f, directionZ * 40f));\n</code></pre>"},{"location":"Learning-Environment-Design-Agents/#masking-discrete-actions","title":"Masking Discrete Actions","text":"<p>When using Discrete Actions, it is possible to specify that some actions are impossible for the next decision. When the Agent is controlled by a neural network, the Agent will be unable to perform the specified action. Note that when the Agent is controlled by its Heuristic, the Agent will still be able to decide to perform the masked action. In order to disallow an action, override the <code>Agent.WriteDiscreteActionMask()</code> virtual method, and call <code>SetActionEnabled()</code> on the provided <code>IDiscreteActionMask</code>:</p> <pre><code>public override void WriteDiscreteActionMask(IDiscreteActionMask actionMask)\n{\n    actionMask.SetActionEnabled(branch, actionIndex, isEnabled);\n}\n</code></pre> <p>Where:</p> <ul> <li><code>branch</code> is the index (starting at 0) of the branch on which you want to allow or disallow the action</li> <li><code>actionIndex</code> is the index of the action that you want to allow or disallow.</li> <li><code>isEnabled</code> is a bool indicating whether the action should be allowed or now.</li> </ul> <p>For example, if you have an Agent with 2 branches and on the first branch (branch 0) there are 4 possible actions : \"do nothing\", \"jump\", \"shoot\" and \"change weapon\". Then with the code bellow, the Agent will either \"do nothing\" or \"change weapon\" for their next decision (since action index 1 and 2 are masked)</p> <pre><code>actionMask.SetActionEnabled(0, 1, false);\nactionMask.SetActionEnabled(0, 2, false);\n</code></pre> <p>Notes:</p> <ul> <li>You can call <code>SetActionEnabled</code> multiple times if you want to put masks on multiple   branches.</li> <li>At each step, the state of an action is reset and enabled by default.</li> <li>You cannot mask all the actions of a branch.</li> <li>You cannot mask actions in continuous control.</li> </ul>"},{"location":"Learning-Environment-Design-Agents/#iactuator-interface-and-actuatorcomponents","title":"IActuator interface and ActuatorComponents","text":"<p>The Actuator API allows users to abstract behavior out of Agents and in to components (similar to the ISensor API).  The <code>IActuator</code> interface and <code>Agent</code> class both implement the <code>IActionReceiver</code> interface to allow for backward compatibility with the current <code>Agent.OnActionReceived</code>. This means you will not have to change your code until you decide to use the <code>IActuator</code> API.</p> <p>Like the <code>ISensor</code> interface, the <code>IActuator</code> interface is intended for advanced users.</p> <p>The <code>ActuatorComponent</code> abstract class is used to create the actual <code>IActuator</code> at runtime. It must be attached to the same <code>GameObject</code> as the <code>Agent</code>, or to a child <code>GameObject</code>.  Actuators and all of their data structures are initialized during <code>Agent.Initialize</code>.  This was done to prevent an unexpected allocations at runtime.</p> <p>You can find an example of an <code>IActuator</code> implementation in the <code>Basic</code> example scene. NOTE: you do not need to adjust the Actions in the Agent's <code>Behavior Parameters</code> when using an <code>IActuator</code> and <code>ActuatorComponents</code>.</p> <p>Internally, <code>Agent.OnActionReceived</code> uses an <code>IActuator</code> to send actions to the Agent, although this is mostly abstracted from the user.</p>"},{"location":"Learning-Environment-Design-Agents/#actions-summary-best-practices","title":"Actions Summary &amp; Best Practices","text":"<ul> <li>Agents can use <code>Discrete</code> and/or <code>Continuous</code> actions.</li> <li>Discrete actions can have multiple action branches, and it's possible to mask   certain actions so that they won't be taken.</li> <li>In general, fewer actions will make for easier learning.</li> <li>Be sure to set the Continuous Action Size and Discrete Branch Size to the desired   number for each type of action, and not greater, as doing the latter can interfere with the   efficiency of the training process.</li> <li>Continuous action values should be clipped to an   appropriate range. The provided PPO model automatically clips these values   between -1 and 1, but third party training systems may not do so.</li> </ul>"},{"location":"Learning-Environment-Design-Agents/#rewards","title":"Rewards","text":"<p>In reinforcement learning, the reward is a signal that the agent has done something right. The PPO reinforcement learning algorithm works by optimizing the choices an agent makes such that the agent earns the highest cumulative reward over time. The better your reward mechanism, the better your agent will learn.</p> <p>Note: Rewards are not used during inference by an Agent using a trained model and is also not used during imitation learning.</p> <p>Perhaps the best advice is to start simple and only add complexity as needed. In general, you should reward results rather than actions you think will lead to the desired results. You can even use the Agent's Heuristic to control the Agent while watching how it accumulates rewards.</p> <p>Allocate rewards to an Agent by calling the <code>AddReward()</code> or <code>SetReward()</code> methods on the agent. The reward assigned between each decision should be in the range [-1,1]. Values outside this range can lead to unstable training. The <code>reward</code> value is reset to zero when the agent receives a new decision. If there are multiple calls to <code>AddReward()</code> for a single agent decision, the rewards will be summed together to evaluate how good the previous decision was. The <code>SetReward()</code> will override all previous rewards given to an agent since the previous decision.</p>"},{"location":"Learning-Environment-Design-Agents/#examples","title":"Examples","text":"<p>You can examine the <code>OnActionReceived()</code> functions defined in the example environments to see how those projects allocate rewards.</p> <p>The <code>GridAgent</code> class in the GridWorld example uses a very simple reward system:</p> <pre><code>Collider[] hitObjects = Physics.OverlapBox(trueAgent.transform.position,\n                                           new Vector3(0.3f, 0.3f, 0.3f));\nif (hitObjects.Where(col =&gt; col.gameObject.tag == \"goal\").ToArray().Length == 1)\n{\n    AddReward(1.0f);\n    EndEpisode();\n}\nelse if (hitObjects.Where(col =&gt; col.gameObject.tag == \"pit\").ToArray().Length == 1)\n{\n    AddReward(-1f);\n    EndEpisode();\n}\n</code></pre> <p>The agent receives a positive reward when it reaches the goal and a negative reward when it falls into the pit. Otherwise, it gets no rewards. This is an example of a sparse reward system. The agent must explore a lot to find the infrequent reward.</p> <p>In contrast, the <code>AreaAgent</code> in the Area example gets a small negative reward every step. In order to get the maximum reward, the agent must finish its task of reaching the goal square as quickly as possible:</p> <pre><code>AddReward( -0.005f);\nMoveAgent(act);\n\nif (gameObject.transform.position.y &lt; 0.0f ||\n    Mathf.Abs(gameObject.transform.position.x - area.transform.position.x) &gt; 8f ||\n    Mathf.Abs(gameObject.transform.position.z + 5 - area.transform.position.z) &gt; 8)\n{\n    AddReward(-1f);\n    EndEpisode();\n}\n</code></pre> <p>The agent also gets a larger negative penalty if it falls off the playing surface.</p> <p>The <code>Ball3DAgent</code> in the 3DBall takes a similar approach, but allocates a small positive reward as long as the agent balances the ball. The agent can maximize its rewards by keeping the ball on the platform:</p> <pre><code>\nSetReward(0.1f);\n\n// When ball falls mark Agent as finished and give a negative penalty\nif ((ball.transform.position.y - gameObject.transform.position.y) &lt; -2f ||\n    Mathf.Abs(ball.transform.position.x - gameObject.transform.position.x) &gt; 3f ||\n    Mathf.Abs(ball.transform.position.z - gameObject.transform.position.z) &gt; 3f)\n{\n    SetReward(-1f);\n    EndEpisode();\n\n}\n</code></pre> <p>The <code>Ball3DAgent</code> also assigns a negative penalty when the ball falls off the platform.</p> <p>Note that all of these environments make use of the <code>EndEpisode()</code> method, which manually terminates an episode when a termination condition is reached. This can be called independently of the <code>Max Step</code> property.</p>"},{"location":"Learning-Environment-Design-Agents/#rewards-summary-best-practices","title":"Rewards Summary &amp; Best Practices","text":"<ul> <li>Use <code>AddReward()</code> to accumulate rewards between decisions. Use <code>SetReward()</code>   to overwrite any previous rewards accumulate between decisions.</li> <li>The magnitude of any given reward should typically not be greater than 1.0 in   order to ensure a more stable learning process.</li> <li>Positive rewards are often more helpful to shaping the desired behavior of an   agent than negative rewards. Excessive negative rewards can result in the   agent failing to learn any meaningful behavior.</li> <li>For locomotion tasks, a small positive reward (+0.1) for forward velocity is   typically used.</li> <li>If you want the agent to finish a task quickly, it is often helpful to provide   a small penalty every step (-0.05) that the agent does not complete the task.   In this case completion of the task should also coincide with the end of the   episode by calling <code>EndEpisode()</code> on the agent when it has accomplished its   goal.</li> </ul>"},{"location":"Learning-Environment-Design-Agents/#agent-properties","title":"Agent Properties","text":"<ul> <li><code>Behavior Parameters</code> - The parameters dictating what Policy the Agent will   receive.</li> <li><code>Behavior Name</code> - The identifier for the behavior. Agents with the same     behavior name will learn the same policy.</li> <li><code>Vector Observation</code><ul> <li><code>Space Size</code> - Length of vector observation for the Agent.</li> <li><code>Stacked Vectors</code> - The number of previous vector observations that will   be stacked and used collectively for decision making. This results in the   effective size of the vector observation being passed to the Policy being:   Space Size x Stacked Vectors.</li> </ul> </li> <li><code>Actions</code><ul> <li><code>Continuous Actions</code> - The number of concurrent continuous actions that  the Agent can take.</li> <li><code>Discrete Branches</code> - An array of integers, defines multiple concurrent   discrete actions. The values in the <code>Discrete Branches</code> array correspond   to the number of possible discrete values for each action branch.</li> </ul> </li> <li><code>Model</code> - The neural network model used for inference (obtained after     training)</li> <li><code>Inference Device</code> - Whether to use CPU or GPU to run the model during     inference</li> <li><code>Behavior Type</code> - Determines whether the Agent will do training, inference,     or use its Heuristic() method:<ul> <li><code>Default</code> - the Agent will train if they connect to a python trainer,   otherwise they will perform inference.</li> <li><code>Heuristic Only</code> - the Agent will always use the <code>Heuristic()</code> method.</li> <li><code>Inference Only</code> - the Agent will always perform inference.</li> </ul> </li> <li><code>Team ID</code> - Used to define the team for self-play</li> <li><code>Use Child Sensors</code> - Whether to use all Sensor components attached to child     GameObjects of this Agent.</li> <li><code>Max Step</code> - The per-agent maximum number of steps. Once this number is   reached, the Agent will be reset.</li> </ul>"},{"location":"Learning-Environment-Design-Agents/#destroying-an-agent","title":"Destroying an Agent","text":"<p>You can destroy an Agent GameObject during the simulation. Make sure that there is always at least one Agent training at all times by either spawning a new Agent every time one is destroyed or by re-spawning new Agents when the whole environment resets.</p>"},{"location":"Learning-Environment-Design-Agents/#defining-multi-agent-scenarios","title":"Defining Multi-agent Scenarios","text":""},{"location":"Learning-Environment-Design-Agents/#teams-for-adversarial-scenarios","title":"Teams for Adversarial Scenarios","text":"<p>Self-play is triggered by including the self-play hyperparameter hierarchy in the trainer configuration. To distinguish opposing agents, set the team ID to different integer values in the behavior parameters script on the agent prefab.</p> <p> </p> <p>Team ID must be 0 or an integer greater than 0.</p> <p>In symmetric games, since all agents (even on opposing teams) will share the same policy, they should have the same 'Behavior Name' in their Behavior Parameters Script. In asymmetric games, they should have a different Behavior Name in their Behavior Parameters script. Note, in asymmetric games, the agents must have both different Behavior Names and different team IDs!</p> <p>For examples of how to use this feature, you can see the trainer configurations and agent prefabs for our Tennis and Soccer environments. Tennis and Soccer provide examples of symmetric games. To train an asymmetric game, specify trainer configurations for each of your behavior names and include the self-play hyperparameter hierarchy in both.</p>"},{"location":"Learning-Environment-Design-Agents/#groups-for-cooperative-scenarios","title":"Groups for Cooperative Scenarios","text":"<p>Cooperative behavior in ML-Agents can be enabled by instantiating a <code>SimpleMultiAgentGroup</code>, typically in an environment controller or similar script, and adding agents to it using the <code>RegisterAgent(Agent agent)</code> method. Note that all agents added to the same <code>SimpleMultiAgentGroup</code> must have the same behavior name and Behavior Parameters. Using <code>SimpleMultiAgentGroup</code> enables the agents within a group to learn how to work together to achieve a common goal (i.e., maximize a group-given reward), even if one or more of the group members are removed before the episode ends. You can then use this group to add/set rewards, end or interrupt episodes at a group level using the <code>AddGroupReward()</code>, <code>SetGroupReward()</code>, <code>EndGroupEpisode()</code>, and <code>GroupEpisodeInterrupted()</code> methods. For example:</p> <pre><code>// Create a Multi Agent Group in Start() or Initialize()\nm_AgentGroup = new SimpleMultiAgentGroup();\n\n// Register agents in group at the beginning of an episode\nfor (var agent in AgentList)\n{\n  m_AgentGroup.RegisterAgent(agent);\n}\n\n// if the team scores a goal\nm_AgentGroup.AddGroupReward(rewardForGoal);\n\n// If the goal is reached and the episode is over\nm_AgentGroup.EndGroupEpisode();\nResetScene();\n\n// If time ran out and we need to interrupt the episode\nm_AgentGroup.GroupEpisodeInterrupted();\nResetScene();\n</code></pre> <p>Multi Agent Groups should be used with the MA-POCA trainer, which is explicitly designed to train cooperative environments. This can be enabled by using the <code>poca</code> trainer - see the training configurations doc for more information on configuring MA-POCA. When using MA-POCA, agents which are deactivated or removed from the Scene during the episode will still learn to contribute to the group's long term rewards, even if they are not active in the scene to experience them.</p> <p>See the Cooperative Push Block environment for an example of how to use Multi Agent Groups, and the Dungeon Escape environment for an example of how the Multi Agent Group can be used with agents that are removed from the scene mid-episode.</p> <p>NOTE: Groups differ from Teams (for competitive settings) in the following way - Agents working together should be added to the same Group, while agents playing against each other should be given different Team Ids. If in the Scene there is one playing field and two teams, there should be two Groups, one for each team, and each team should be assigned a different Team Id. If this playing field is duplicated many times in the Scene (e.g. for training speedup), there should be two Groups per playing field, and two unique Team Ids for the entire Scene. In environments with both Groups and Team Ids configured, MA-POCA and self-play can be used together for training. In the diagram below, there are two agents on each team, and two playing fields where teams are pitted against each other. All the blue agents should share a Team Id (and the orange ones a different ID), and there should be four group managers, one per pair of agents.</p> <p> </p> <p>Please see the SoccerTwos environment for an example.</p>"},{"location":"Learning-Environment-Design-Agents/#cooperative-behaviors-notes-and-best-practices","title":"Cooperative Behaviors Notes and Best Practices","text":"<ul> <li> <p>An agent can only be registered to one MultiAgentGroup at a time. If you want to re-assign an agent from one group to another, you have to unregister it from the current group first.</p> </li> <li> <p>Agents with different behavior names in the same group are not supported.</p> </li> <li> <p>Agents within groups should always set the <code>Max Steps</code> parameter in the Agent script to 0. Instead, handle Max Steps using the MultiAgentGroup by ending the episode for the entire Group using <code>GroupEpisodeInterrupted()</code>.</p> </li> <li> <p><code>EndGroupEpisode</code> and <code>GroupEpisodeInterrupted</code> do the same job in the game, but has slightly different effect on the training. If the episode is completed, you would want to call <code>EndGroupEpisode</code>. But if the episode is not over but it has been running for enough steps, i.e. reaching max step, you would call <code>GroupEpisodeInterrupted</code>.</p> </li> <li> <p>If an agent finished earlier, e.g. completed tasks/be removed/be killed in the game, do not call <code>EndEpisode()</code> on the Agent. Instead, disable the agent and re-enable it when the next episode starts, or destroy the agent entirely. This is because calling <code>EndEpisode()</code> will call <code>OnEpisodeBegin()</code>, which will reset the agent immediately. While it is possible to call <code>EndEpisode()</code> in this way, it is usually not the desired behavior when training groups of agents.</p> </li> <li> <p>If an agent that was disabled in a scene needs to be re-enabled, it must be re-registered to the MultiAgentGroup.</p> </li> <li> <p>Group rewards are meant to reinforce agents to act in the group's best interest instead of individual ones, and are treated differently than individual agent rewards during training. So calling <code>AddGroupReward()</code> is not equivalent to calling agent.AddReward() on each agent in the group.</p> </li> <li> <p>You can still add incremental rewards to agents using <code>Agent.AddReward()</code> if they are in a Group. These rewards will only be given to those agents and are received when the Agent is active.</p> </li> <li> <p>Environments which use Multi Agent Groups can be trained using PPO or SAC, but agents will not be able to learn from group rewards after deactivation/removal, nor will they behave as cooperatively.</p> </li> </ul>"},{"location":"Learning-Environment-Design-Agents/#recording-demonstrations","title":"Recording Demonstrations","text":"<p>In order to record demonstrations from an agent, add the <code>Demonstration Recorder</code> component to a GameObject in the scene which contains an <code>Agent</code> component. Once added, it is possible to name the demonstration that will be recorded from the agent.</p> <p> </p> <p>When <code>Record</code> is checked, a demonstration will be created whenever the scene is played from the Editor. Depending on the complexity of the task, anywhere from a few minutes or a few hours of demonstration data may be necessary to be useful for imitation learning. To specify an exact number of steps you want to record use the <code>Num Steps To Record</code> field and the editor will end your play session automatically once that many steps are recorded. If you set <code>Num Steps To Record</code> to <code>0</code> then recording will continue until you manually end the play session. Once the play session ends a <code>.demo</code> file will be created in the <code>Assets/Demonstrations</code> folder (by default). This file contains the demonstrations. Clicking on the file will provide metadata about the demonstration in the inspector.</p> <p> </p> <p>You can then specify the path to this file in your training configurations.</p>"},{"location":"Learning-Environment-Design/","title":"Designing a Learning Environment","text":"<p>This page contains general advice on how to design your learning environment, in addition to overviewing aspects of the ML-Agents Unity SDK that pertain to setting up your scene and simulation as opposed to designing your agents within the scene. We have a dedicated page on Designing Agents which includes how to instrument observations, actions and rewards, define teams for multi-agent scenarios and record agent demonstrations for imitation learning.</p> <p>To help on-board to the entire set of functionality provided by the ML-Agents Toolkit, we recommend exploring our API documentation. Additionally, our example environments are a great resource as they provide sample usage of almost all of our features.</p>"},{"location":"Learning-Environment-Design/#the-simulation-and-training-process","title":"The Simulation and Training Process","text":"<p>Training and simulation proceed in steps orchestrated by the ML-Agents Academy class. The Academy works with Agent objects in the scene to step through the simulation.</p> <p>During training, the external Python training process communicates with the Academy to run a series of episodes while it collects data and optimizes its neural network model. When training is completed successfully, you can add the trained model file to your Unity project for later use.</p> <p>The ML-Agents Academy class orchestrates the agent simulation loop as follows:</p> <ol> <li>Calls your Academy's <code>OnEnvironmentReset</code> delegate.</li> <li>Calls the <code>OnEpisodeBegin()</code> function for each Agent in the scene.</li> <li>Gathers information about the scene. This is done by calling the   <code>CollectObservations(VectorSensor sensor)</code> function for each Agent in the   scene, as well as updating their sensor and collecting the resulting   observations.</li> <li>Uses each Agent's Policy to decide on the Agent's next action.</li> <li>Calls the <code>OnActionReceived()</code> function for each Agent in the scene, passing    in the action chosen by the Agent's Policy.</li> <li>Calls the Agent's <code>OnEpisodeBegin()</code> function if the Agent has reached its    <code>Max Step</code> count or has otherwise marked itself as <code>EndEpisode()</code>.</li> </ol> <p>To create a training environment, extend the Agent class to implement the above methods whether you need to implement them or not depends on your specific scenario.</p>"},{"location":"Learning-Environment-Design/#organizing-the-unity-scene","title":"Organizing the Unity Scene","text":"<p>To train and use the ML-Agents Toolkit in a Unity scene, the scene as many Agent subclasses as you need. Agent instances should be attached to the GameObject representing that Agent.</p>"},{"location":"Learning-Environment-Design/#academy","title":"Academy","text":"<p>The Academy is a singleton which orchestrates Agents and their decision making processes. Only a single Academy exists at a time.</p>"},{"location":"Learning-Environment-Design/#academy-resetting","title":"Academy resetting","text":"<p>To alter the environment at the start of each episode, add your method to the Academy's OnEnvironmentReset action.</p> <pre><code>public class MySceneBehavior : MonoBehaviour\n{\n    public void Awake()\n    {\n        Academy.Instance.OnEnvironmentReset += EnvironmentReset;\n    }\n\n    void EnvironmentReset()\n    {\n        // Reset the scene here\n    }\n}\n</code></pre> <p>For example, you might want to reset an Agent to its starting position or move a goal to a random position. An environment resets when the <code>reset()</code> method is called on the Python <code>UnityEnvironment</code>.</p> <p>When you reset an environment, consider the factors that should change so that training is generalizable to different conditions. For example, if you were training a maze-solving agent, you would probably want to change the maze itself for each training episode. Otherwise, the agent would probably on learn to solve one, particular maze, not mazes in general.</p>"},{"location":"Learning-Environment-Design/#multiple-areas","title":"Multiple Areas","text":"<p>In many of the example environments, many copies of the training area are instantiated in the scene. This generally speeds up training, allowing the environment to gather many experiences in parallel. This can be achieved simply by instantiating many Agents with the same Behavior Name. If possible, consider designing your scene to support multiple areas.</p> <p>Check out our example environments to see examples of multiple areas. Additionally, the Making a New Learning Environment guide demonstrates this option.</p>"},{"location":"Learning-Environment-Design/#environments","title":"Environments","text":"<p>When you create a training environment in Unity, you must set up the scene so that it can be controlled by the external training process. Considerations include:</p> <ul> <li>The training scene must start automatically when your Unity application is   launched by the training process.</li> <li>The Academy must reset the scene to a valid starting point for each episode of   training.</li> <li>A training episode must have a definite end \u2014 either using <code>Max Steps</code> or by   each Agent ending its episode manually with <code>EndEpisode()</code>.</li> </ul>"},{"location":"Learning-Environment-Design/#environment-parameters","title":"Environment Parameters","text":"<p>Curriculum learning and environment parameter randomization are two training methods that control specific parameters in your environment. As such, it is important to ensure that your environment parameters are updated at each step to the correct values. To enable this, we expose a <code>EnvironmentParameters</code> C# class that you can use to retrieve the values of the parameters defined in the training configurations for both of those features. Please see our documentation for curriculum learning and environment parameter randomization for details.</p> <p>We recommend modifying the environment from the Agent's <code>OnEpisodeBegin()</code> function by leveraging <code>Academy.Instance.EnvironmentParameters</code>. See the WallJump example environment for a sample usage (specifically, WallJumpAgent.cs ).</p>"},{"location":"Learning-Environment-Design/#agent","title":"Agent","text":"<p>The Agent class represents an actor in the scene that collects observations and carries out actions. The Agent class is typically attached to the GameObject in the scene that otherwise represents the actor \u2014 for example, to a player object in a football game or a car object in a vehicle simulation. Every Agent must have appropriate <code>Behavior Parameters</code>.</p> <p>Generally, when creating an Agent, you should extend the Agent class and implement the <code>CollectObservations(VectorSensor sensor)</code> and <code>OnActionReceived()</code> methods:</p> <ul> <li><code>CollectObservations(VectorSensor sensor)</code> \u2014 Collects the Agent's observation   of its environment.</li> <li><code>OnActionReceived()</code> \u2014 Carries out the action chosen by the Agent's Policy and   assigns a reward to the current state.</li> </ul> <p>Your implementations of these functions determine how the Behavior Parameters assigned to this Agent must be set.</p> <p>You must also determine how an Agent finishes its task or times out. You can manually terminate an Agent episode in your <code>OnActionReceived()</code> function when the Agent has finished (or irrevocably failed) its task by calling the <code>EndEpisode()</code> function. You can also set the Agent's <code>Max Steps</code> property to a positive value and the Agent will consider the episode over after it has taken that many steps. You can use the <code>Agent.OnEpisodeBegin()</code> function to prepare the Agent to start again.</p> <p>See Agents for detailed information about programming your own Agents.</p>"},{"location":"Learning-Environment-Design/#recording-statistics","title":"Recording Statistics","text":"<p>We offer developers a mechanism to record statistics from within their Unity environments. These statistics are aggregated and generated during the training process. To record statistics, see the <code>StatsRecorder</code> C# class.</p> <p>See the FoodCollector example environment for a sample usage (specifically, FoodCollectorSettings.cs ).</p>"},{"location":"Learning-Environment-Examples/","title":"Example Learning Environments","text":"<p>The Unity ML-Agents Toolkit includes an expanding set of example environments that highlight the various features of the toolkit. These environments can also serve as templates for new environments or as ways to test new ML algorithms. Environments are located in <code>Project/Assets/ML-Agents/Examples</code> and summarized below.</p> <p>For the environments that highlight specific features of the toolkit, we provide the pre-trained model files and the training config file that enables you to train the scene yourself. The environments that are designed to serve as challenges for researchers do not have accompanying pre-trained model files or training configs and are marked as Optional below.</p> <p>This page only overviews the example environments we provide. To learn more on how to design and build your own environments see our Making a New Learning Environment page. If you would like to contribute environments, please see our contribution guidelines page.</p>"},{"location":"Learning-Environment-Examples/#basic","title":"Basic","text":"<ul> <li>Set-up: A linear movement task where the agent must move left or right to   rewarding states.</li> <li>Goal: Move to the most reward state.</li> <li>Agents: The environment contains one agent.</li> <li>Agent Reward Function:</li> <li>-0.01 at each step</li> <li>+0.1 for arriving at suboptimal state.</li> <li>+1.0 for arriving at optimal state.</li> <li>Behavior Parameters:</li> <li>Vector Observation space: One variable corresponding to current state.</li> <li>Actions: 1 discrete action branch with 3 actions (Move left, do nothing, move     right).</li> <li>Visual Observations: None</li> <li>Float Properties: None</li> <li>Benchmark Mean Reward: 0.93</li> </ul>"},{"location":"Learning-Environment-Examples/#3dball-3d-balance-ball","title":"3DBall: 3D Balance Ball","text":"<ul> <li>Set-up: A balance-ball task, where the agent balances the ball on it's head.</li> <li>Goal: The agent must balance the ball on it's head for as long as possible.</li> <li>Agents: The environment contains 12 agents of the same kind, all using the   same Behavior Parameters.</li> <li>Agent Reward Function:</li> <li>+0.1 for every step the ball remains on it's head.</li> <li>-1.0 if the ball falls off.</li> <li>Behavior Parameters:</li> <li>Vector Observation space: 8 variables corresponding to rotation of the agent     cube, and position and velocity of ball.</li> <li>Vector Observation space (Hard Version): 5 variables corresponding to     rotation of the agent cube and position of ball.</li> <li>Actions: 2 continuous actions, with one value corresponding to     X-rotation, and the other to Z-rotation.</li> <li>Visual Observations: Third-person view from the upper-front of the agent. Use     <code>Visual3DBall</code> scene.</li> <li>Float Properties: Three</li> <li>scale: Specifies the scale of the ball in the 3 dimensions (equal across the     three dimensions)<ul> <li>Default: 1</li> <li>Recommended Minimum: 0.2</li> <li>Recommended Maximum: 5</li> </ul> </li> <li>gravity: Magnitude of gravity<ul> <li>Default: 9.81</li> <li>Recommended Minimum: 4</li> <li>Recommended Maximum: 105</li> </ul> </li> <li>mass: Specifies mass of the ball<ul> <li>Default: 1</li> <li>Recommended Minimum: 0.1</li> <li>Recommended Maximum: 20</li> </ul> </li> <li>Benchmark Mean Reward: 100</li> </ul>"},{"location":"Learning-Environment-Examples/#gridworld","title":"GridWorld","text":"<ul> <li>Set-up: A multi-goal version of the grid-world task. Scene contains agent, goal,   and obstacles.</li> <li>Goal: The agent must navigate the grid to the appropriate goal while   avoiding the obstacles.</li> <li>Agents: The environment contains nine agents with the same Behavior   Parameters.</li> <li>Agent Reward Function:</li> <li>-0.01 for every step.</li> <li>+1.0 if the agent navigates to the correct goal (episode ends).</li> <li>-1.0 if the agent navigates to an incorrect goal (episode ends).</li> <li>Behavior Parameters:</li> <li>Vector Observation space: None</li> <li>Actions: 1 discrete action branch with 5 actions, corresponding to movement in     cardinal directions or not moving. Note that for this environment,     action masking     is turned on by default (this option can be toggled using the <code>Mask Actions</code>     checkbox within the <code>trueAgent</code> GameObject). The trained model file provided     was generated with action masking turned on.</li> <li>Visual Observations: One corresponding to top-down view of GridWorld.</li> <li>Goal Signal : A one hot vector corresponding to which color is the correct goal   for the Agent</li> <li>Float Properties: Three, corresponding to grid size, number of green goals, and   number of red goals.</li> <li>Benchmark Mean Reward: 0.8</li> </ul>"},{"location":"Learning-Environment-Examples/#push-block","title":"Push Block","text":"<ul> <li>Set-up: A platforming environment where the agent can push a block around.</li> <li>Goal: The agent must push the block to the goal.</li> <li>Agents: The environment contains one agent.</li> <li>Agent Reward Function:</li> <li>-0.0025 for every step.</li> <li>+1.0 if the block touches the goal.</li> <li>Behavior Parameters:</li> <li>Vector Observation space: (Continuous) 70 variables corresponding to 14     ray-casts each detecting one of three possible objects (wall, goal, or     block).</li> <li>Actions: 1 discrete action branch with 7 actions, corresponding to turn clockwise     and counterclockwise, move along four different face directions, or do nothing.</li> <li>Float Properties: Four</li> <li>block_scale: Scale of the block along the x and z dimensions<ul> <li>Default: 2</li> <li>Recommended Minimum: 0.5</li> <li>Recommended Maximum: 4</li> </ul> </li> <li>dynamic_friction: Coefficient of friction for the ground material acting on     moving objects<ul> <li>Default: 0</li> <li>Recommended Minimum: 0</li> <li>Recommended Maximum: 1</li> </ul> </li> <li>static_friction: Coefficient of friction for the ground material acting on     stationary objects<ul> <li>Default: 0</li> <li>Recommended Minimum: 0</li> <li>Recommended Maximum: 1</li> </ul> </li> <li>block_drag: Effect of air resistance on block<ul> <li>Default: 0.5</li> <li>Recommended Minimum: 0</li> <li>Recommended Maximum: 2000</li> </ul> </li> <li>Benchmark Mean Reward: 4.5</li> </ul>"},{"location":"Learning-Environment-Examples/#wall-jump","title":"Wall Jump","text":"<ul> <li>Set-up: A platforming environment where the agent can jump over a wall.</li> <li>Goal: The agent must use the block to scale the wall and reach the goal.</li> <li>Agents: The environment contains one agent linked to two different Models. The   Policy the agent is linked to changes depending on the height of the wall. The   change of Policy is done in the WallJumpAgent class.</li> <li>Agent Reward Function:</li> <li>-0.0005 for every step.</li> <li>+1.0 if the agent touches the goal.</li> <li>-1.0 if the agent falls off the platform.</li> <li>Behavior Parameters:</li> <li>Vector Observation space: Size of 74, corresponding to 14 ray casts each     detecting 4 possible objects. plus the global position of the agent and     whether or not the agent is grounded.</li> <li>Actions: 4 discrete action branches:<ul> <li>Forward Motion (3 possible actions: Forward, Backwards, No Action)</li> <li>Rotation (3 possible actions: Rotate Left, Rotate Right, No Action)</li> <li>Side Motion (3 possible actions: Left, Right, No Action)</li> <li>Jump (2 possible actions: Jump, No Action)</li> </ul> </li> <li>Visual Observations: None</li> <li>Float Properties: Four</li> <li>Benchmark Mean Reward (Big &amp; Small Wall): 0.8</li> </ul>"},{"location":"Learning-Environment-Examples/#crawler","title":"Crawler","text":"<ul> <li>Set-up: A creature with 4 arms and 4 forearms.</li> <li>Goal: The agents must move its body toward the goal direction without falling.</li> <li>Agents: The environment contains 10 agents with same Behavior Parameters.</li> <li>Agent Reward Function (independent):   The reward function is now geometric meaning the reward each step is a product   of all the rewards instead of a sum, this helps the agent try to maximize all   rewards instead of the easiest rewards.</li> <li>Body velocity matches goal velocity. (normalized between (0,1))</li> <li>Head direction alignment with goal direction. (normalized between (0,1))</li> <li>Behavior Parameters:</li> <li>Vector Observation space: 172 variables corresponding to position, rotation,     velocity, and angular velocities of each limb plus the acceleration and     angular acceleration of the body.</li> <li>Actions: 20 continuous actions, corresponding to target     rotations for joints.</li> <li>Visual Observations: None</li> <li>Float Properties: None</li> <li>Benchmark Mean Reward: 3000</li> </ul>"},{"location":"Learning-Environment-Examples/#worm","title":"Worm","text":"<ul> <li>Set-up: A worm with a head and 3 body segments.</li> <li>Goal: The agents must move its body toward the goal direction.</li> <li>Agents: The environment contains 10 agents with same Behavior Parameters.</li> <li>Agent Reward Function (independent):   The reward function is now geometric meaning the reward each step is a product   of all the rewards instead of a sum, this helps the agent try to maximize all   rewards instead of the easiest rewards.</li> <li>Body velocity matches goal velocity. (normalized between (0,1))</li> <li>Body direction alignment with goal direction. (normalized between (0,1))</li> <li>Behavior Parameters:</li> <li>Vector Observation space: 64 variables corresponding to position, rotation,     velocity, and angular velocities of each limb plus the acceleration and     angular acceleration of the body.</li> <li>Actions: 9 continuous actions, corresponding to target     rotations for joints.</li> <li>Visual Observations: None</li> <li>Float Properties: None</li> <li>Benchmark Mean Reward: 800</li> </ul>"},{"location":"Learning-Environment-Examples/#food-collector","title":"Food Collector","text":"<ul> <li>Set-up: A multi-agent environment where agents compete to collect food.</li> <li>Goal: The agents must learn to collect as many green food spheres as possible   while avoiding red spheres.</li> <li>Agents: The environment contains 5 agents with same Behavior Parameters.</li> <li>Agent Reward Function (independent):</li> <li>+1 for interaction with green spheres</li> <li>-1 for interaction with red spheres</li> <li>Behavior Parameters:</li> <li>Vector Observation space: 53 corresponding to velocity of agent (2), whether     agent is frozen and/or shot its laser (2), plus grid based perception of     objects around agent's forward direction (40 by 40 with 6 different categories).</li> <li>Actions:<ul> <li>3 continuous actions correspond to Forward Motion, Side Motion and Rotation</li> <li>1 discrete acion branch for Laser with 2 possible actions corresponding to   Shoot Laser or No Action</li> </ul> </li> <li>Visual Observations (Optional): First-person camera per-agent, plus one vector     flag representing the frozen state of the agent. This scene uses a combination     of vector and visual observations and the training will not succeed without     the frozen vector flag. Use <code>VisualFoodCollector</code> scene.</li> <li>Float Properties: Two</li> <li>laser_length: Length of the laser used by the agent<ul> <li>Default: 1</li> <li>Recommended Minimum: 0.2</li> <li>Recommended Maximum: 7</li> </ul> </li> <li>agent_scale: Specifies the scale of the agent in the 3 dimensions (equal     across the three dimensions)<ul> <li>Default: 1</li> <li>Recommended Minimum: 0.5</li> <li>Recommended Maximum: 5</li> </ul> </li> <li>Benchmark Mean Reward: 10</li> </ul>"},{"location":"Learning-Environment-Examples/#hallway","title":"Hallway","text":"<ul> <li>Set-up: Environment where the agent needs to find information in a room,   remember it, and use it to move to the correct goal.</li> <li>Goal: Move to the goal which corresponds to the color of the block in the   room.</li> <li>Agents: The environment contains one agent.</li> <li>Agent Reward Function (independent):</li> <li>+1 For moving to correct goal.</li> <li>-0.1 For moving to incorrect goal.</li> <li>-0.0003 Existential penalty.</li> <li>Behavior Parameters:</li> <li>Vector Observation space: 30 corresponding to local ray-casts detecting     objects, goals, and walls.</li> <li>Actions: 1 discrete action Branch, with 4 actions corresponding to agent     rotation and forward/backward movement.</li> <li>Float Properties: None</li> <li>Benchmark Mean Reward: 0.7</li> <li>To train this environment, you can enable curiosity by adding the <code>curiosity</code> reward signal     in <code>config/ppo/Hallway.yaml</code></li> </ul>"},{"location":"Learning-Environment-Examples/#soccer-twos","title":"Soccer Twos","text":"<ul> <li>Set-up: Environment where four agents compete in a 2 vs 2 toy soccer game.</li> <li>Goal:</li> <li>Get the ball into the opponent's goal while preventing the ball from     entering own goal.</li> <li>Agents: The environment contains two different Multi Agent Groups with two agents in each.   Parameters : SoccerTwos.</li> <li>Agent Reward Function (dependent):</li> <li>(1 - <code>accumulated time penalty</code>) When ball enters opponent's goal     <code>accumulated time penalty</code> is incremented by (1 / <code>MaxStep</code>) every fixed     update and is reset to 0 at the beginning of an episode.</li> <li>-1 When ball enters team's goal.</li> <li>Behavior Parameters:</li> <li>Vector Observation space: 336 corresponding to 11 ray-casts forward     distributed over 120 degrees and 3 ray-casts backward distributed over 90     degrees each detecting 6 possible object types, along with the object's     distance. The forward ray-casts contribute 264 state dimensions and backward     72 state dimensions over three observation stacks.</li> <li>Actions: 3 discrete branched actions corresponding to     forward, backward, sideways movement, as well as rotation.</li> <li>Visual Observations: None</li> <li>Float Properties: Two</li> <li>ball_scale: Specifies the scale of the ball in the 3 dimensions (equal     across the three dimensions)<ul> <li>Default: 7.5</li> <li>Recommended minimum: 4</li> <li>Recommended maximum: 10</li> </ul> </li> <li>gravity: Magnitude of the gravity<ul> <li>Default: 9.81</li> <li>Recommended minimum: 6</li> <li>Recommended maximum: 20</li> </ul> </li> </ul>"},{"location":"Learning-Environment-Examples/#strikers-vs-goalie","title":"Strikers Vs. Goalie","text":"<ul> <li>Set-up: Environment where two agents compete in a 2 vs 1 soccer variant.</li> <li>Goal:</li> <li>Striker: Get the ball into the opponent's goal.</li> <li>Goalie: Keep the ball out of the goal.</li> <li>Agents: The environment contains two different Multi Agent Groups. One with two Strikers and the other one Goalie.   Behavior Parameters : Striker, Goalie.</li> <li>Striker Agent Reward Function (dependent):</li> <li>+1 When ball enters opponent's goal.</li> <li>-0.001 Existential penalty.</li> <li>Goalie Agent Reward Function (dependent):</li> <li>-1 When ball enters goal.</li> <li>0.001 Existential bonus.</li> <li>Behavior Parameters:</li> <li>Striker Vector Observation space: 294 corresponding to 11 ray-casts forward     distributed over 120 degrees and 3 ray-casts backward distributed over 90     degrees each detecting 5 possible object types, along with the object's     distance. The forward ray-casts contribute 231 state dimensions and backward     63 state dimensions over three observation stacks.</li> <li>Striker Actions: 3 discrete branched actions corresponding     to forward, backward, sideways movement, as well as rotation.</li> <li>Goalie Vector Observation space: 738 corresponding to 41 ray-casts     distributed over 360 degrees each detecting 4 possible object types, along     with the object's distance and 3 observation stacks.</li> <li>Goalie Actions: 3 discrete branched actions corresponding     to forward, backward, sideways movement, as well as rotation.</li> <li>Visual Observations: None</li> <li>Float Properties: Two</li> <li>ball_scale: Specifies the scale of the ball in the 3 dimensions (equal     across the three dimensions)<ul> <li>Default: 7.5</li> <li>Recommended minimum: 4</li> <li>Recommended maximum: 10</li> </ul> </li> <li>gravity: Magnitude of the gravity<ul> <li>Default: 9.81</li> <li>Recommended minimum: 6</li> <li>Recommended maximum: 20</li> </ul> </li> </ul>"},{"location":"Learning-Environment-Examples/#walker","title":"Walker","text":"<ul> <li>Set-up: Physics-based Humanoid agents with 26 degrees of freedom. These DOFs   correspond to articulation of the following body-parts: hips, chest, spine,   head, thighs, shins, feet, arms, forearms and hands.</li> <li>Goal: The agents must move its body toward the goal direction without falling.</li> <li>Agents: The environment contains 10 independent agents with same Behavior   Parameters.</li> <li>Agent Reward Function (independent):   The reward function is now geometric meaning the reward each step is a product   of all the rewards instead of a sum, this helps the agent try to maximize all   rewards instead of the easiest rewards.</li> <li>Body velocity matches goal velocity. (normalized between (0,1))</li> <li>Head direction alignment with goal direction. (normalized between (0,1))</li> <li>Behavior Parameters:</li> <li>Vector Observation space: 243 variables corresponding to position, rotation,     velocity, and angular velocities of each limb, along with goal direction.</li> <li>Actions: 39 continuous actions, corresponding to target     rotations and strength applicable to the joints.</li> <li>Visual Observations: None</li> <li>Float Properties: Four</li> <li>gravity: Magnitude of gravity<ul> <li>Default: 9.81</li> <li>Recommended Minimum:</li> <li>Recommended Maximum:</li> </ul> </li> <li>hip_mass: Mass of the hip component of the walker<ul> <li>Default: 8</li> <li>Recommended Minimum: 7</li> <li>Recommended Maximum: 28</li> </ul> </li> <li>chest_mass: Mass of the chest component of the walker<ul> <li>Default: 8</li> <li>Recommended Minimum: 3</li> <li>Recommended Maximum: 20</li> </ul> </li> <li>spine_mass: Mass of the spine component of the walker<ul> <li>Default: 8</li> <li>Recommended Minimum: 3</li> <li>Recommended Maximum: 20</li> </ul> </li> <li>Benchmark Mean Reward : 2500</li> </ul>"},{"location":"Learning-Environment-Examples/#pyramids","title":"Pyramids","text":"<ul> <li>Set-up: Environment where the agent needs to press a button to spawn a   pyramid, then navigate to the pyramid, knock it over, and move to the gold   brick at the top.</li> <li>Goal: Move to the golden brick on top of the spawned pyramid.</li> <li>Agents: The environment contains one agent.</li> <li>Agent Reward Function (independent):</li> <li>+2 For moving to golden brick (minus 0.001 per step).</li> <li>Behavior Parameters:</li> <li>Vector Observation space: 148 corresponding to local ray-casts detecting     switch, bricks, golden brick, and walls, plus variable indicating switch     state.</li> <li>Actions: 1 discrete action branch, with 4 actions corresponding to agent rotation and     forward/backward movement.</li> <li>Float Properties: None</li> <li>Benchmark Mean Reward: 1.75</li> </ul>"},{"location":"Learning-Environment-Examples/#match-3","title":"Match 3","text":"<ul> <li>Set-up: Simple match-3 game. Matched pieces are removed, and remaining pieces drop down. New pieces are spawned randomly at the top, with a chance of being \"special\".</li> <li>Goal: Maximize score from matching pieces.</li> <li>Agents: The environment contains several independent Agents.</li> <li>Agent Reward Function (independent):</li> <li>.01 for each normal piece cleared. Special pieces are worth 2x or 3x.</li> <li>Behavior Parameters:</li> <li>None</li> <li>Observations and actions are defined with a sensor and actuator respectively.</li> <li>Float Properties: None</li> <li>Benchmark Mean Reward:</li> <li>39.5 for visual observations</li> <li>38.5 for vector observations</li> <li>34.2 for simple heuristic (pick a random valid move)</li> <li>37.0 for greedy heuristic (pick the highest-scoring valid move)</li> </ul>"},{"location":"Learning-Environment-Examples/#sorter","title":"Sorter","text":"<ul> <li>Set-up: The Agent is in a circular room with numbered tiles. The values of the  tiles are random between 1 and 20. The tiles present in the room are randomized  at each episode. When the Agent visits a tile, it turns green.</li> <li>Goal: Visit all the tiles in ascending order.</li> <li>Agents: The environment contains a single Agent</li> <li>Agent Reward Function:</li> <li>-.0002 Existential penalty.</li> <li>+1 For visiting the right tile</li> <li>-1 For visiting the wrong tile</li> <li>BehaviorParameters:</li> <li>Vector Observations : 4 : 2 floats for Position and 2 floats for orientation</li> <li>Variable Length Observations : Between 1 and 20 entities (one for each tile)   each with 22 observations, the first 20 are one hot encoding of the value of the tile,   the 21st and 22nd represent the position of the tile relative to the Agent and the 23rd   is <code>1</code> if the tile was visited and <code>0</code> otherwise.</li> <li>Actions: 3 discrete branched actions corresponding to forward, backward,   sideways movement, as well as rotation.</li> <li>Float Properties: One<ul> <li>num_tiles: The maximum number of tiles to sample.</li> <li>Default: 2</li> <li>Recommended Minimum: 1</li> <li>Recommended Maximum: 20</li> </ul> </li> <li>Benchmark Mean Reward: Depends on the number of tiles.</li> </ul>"},{"location":"Learning-Environment-Examples/#cooperative-push-block","title":"Cooperative Push Block","text":"<ul> <li>Set-up: Similar to Push Block, the agents are in an area with blocks that need to be pushed into a goal. Small blocks can be pushed by one agents and are worth +1 value, medium blocks require two agents to push in and are worth +2, and large blocks require all 3 agents to push and are worth +3.</li> <li>Goal: Push all blocks into the goal.</li> <li>Agents: The environment contains three Agents in a Multi Agent Group.</li> <li>Agent Reward Function:</li> <li>-0.0001 Existential penalty, as a group reward.</li> <li>+1, +2, or +3 for pushing in a block, added as a group reward.</li> <li>Behavior Parameters:</li> <li>Observation space: A single Grid Sensor with separate tags for each block size,     the goal, the walls, and other agents.</li> <li>Actions: 1 discrete action branch with 7 actions, corresponding to turn clockwise     and counterclockwise, move along four different face directions, or do nothing.</li> <li>Float Properties: None</li> <li>Benchmark Mean Reward: 11 (Group Reward)</li> </ul>"},{"location":"Learning-Environment-Examples/#dungeon-escape","title":"Dungeon Escape","text":"<ul> <li>Set-up: Agents are trapped in a dungeon with a dragon, and must work together to escape.   To retrieve the key, one of the agents must find and slay the dragon, sacrificing itself   to do so. The dragon will drop a key for the others to use. The other agents can then pick   up this key and unlock the dungeon door. If the agents take too long, the dragon will escape   through a portal and the environment resets.</li> <li>Goal: Unlock the dungeon door and leave.</li> <li>Agents: The environment contains three Agents in a Multi Agent Group and one Dragon, which   moves in a predetermined pattern.</li> <li>Agent Reward Function:</li> <li>+1 group reward if any agent successfully unlocks the door and leaves the dungeon.</li> <li>Behavior Parameters:</li> <li>Observation space: A Ray Perception Sensor with separate tags for the walls, other agents,     the door, key, the dragon, and the dragon's portal. A single Vector Observation which indicates     whether the agent is holding a key.</li> <li>Actions: 1 discrete action branch with 7 actions, corresponding to turn clockwise     and counterclockwise, move along four different face directions, or do nothing.</li> <li>Float Properties: None</li> <li>Benchmark Mean Reward: 1.0 (Group Reward)</li> </ul>"},{"location":"Learning-Environment-Executable/","title":"Using an Environment Executable","text":"<p>This section will help you create and use built environments rather than the Editor to interact with an environment. Using an executable has some advantages over using the Editor:</p> <ul> <li>You can exchange executable with other people without having to share your   entire repository.</li> <li>You can put your executable on a remote machine for faster training.</li> <li>You can use <code>Server Build</code> (<code>Headless</code>) mode for faster training (as long as the executable does not need rendering).</li> <li>You can keep using the Unity Editor for other tasks while the agents are   training.</li> </ul>"},{"location":"Learning-Environment-Executable/#building-the-3dball-environment","title":"Building the 3DBall environment","text":"<p>The first step is to open the Unity scene containing the 3D Balance Ball environment:</p> <ol> <li>Launch Unity.</li> <li>On the Projects dialog, choose the Open option at the top of the window.</li> <li>Using the file dialog that opens, locate the <code>Project</code> folder within the    ML-Agents project and click Open.</li> <li>In the Project window, navigate to the folder    <code>Assets/ML-Agents/Examples/3DBall/Scenes/</code>.</li> <li>Double-click the <code>3DBall</code> file to load the scene containing the Balance Ball    environment.</li> </ol> <p></p> <p>Next, we want the set up scene to play correctly when the training process launches our environment executable. This means:</p> <ul> <li>The environment application runs in the background.</li> <li>No dialogs require interaction.</li> <li> <p>The correct scene loads automatically.</p> </li> <li> <p>Open Player Settings (menu: Edit &gt; Project Settings &gt; Player).</p> </li> <li>Under Resolution and Presentation:</li> <li>Ensure that Run in Background is Checked.</li> <li>Ensure that Display Resolution Dialog is set to Disabled. (Note: this     setting may not be available in newer versions of the editor.)</li> <li>Open the Build Settings window (menu:File &gt; Build Settings).</li> <li>Choose your target platform.</li> <li>(optional) Select \u201cDevelopment Build\u201d to      log debug messages.</li> <li>If any scenes are shown in the Scenes in Build list, make sure that the    3DBall Scene is the only one checked. (If the list is empty, then only the    current scene is included in the build).</li> <li>Click Build:</li> <li>In the File dialog, navigate to your ML-Agents directory.</li> <li>Assign a file name and click Save.</li> <li>(For Windows\uff09With Unity 2018.1, it will ask you to select a folder instead      of a file name. Create a subfolder within the root directory and select      that folder to build. In the following steps you will refer to this      subfolder's name as <code>env_name</code>. You cannot create builds in the Assets      folder</li> </ul> <p></p> <p>Now that we have a Unity executable containing the simulation environment, we can interact with it.</p>"},{"location":"Learning-Environment-Executable/#interacting-with-the-environment","title":"Interacting with the Environment","text":"<p>If you want to use the Python API to interact with your executable, you can pass the name of the executable with the argument 'file_name' of the <code>UnityEnvironment</code>. For instance:</p> <pre><code>from mlagents_envs.environment import UnityEnvironment\nenv = UnityEnvironment(file_name=&lt;env_name&gt;)\n</code></pre>"},{"location":"Learning-Environment-Executable/#training-the-environment","title":"Training the Environment","text":"<ol> <li>Open a command or terminal window.</li> <li>Navigate to the folder where you installed the ML-Agents Toolkit. If you    followed the default installation, then navigate to the    <code>ml-agents/</code> folder.</li> <li>Run    <code>mlagents-learn &lt;trainer-config-file&gt; --env=&lt;env_name&gt; --run-id=&lt;run-identifier&gt;</code>    Where:</li> <li><code>&lt;trainer-config-file&gt;</code> is the file path of the trainer configuration yaml</li> <li><code>&lt;env_name&gt;</code> is the name and path to the executable you exported from Unity      (without extension)</li> <li><code>&lt;run-identifier&gt;</code> is a string used to separate the results of different      training runs</li> </ol> <p>For example, if you are training with a 3DBall executable, and you saved it to the directory where you installed the ML-Agents Toolkit, run:</p> <pre><code>mlagents-learn config/ppo/3DBall.yaml --env=3DBall --run-id=firstRun\n</code></pre> <p>And you should see something like</p> <pre><code>ml-agents$ mlagents-learn config/ppo/3DBall.yaml --env=3DBall --run-id=first-run\n\n\n                        \u2584\u2584\u2584\u2593\u2593\u2593\u2593\n                   \u2553\u2593\u2593\u2593\u2593\u2593\u2593\u2588\u2593\u2593\u2593\u2593\u2593\n              ,\u2584\u2584\u2584m\u2580\u2580\u2580'  ,\u2593\u2593\u2593\u2580\u2593\u2593\u2584                           \u2593\u2593\u2593  \u2593\u2593\u258c\n            \u2584\u2593\u2593\u2593\u2580'      \u2584\u2593\u2593\u2580  \u2593\u2593\u2593      \u2584\u2584     \u2584\u2584 ,\u2584\u2584 \u2584\u2584\u2584\u2584   ,\u2584\u2584 \u2584\u2593\u2593\u258c\u2584 \u2584\u2584\u2584    ,\u2584\u2584\n          \u2584\u2593\u2593\u2593\u2580        \u2584\u2593\u2593\u2580   \u2590\u2593\u2593\u258c     \u2593\u2593\u258c   \u2590\u2593\u2593 \u2590\u2593\u2593\u2593\u2580\u2580\u2580\u2593\u2593\u258c \u2593\u2593\u2593 \u2580\u2593\u2593\u258c\u2580 ^\u2593\u2593\u258c  \u2552\u2593\u2593\u258c\n        \u2584\u2593\u2593\u2593\u2593\u2593\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2593\u2593\u2593      \u2593\u2580      \u2593\u2593\u258c   \u2590\u2593\u2593 \u2590\u2593\u2593    \u2593\u2593\u2593 \u2593\u2593\u2593  \u2593\u2593\u258c   \u2590\u2593\u2593\u2584 \u2593\u2593\u258c\n        \u2580\u2593\u2593\u2593\u2593\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2593\u2593\u2584     \u2593\u2593      \u2593\u2593\u258c   \u2590\u2593\u2593 \u2590\u2593\u2593    \u2593\u2593\u2593 \u2593\u2593\u2593  \u2593\u2593\u258c    \u2590\u2593\u2593\u2590\u2593\u2593\n          ^\u2588\u2593\u2593\u2593        \u2580\u2593\u2593\u2584   \u2590\u2593\u2593\u258c     \u2593\u2593\u2593\u2593\u2584\u2593\u2593\u2593\u2593 \u2590\u2593\u2593    \u2593\u2593\u2593 \u2593\u2593\u2593  \u2593\u2593\u2593\u2584    \u2593\u2593\u2593\u2593`\n            '\u2580\u2593\u2593\u2593\u2584      ^\u2593\u2593\u2593  \u2593\u2593\u2593       \u2514\u2580\u2580\u2580\u2580 \u2580\u2580 ^\u2580\u2580    `\u2580\u2580 `\u2580\u2580   '\u2580\u2580    \u2590\u2593\u2593\u258c\n               \u2580\u2580\u2580\u2580\u2593\u2584\u2584\u2584   \u2593\u2593\u2593\u2593\u2593\u2593,                                      \u2593\u2593\u2593\u2593\u2580\n                   `\u2580\u2588\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u258c\n                        \u00ac`\u2580\u2580\u2580\u2588\u2593\n\n</code></pre> <p>Note: If you're using Anaconda, don't forget to activate the ml-agents environment first.</p> <p>If <code>mlagents-learn</code> runs correctly and starts training, you should see something like this:</p> <pre><code>CrashReporter: initialized\nMono path[0] = '/Users/dericp/workspace/ml-agents/3DBall.app/Contents/Resources/Data/Managed'\nMono config path = '/Users/dericp/workspace/ml-agents/3DBall.app/Contents/MonoBleedingEdge/etc'\nINFO:mlagents_envs:\n'Ball3DAcademy' started successfully!\nUnity Academy name: Ball3DAcademy\n\nINFO:mlagents_envs:Connected new brain:\nUnity brain name: Ball3DLearning\n        Number of Visual Observations (per agent): 0\n        Vector Observation space size (per agent): 8\n        Number of stacked Vector Observation: 1\nINFO:mlagents_envs:Hyperparameters for the PPO Trainer of brain Ball3DLearning:\n        batch_size:          64\n        beta:                0.001\n        buffer_size:         12000\n        epsilon:             0.2\n        gamma:               0.995\n        hidden_units:        128\n        lambd:               0.99\n        learning_rate:       0.0003\n        max_steps:           5.0e4\n        normalize:           True\n        num_epoch:           3\n        num_layers:          2\n        time_horizon:        1000\n        sequence_length:     64\n        summary_freq:        1000\n        use_recurrent:       False\n        memory_size:         256\n        use_curiosity:       False\n        curiosity_strength:  0.01\n        curiosity_enc_size:  128\n        output_path: ./results/first-run-0/Ball3DLearning\nINFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 1000. Mean Reward: 1.242. Std of Reward: 0.746. Training.\nINFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 2000. Mean Reward: 1.319. Std of Reward: 0.693. Training.\nINFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 3000. Mean Reward: 1.804. Std of Reward: 1.056. Training.\nINFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 4000. Mean Reward: 2.151. Std of Reward: 1.432. Training.\nINFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 5000. Mean Reward: 3.175. Std of Reward: 2.250. Training.\nINFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 6000. Mean Reward: 4.898. Std of Reward: 4.019. Training.\nINFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 7000. Mean Reward: 6.716. Std of Reward: 5.125. Training.\nINFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 8000. Mean Reward: 12.124. Std of Reward: 11.929. Training.\nINFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 9000. Mean Reward: 18.151. Std of Reward: 16.871. Training.\nINFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 10000. Mean Reward: 27.284. Std of Reward: 28.667. Training.\n</code></pre> <p>You can press Ctrl+C to stop the training, and your trained model will be at <code>results/&lt;run-identifier&gt;/&lt;behavior_name&gt;.onnx</code>, which corresponds to your model's latest checkpoint. (Note: There is a known bug on Windows that causes the saving of the model to fail when you early terminate the training, it's recommended to wait until Step has reached the max_steps parameter you set in your config YAML.) You can now embed this trained model into your Agent by following the steps below:</p> <ol> <li>Move your model file into    <code>Project/Assets/ML-Agents/Examples/3DBall/TFModels/</code>.</li> <li>Open the Unity Editor, and select the 3DBall scene as described above.</li> <li>Select the 3DBall prefab from the Project window and select Agent.</li> <li>Drag the <code>&lt;behavior_name&gt;.onnx</code> file from the Project window of the Editor to    the Model placeholder in the Ball3DAgent inspector window.</li> <li>Press the Play button at the top of the Editor.</li> </ol>"},{"location":"Learning-Environment-Executable/#training-on-headless-server","title":"Training on Headless Server","text":"<p>To run training on headless server with no graphics rendering support, you need to turn off graphics display in the Unity executable. There are two ways to achieve this: 1. Pass <code>--no-graphics</code> option to mlagents-learn training command. This is equivalent to    adding <code>-nographics -batchmode</code> to the Unity executable's commandline. 2. Build your Unity executable with Server Build. You can find this setting in Build Settings    in the Unity Editor.</p> <p>If you want to train with graphics (for example, using camera and visual observations), you'll need to set up display rendering support (e.g. xvfb) on you server machine. In our Colab Notebook Tutorials, the Setup section has examples of setting up xvfb on servers.</p>"},{"location":"Limitations/","title":"Limitations","text":"<p>See the package-specific Limitations pages:</p> <ul> <li><code>com.unity.mlagents</code> Unity package</li> <li><code>mlagents</code> Python package</li> <li><code>mlagents_envs</code> Python package</li> </ul>"},{"location":"ML-Agents-Envs-README/","title":"Unity ML-Agents Python Interface","text":"<p>The <code>mlagents_envs</code> Python package is part of the ML-Agents Toolkit. <code>mlagents_envs</code> provides three Python APIs that allows direct interaction with the Unity game engine: - A single agent API (Gym API) - A gym-like multi-agent API (PettingZoo API) - A low-level API (LLAPI)</p> <p>The LLAPI is used by the trainer implementation in <code>mlagents</code>. <code>mlagents_envs</code> can be used independently of <code>mlagents</code> for Python communication.</p>"},{"location":"ML-Agents-Envs-README/#installation","title":"Installation","text":"<p>Install the <code>mlagents_envs</code> package with:</p> <pre><code>python -m pip install mlagents_envs==1.0.0\n</code></pre>"},{"location":"ML-Agents-Envs-README/#usage-more-information","title":"Usage &amp; More Information","text":"<p>See - Gym API Guide - PettingZoo API Guide - Python API Guide</p> <p>for more information on how to use the API to interact with a Unity environment.</p> <p>For more information on the ML-Agents Toolkit and how to instrument a Unity scene with the ML-Agents SDK, check out the main ML-Agents Toolkit documentation.</p>"},{"location":"ML-Agents-Envs-README/#limitations","title":"Limitations","text":"<ul> <li><code>mlagents_envs</code> uses localhost ports to exchange data between Unity and   Python. As such, multiple instances can have their ports collide, leading to   errors. Make sure to use a different port if you are using multiple instances   of <code>UnityEnvironment</code>.</li> <li>Communication between Unity and the Python <code>UnityEnvironment</code> is not secure.</li> <li>On Linux, ports are not released immediately after the communication closes.   As such, you cannot reuse ports right after closing a <code>UnityEnvironment</code>.</li> </ul>"},{"location":"ML-Agents-Overview/","title":"ML-Agents Toolkit Overview","text":"<p>Table of Contents</p> <ul> <li>Running Example: Training NPC Behaviors</li> <li>Key Components</li> <li>Training Modes</li> <li>Built-in Training and Inference<ul> <li>Cross-Platform Inference</li> </ul> </li> <li>Custom Training and Inference</li> <li>Flexible Training Scenarios</li> <li>Training Methods: Environment-agnostic</li> <li>A Quick Note on Reward Signals</li> <li>Deep Reinforcement Learning<ul> <li>Curiosity for Sparse-reward Environments</li> <li>RND for Sparse-reward Environments</li> </ul> </li> <li>Imitation Learning<ul> <li>GAIL (Generative Adversarial Imitation Learning)</li> <li>Behavioral Cloning (BC)</li> <li>Recording Demonstrations</li> </ul> </li> <li>Summary</li> <li>Training Methods: Environment-specific</li> <li>Training in Competitive Multi-Agent Environments with Self-Play</li> <li>Training in Cooperative Multi-Agent Environments with MA-POCA</li> <li>Solving Complex Tasks using Curriculum Learning</li> <li>Training Robust Agents using Environment Parameter Randomization</li> <li>Model Types</li> <li>Learning from Vector Observations</li> <li>Learning from Cameras using Convolutional Neural Networks</li> <li>Learning from Variable Length Observations using Attention</li> <li>Memory-enhanced Agents using Recurrent Neural Networks</li> <li>Additional Features</li> <li>Summary and Next Steps</li> </ul> <p>The Unity Machine Learning Agents Toolkit (ML-Agents Toolkit) is an open-source project that enables games and simulations to serve as environments for training intelligent agents. Agents can be trained using reinforcement learning, imitation learning, neuroevolution, or other machine learning methods through a simple-to-use Python API. We also provide implementations (based on PyTorch) of state-of-the-art algorithms to enable game developers and hobbyists to easily train intelligent agents for 2D, 3D and VR/AR games. These trained agents can be used for multiple purposes, including controlling NPC behavior (in a variety of settings such as multi-agent and adversarial), automated testing of game builds and evaluating different game design decisions pre-release. The ML-Agents Toolkit is mutually beneficial for both game developers and AI researchers as it provides a central platform where advances in AI can be evaluated on Unity\u2019s rich environments and then made accessible to the wider research and game developer communities.</p> <p>Depending on your background (i.e. researcher, game developer, hobbyist), you may have very different questions on your mind at the moment. To make your transition to the ML-Agents Toolkit easier, we provide several background pages that include overviews and helpful resources on the Unity Engine, machine learning and PyTorch. We strongly recommend browsing the relevant background pages if you're not familiar with a Unity scene, basic machine learning concepts or have not previously heard of PyTorch.</p> <p>The remainder of this page contains a deep dive into ML-Agents, its key components, different training modes and scenarios. By the end of it, you should have a good sense of what the ML-Agents Toolkit allows you to do. The subsequent documentation pages provide examples of how to use ML-Agents. To get started, watch this demo video of ML-Agents in action.</p>"},{"location":"ML-Agents-Overview/#running-example-training-npc-behaviors","title":"Running Example: Training NPC Behaviors","text":"<p>To help explain the material and terminology in this page, we'll use a hypothetical, running example throughout. We will explore the problem of training the behavior of a non-playable character (NPC) in a game. (An NPC is a game character that is never controlled by a human player and its behavior is pre-defined by the game developer.) More specifically, let's assume we're building a multi-player, war-themed game in which players control the soldiers. In this game, we have a single NPC who serves as a medic, finding and reviving wounded players. Lastly, let us assume that there are two teams, each with five players and one NPC medic.</p> <p>The behavior of a medic is quite complex. It first needs to avoid getting injured, which requires detecting when it is in danger and moving to a safe location. Second, it needs to be aware of which of its team members are injured and require assistance. In the case of multiple injuries, it needs to assess the degree of injury and decide who to help first. Lastly, a good medic will always place itself in a position where it can quickly help its team members. Factoring in all of these traits means that at every instance, the medic needs to measure several attributes of the environment (e.g. position of team members, position of enemies, which of its team members are injured and to what degree) and then decide on an action (e.g. hide from enemy fire, move to help one of its members). Given the large number of settings of the environment and the large number of actions that the medic can take, defining and implementing such complex behaviors by hand is challenging and prone to errors.</p> <p>With ML-Agents, it is possible to train the behaviors of such NPCs (called Agents) using a variety of methods. The basic idea is quite simple. We need to define three entities at every moment of the game (called environment):</p> <ul> <li>Observations - what the medic perceives about the environment.   Observations can be numeric and/or visual. Numeric observations measure   attributes of the environment from the point of view of the agent. For our   medic this would be attributes of the battlefield that are visible to it. For   most interesting environments, an agent will require several continuous   numeric observations. Visual observations, on the other hand, are images   generated from the cameras attached to the agent and represent what the agent   is seeing at that point in time. It is common to confuse an agent's   observation with the environment (or game) state. The environment state   represents information about the entire scene containing all the game   characters. The agents observation, however, only contains information that   the agent is aware of and is typically a subset of the environment state. For   example, the medic observation cannot include information about an enemy in   hiding that the medic is unaware of.</li> <li>Actions - what actions the medic can take. Similar to observations,   actions can either be continuous or discrete depending on the complexity of   the environment and agent. In the case of the medic, if the environment is a   simple grid world where only their location matters, then a discrete action   taking on one of four values (north, south, east, west) suffices. However, if   the environment is more complex and the medic can move freely then using two   continuous actions (one for direction and another for speed) is more   appropriate.</li> <li>Reward signals - a scalar value indicating how well the medic is doing.   Note that the reward signal need not be provided at every moment, but only   when the medic performs an action that is good or bad. For example, it can   receive a large negative reward if it dies, a modest positive reward whenever   it revives a wounded team member, and a modest negative reward when a wounded   team member dies due to lack of assistance. Note that the reward signal is how   the objectives of the task are communicated to the agent, so they need to be   set up in a manner where maximizing reward generates the desired optimal   behavior.</li> </ul> <p>After defining these three entities (the building blocks of a reinforcement learning task), we can now train the medic's behavior. This is achieved by simulating the environment for many trials where the medic, over time, learns what is the optimal action to take for every observation it measures by maximizing its future reward. The key is that by learning the actions that maximize its reward, the medic is learning the behaviors that make it a good medic (i.e. one who saves the most number of lives). In reinforcement learning terminology, the behavior that is learned is called a policy, which is essentially a (optimal) mapping from observations to actions. Note that the process of learning a policy through running simulations is called the training phase, while playing the game with an NPC that is using its learned policy is called the inference phase.</p> <p>The ML-Agents Toolkit provides all the necessary tools for using Unity as the simulation engine for learning the policies of different objects in a Unity environment. In the next few sections, we discuss how the ML-Agents Toolkit achieves this and what features it provides.</p>"},{"location":"ML-Agents-Overview/#key-components","title":"Key Components","text":"<p>The ML-Agents Toolkit contains five high-level components:</p> <ul> <li>Learning Environment - which contains the Unity scene and all the game   characters. The Unity scene provides the environment in which agents observe,   act, and learn. How you set up the Unity scene to serve as a learning   environment really depends on your goal. You may be trying to solve a specific   reinforcement learning problem of limited scope, in which case you can use the   same scene for both training and for testing trained agents. Or, you may be   training agents to operate in a complex game or simulation. In this case, it   might be more efficient and practical to create a purpose-built training   scene. The ML-Agents Toolkit includes an ML-Agents Unity SDK   (<code>com.unity.ml-agents</code> package) that enables you to transform any Unity scene   into a learning environment by defining the agents and their behaviors.</li> <li>Python Low-Level API - which contains a low-level Python interface for   interacting and manipulating a learning environment. Note that, unlike the   Learning Environment, the Python API is not part of Unity, but lives outside   and communicates with Unity through the Communicator. This API is contained in   a dedicated <code>mlagents_envs</code> Python package and is used by the Python training   process to communicate with and control the Academy during training. However,   it can be used for other purposes as well. For example, you could use the API   to use Unity as the simulation engine for your own machine learning   algorithms. See Python API for more information.</li> <li>External Communicator - which connects the Learning Environment with the   Python Low-Level API. It lives within the Learning Environment.</li> <li>Python Trainers which contains all the machine learning algorithms that   enable training agents. The algorithms are implemented in Python and are part   of their own <code>mlagents</code> Python package. The package exposes a single   command-line utility <code>mlagents-learn</code> that supports all the training methods   and options outlined in this document. The Python Trainers interface solely   with the Python Low-Level API.</li> <li>Gym Wrapper (not pictured). A common way in which machine learning   researchers interact with simulation environments is via a wrapper provided by   OpenAI called gym. We provide a gym wrapper   in the <code>ml-agents-envs</code> package and instructions for using   it with existing machine learning algorithms which utilize gym.</li> <li>PettingZoo Wrapper (not pictured) PettingZoo is python API for   interacting with multi-agent simulation environments that provides a   gym-like interface. We provide a PettingZoo wrapper for Unity ML-Agents   environments in the <code>ml-agents-envs</code> package and   instructions for using it with machine learning   algorithms.</li> </ul> <p> </p> <p>Simplified block diagram of ML-Agents.</p> <p>The Learning Environment contains two Unity Components that help organize the Unity scene:</p> <ul> <li>Agents - which is attached to a Unity GameObject (any character within a   scene) and handles generating its observations, performing the actions it   receives and assigning a reward (positive / negative) when appropriate. Each   Agent is linked to a Behavior.</li> <li>Behavior - defines specific attributes of the agent such as the number of   actions that agent can take. Each Behavior is uniquely identified by a   <code>Behavior Name</code> field. A Behavior can be thought as a function that receives   observations and rewards from the Agent and returns actions. A Behavior can be   of one of three types: Learning, Heuristic or Inference. A Learning Behavior   is one that is not, yet, defined but about to be trained. A Heuristic Behavior   is one that is defined by a hard-coded set of rules implemented in code. An   Inference Behavior is one that includes a trained Neural Network file. In   essence, after a Learning Behavior is trained, it becomes an Inference   Behavior.</li> </ul> <p>Every Learning Environment will always have one Agent for every character in the scene. While each Agent must be linked to a Behavior, it is possible for Agents that have similar observations and actions to have the same Behavior. In our sample game, we have two teams each with their own medic. Thus we will have two Agents in our Learning Environment, one for each medic, but both of these medics can have the same Behavior. This does not mean that at each instance they will have identical observation and action values.</p> <p> </p> <p>Example block diagram of ML-Agents Toolkit for our sample game.</p> <p>Note that in a single environment, there can be multiple Agents and multiple Behaviors at the same time. For example, if we expanded our game to include tank driver NPCs, then the Agent attached to those characters cannot share its Behavior with the Agent linked to the medics (medics and drivers have different actions). The Learning Environment through the Academy (not represented in the diagram) ensures that all the Agents are in sync in addition to controlling environment-wide settings.</p> <p>Lastly, it is possible to exchange data between Unity and Python outside of the machine learning loop through Side Channels. One example of using Side Channels is to exchange data with Python about Environment Parameters. The following diagram illustrates the above.</p> <p> </p>"},{"location":"ML-Agents-Overview/#training-modes","title":"Training Modes","text":"<p>Given the flexibility of ML-Agents, there are a few ways in which training and inference can proceed.</p>"},{"location":"ML-Agents-Overview/#built-in-training-and-inference","title":"Built-in Training and Inference","text":"<p>As mentioned previously, the ML-Agents Toolkit ships with several implementations of state-of-the-art algorithms for training intelligent agents. More specifically, during training, all the medics in the scene send their observations to the Python API through the External Communicator. The Python API processes these observations and sends back actions for each medic to take. During training these actions are mostly exploratory to help the Python API learn the best policy for each medic. Once training concludes, the learned policy for each medic can be exported as a model file. Then during the inference phase, the medics still continue to generate their observations, but instead of being sent to the Python API, they will be fed into their (internal, embedded) model to generate the optimal action for each medic to take at every point in time.</p> <p>The Getting Started Guide tutorial covers this training mode with the 3D Balance Ball sample environment.</p>"},{"location":"ML-Agents-Overview/#cross-platform-inference","title":"Cross-Platform Inference","text":"<p>It is important to note that the ML-Agents Toolkit leverages the Sentis to run the models within a Unity scene such that an agent can take the optimal action at each step. Given that Sentis support most platforms that Unity does, this means that any model you train with the ML-Agents Toolkit can be embedded into your Unity application that runs on any platform. See our dedicated blog post for additional information.</p>"},{"location":"ML-Agents-Overview/#custom-training-and-inference","title":"Custom Training and Inference","text":"<p>In the previous mode, the Agents were used for training to generate a PyTorch model that the Agents can later use. However, any user of the ML-Agents Toolkit can leverage their own algorithms for training. In this case, the behaviors of all the Agents in the scene will be controlled within Python. You can even turn your environment into a gym.</p> <p>We do not currently have a tutorial highlighting this mode, but you can learn more about the Python API here.</p>"},{"location":"ML-Agents-Overview/#flexible-training-scenarios","title":"Flexible Training Scenarios","text":"<p>While the discussion so-far has mostly focused on training a single agent, with ML-Agents, several training scenarios are possible. We are excited to see what kinds of novel and fun environments the community creates. For those new to training intelligent agents, below are a few examples that can serve as inspiration:</p> <ul> <li>Single-Agent. A single agent, with its own reward signal. The traditional way   of training an agent. An example is any single-player game, such as Chicken.</li> <li>Simultaneous Single-Agent. Multiple independent agents with independent reward   signals with same <code>Behavior Parameters</code>. A parallelized version of the   traditional training scenario, which can speed-up and stabilize the training   process. Helpful when you have multiple versions of the same character in an   environment who should learn similar behaviors. An example might be training a   dozen robot-arms to each open a door simultaneously.</li> <li>Adversarial Self-Play. Two interacting agents with inverse reward signals. In   two-player games, adversarial self-play can allow an agent to become   increasingly more skilled, while always having the perfectly matched opponent:   itself. This was the strategy employed when training AlphaGo, and more   recently used by OpenAI to train a human-beating 1-vs-1 Dota 2 agent.</li> <li>Cooperative Multi-Agent. Multiple interacting agents with a shared reward   signal with same or different <code>Behavior Parameters</code>. In this scenario, all   agents must work together to accomplish a task that cannot be done alone.   Examples include environments where each agent only has access to partial   information, which needs to be shared in order to accomplish the task or   collaboratively solve a puzzle.</li> <li>Competitive Multi-Agent. Multiple interacting agents with inverse reward   signals with same or different <code>Behavior Parameters</code>. In this scenario, agents   must compete with one another to either win a competition, or obtain some   limited set of resources. All team sports fall into this scenario.</li> <li>Ecosystem. Multiple interacting agents with independent reward signals with   same or different <code>Behavior Parameters</code>. This scenario can be thought of as   creating a small world in which animals with different goals all interact,   such as a savanna in which there might be zebras, elephants and giraffes, or   an autonomous driving simulation within an urban environment.</li> </ul>"},{"location":"ML-Agents-Overview/#training-methods-environment-agnostic","title":"Training Methods: Environment-agnostic","text":"<p>The remaining sections overview the various state-of-the-art machine learning algorithms that are part of the ML-Agents Toolkit. If you aren't studying machine and reinforcement learning as a subject and just want to train agents to accomplish tasks, you can treat these algorithms as black boxes. There are a few training-related parameters to adjust inside Unity as well as on the Python training side, but you do not need in-depth knowledge of the algorithms themselves to successfully create and train agents. Step-by-step procedures for running the training process are provided in the Training ML-Agents page.</p> <p>This section specifically focuses on the training methods that are available regardless of the specifics of your learning environment.</p>"},{"location":"ML-Agents-Overview/#a-quick-note-on-reward-signals","title":"A Quick Note on Reward Signals","text":"<p>In this section we introduce the concepts of intrinsic and extrinsic rewards, which helps explain some of the training methods.</p> <p>In reinforcement learning, the end goal for the Agent is to discover a behavior (a Policy) that maximizes a reward. You will need to provide the agent one or more reward signals to use during training. Typically, a reward is defined by your environment, and corresponds to reaching some goal. These are what we refer to as extrinsic rewards, as they are defined external of the learning algorithm.</p> <p>Rewards, however, can be defined outside of the environment as well, to encourage the agent to behave in certain ways, or to aid the learning of the true extrinsic reward. We refer to these rewards as intrinsic reward signals. The total reward that the agent will learn to maximize can be a mix of extrinsic and intrinsic reward signals.</p> <p>The ML-Agents Toolkit allows reward signals to be defined in a modular way, and we provide four reward signals that can the mixed and matched to help shape your agent's behavior:</p> <ul> <li><code>extrinsic</code>: represents the rewards defined in your environment, and is   enabled by default</li> <li><code>gail</code>: represents an intrinsic reward signal that is defined by GAIL (see   below)</li> <li><code>curiosity</code>: represents an intrinsic reward signal that encourages exploration   in sparse-reward environments that is defined by the Curiosity module (see   below).</li> <li><code>rnd</code>: represents an intrinsic reward signal that encourages exploration   in sparse-reward environments that is defined by the Curiosity module (see   below).</li> </ul>"},{"location":"ML-Agents-Overview/#deep-reinforcement-learning","title":"Deep Reinforcement Learning","text":"<p>ML-Agents provide an implementation of two reinforcement learning algorithms:</p> <ul> <li>Proximal Policy Optimization (PPO)</li> <li>Soft Actor-Critic (SAC)</li> </ul> <p>The default algorithm is PPO. This is a method that has been shown to be more general purpose and stable than many other RL algorithms.</p> <p>In contrast with PPO, SAC is off-policy, which means it can learn from experiences collected at any time during the past. As experiences are collected, they are placed in an experience replay buffer and randomly drawn during training. This makes SAC significantly more sample-efficient, often requiring 5-10 times less samples to learn the same task as PPO. However, SAC tends to require more model updates. SAC is a good choice for heavier or slower environments (about 0.1 seconds per step or more). SAC is also a \"maximum entropy\" algorithm, and enables exploration in an intrinsic way. Read more about maximum entropy RL here.</p>"},{"location":"ML-Agents-Overview/#curiosity-for-sparse-reward-environments","title":"Curiosity for Sparse-reward Environments","text":"<p>In environments where the agent receives rare or infrequent rewards (i.e. sparse-reward), an agent may never receive a reward signal on which to bootstrap its training process. This is a scenario where the use of an intrinsic reward signals can be valuable. Curiosity is one such signal which can help the agent explore when extrinsic rewards are sparse.</p> <p>The <code>curiosity</code> Reward Signal enables the Intrinsic Curiosity Module. This is an implementation of the approach described in Curiosity-driven Exploration by Self-supervised Prediction by Pathak, et al. It trains two networks:</p> <ul> <li>an inverse model, which takes the current and next observation of the agent,   encodes them, and uses the encoding to predict the action that was taken   between the observations</li> <li>a forward model, which takes the encoded current observation and action, and   predicts the next encoded observation.</li> </ul> <p>The loss of the forward model (the difference between the predicted and actual encoded observations) is used as the intrinsic reward, so the more surprised the model is, the larger the reward will be.</p> <p>For more information, see our dedicated blog post on the Curiosity module.</p>"},{"location":"ML-Agents-Overview/#rnd-for-sparse-reward-environments","title":"RND for Sparse-reward Environments","text":"<p>Similarly to Curiosity, Random Network Distillation (RND) is useful in sparse or rare reward environments as it helps the Agent explore. The RND Module is implemented following the paper Exploration by Random Network Distillation. RND uses two networks:  - The first is a network with fixed random weights that takes observations as inputs and  generates an encoding  - The second is a network with similar architecture that is trained to predict the  outputs of the first network and uses the observations the Agent collects as training data.</p> <p>The loss (the squared difference between the predicted and actual encoded observations) of the trained model is used as intrinsic reward. The more an Agent visits a state, the more accurate the predictions and the lower the rewards which encourages the Agent to explore new states with higher prediction errors.</p>"},{"location":"ML-Agents-Overview/#imitation-learning","title":"Imitation Learning","text":"<p>It is often more intuitive to simply demonstrate the behavior we want an agent to perform, rather than attempting to have it learn via trial-and-error methods. For example, instead of indirectly training a medic with the help of a reward function, we can give the medic real world examples of observations from the game and actions from a game controller to guide the medic's behavior. Imitation Learning uses pairs of observations and actions from a demonstration to learn a policy. See this video demo of imitation learning .</p> <p>Imitation learning can either be used alone or in conjunction with reinforcement learning. If used alone it can provide a mechanism for learning a specific type of behavior (i.e. a specific style of solving the task). If used in conjunction with reinforcement learning it can dramatically reduce the time the agent takes to solve the environment. This can be especially pronounced in sparse-reward environments. For instance, on the Pyramids environment, using 6 episodes of demonstrations can reduce training steps by more than 4 times. See Behavioral Cloning + GAIL + Curiosity + RL below.</p> <p> </p> <p>The ML-Agents Toolkit provides a way to learn directly from demonstrations, as well as use them to help speed up reward-based training (RL). We include two algorithms called Behavioral Cloning (BC) and Generative Adversarial Imitation Learning (GAIL). In most scenarios, you can combine these two features:</p> <ul> <li>If you want to help your agents learn (especially with environments that have   sparse rewards) using pre-recorded demonstrations, you can generally enable   both GAIL and Behavioral Cloning at low strengths in addition to having an   extrinsic reward. An example of this is provided for the PushBlock example   environment in <code>config/imitation/PushBlock.yaml</code>.</li> <li>If you want to train purely from demonstrations with GAIL and BC without an   extrinsic reward signal, please see the CrawlerStatic example environment under   in <code>config/imitation/CrawlerStatic.yaml</code>.</li> </ul> <p>Note: GAIL introduces a survivor bias to the learning process. That is, by giving positive rewards based on similarity to the expert, the agent is incentivized to remain alive for as long as possible. This can directly conflict with goal-oriented tasks like our PushBlock or Pyramids example environments where an agent must reach a goal state thus ending the episode as quickly as possible. In these cases, we strongly recommend that you use a low strength GAIL reward signal and a sparse extrinisic signal when the agent achieves the task. This way, the GAIL reward signal will guide the agent until it discovers the extrnisic signal and will not overpower it. If the agent appears to be ignoring the extrinsic reward signal, you should reduce the strength of GAIL.</p>"},{"location":"ML-Agents-Overview/#gail-generative-adversarial-imitation-learning","title":"GAIL (Generative Adversarial Imitation Learning)","text":"<p>GAIL, or Generative Adversarial Imitation Learning, uses an adversarial approach to reward your Agent for behaving similar to a set of demonstrations. GAIL can be used with or without environment rewards, and works well when there are a limited number of demonstrations. In this framework, a second neural network, the discriminator, is taught to distinguish whether an observation/action is from a demonstration or produced by the agent. This discriminator can then examine a new observation/action and provide it a reward based on how close it believes this new observation/action is to the provided demonstrations.</p> <p>At each training step, the agent tries to learn how to maximize this reward. Then, the discriminator is trained to better distinguish between demonstrations and agent state/actions. In this way, while the agent gets better and better at mimicking the demonstrations, the discriminator keeps getting stricter and stricter and the agent must try harder to \"fool\" it.</p> <p>This approach learns a policy that produces states and actions similar to the demonstrations, requiring fewer demonstrations than direct cloning of the actions. In addition to learning purely from demonstrations, the GAIL reward signal can be mixed with an extrinsic reward signal to guide the learning process.</p>"},{"location":"ML-Agents-Overview/#behavioral-cloning-bc","title":"Behavioral Cloning (BC)","text":"<p>BC trains the Agent's policy to exactly mimic the actions shown in a set of demonstrations. The BC feature can be enabled on the PPO or SAC trainers. As BC cannot generalize past the examples shown in the demonstrations, BC tends to work best when there exists demonstrations for nearly all of the states that the agent can experience, or in conjunction with GAIL and/or an extrinsic reward.</p>"},{"location":"ML-Agents-Overview/#recording-demonstrations","title":"Recording Demonstrations","text":"<p>Demonstrations of agent behavior can be recorded from the Unity Editor or build, and saved as assets. These demonstrations contain information on the observations, actions, and rewards for a given agent during the recording session. They can be managed in the Editor, as well as used for training with BC and GAIL. See the Designing Agents page for more information on how to record demonstrations for your agent.</p>"},{"location":"ML-Agents-Overview/#summary","title":"Summary","text":"<p>To summarize, we provide 3 training methods: BC, GAIL and RL (PPO or SAC) that can be used independently or together:</p> <ul> <li>BC can be used on its own or as a pre-training step before GAIL and/or RL</li> <li>GAIL can be used with or without extrinsic rewards</li> <li>RL can be used on its own (either PPO or SAC) or in conjunction with BC and/or   GAIL.</li> </ul> <p>Leveraging either BC or GAIL requires recording demonstrations to be provided as input to the training algorithms.</p>"},{"location":"ML-Agents-Overview/#training-methods-environment-specific","title":"Training Methods: Environment-specific","text":"<p>In addition to the three environment-agnostic training methods introduced in the previous section, the ML-Agents Toolkit provides additional methods that can aid in training behaviors for specific types of environments.</p>"},{"location":"ML-Agents-Overview/#training-in-competitive-multi-agent-environments-with-self-play","title":"Training in Competitive Multi-Agent Environments with Self-Play","text":"<p>ML-Agents provides the functionality to train both symmetric and asymmetric adversarial games with Self-Play. A symmetric game is one in which opposing agents are equal in form, function and objective. Examples of symmetric games are our Tennis and Soccer example environments. In reinforcement learning, this means both agents have the same observation and actions and learn from the same reward function and so they can share the same policy. In asymmetric games, this is not the case. An example of an asymmetric games are Hide and Seek. Agents in these types of games do not always have the same observation or actions and so sharing policy networks is not necessarily ideal.</p> <p>With self-play, an agent learns in adversarial games by competing against fixed, past versions of its opponent (which could be itself as in symmetric games) to provide a more stable, stationary learning environment. This is compared to competing against the current, best opponent in every episode, which is constantly changing (because it's learning).</p> <p>Self-play can be used with our implementations of both Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC). However, from the perspective of an individual agent, these scenarios appear to have non-stationary dynamics because the opponent is often changing. This can cause significant issues in the experience replay mechanism used by SAC. Thus, we recommend that users use PPO. For further reading on this issue in particular, see the paper Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning.</p> <p>See our Designing Agents page for more information on setting up teams in your Unity scene. Also, read our blog post on self-play for additional information. Additionally, check ELO Rating System the method we use to calculate the relative skill level between two players.</p>"},{"location":"ML-Agents-Overview/#training-in-cooperative-multi-agent-environments-with-ma-poca","title":"Training In Cooperative Multi-Agent Environments with MA-POCA","text":"<p>ML-Agents provides the functionality for training cooperative behaviors - i.e., groups of agents working towards a common goal, where the success of the individual is linked to the success of the whole group. In such a scenario, agents typically receive rewards as a group. For instance, if a team of agents wins a game against an opposing team, everyone is rewarded - even agents who did not directly contribute to the win. This makes learning what to do as an individual difficult - you may get a win for doing nothing, and a loss for doing your best.</p> <p>In ML-Agents, we provide MA-POCA (MultiAgent POsthumous Credit Assignment), which is a novel multi-agent trainer that trains a centralized critic, a neural network that acts as a \"coach\" for a whole group of agents. You can then give rewards to the team as a whole, and the agents will learn how best to contribute to achieving that reward. Agents can also be given rewards individually, and the team will work together to help the individual achieve those goals. During an episode, agents can be added or removed from the group, such as when agents spawn or die in a game. If agents are removed mid-episode (e.g., if teammates die or are removed from the game), they will still learn whether their actions contributed to the team winning later, enabling agents to take group-beneficial actions even if they result in the individual being removed from the game (i.e., self-sacrifice). MA-POCA can also be combined with self-play to train teams of agents to play against each other.</p> <p>To learn more about enabling cooperative behaviors for agents in an ML-Agents environment, check out this page.</p> <p>To learn more about MA-POCA, please see our paper On the Use and Misuse of Absorbing States in Multi-Agent Reinforcement Learning. For further reading, MA-POCA builds on previous work in multi-agent cooperative learning (Lowe et al., Foerster et al., among others) to enable the above use-cases.</p>"},{"location":"ML-Agents-Overview/#solving-complex-tasks-using-curriculum-learning","title":"Solving Complex Tasks using Curriculum Learning","text":"<p>Curriculum learning is a way of training a machine learning model where more difficult aspects of a problem are gradually introduced in such a way that the model is always optimally challenged. This idea has been around for a long time, and it is how we humans typically learn. If you imagine any childhood primary school education, there is an ordering of classes and topics. Arithmetic is taught before algebra, for example. Likewise, algebra is taught before calculus. The skills and knowledge learned in the earlier subjects provide a scaffolding for later lessons. The same principle can be applied to machine learning, where training on easier tasks can provide a scaffolding for harder tasks in the future.</p> <p>Imagine training the medic to scale a wall to arrive at a wounded team member. The starting point when training a medic to accomplish this task will be a random policy. That starting policy will have the medic running in circles, and will likely never, or very rarely scale the wall properly to revive their team member (and achieve the reward). If we start with a simpler task, such as moving toward an unobstructed team member, then the medic can easily learn to accomplish the task. From there, we can slowly add to the difficulty of the task by increasing the size of the wall until the medic can complete the initially near-impossible task of scaling the wall. We have included an environment to demonstrate this with ML-Agents, called Wall Jump.</p> <p></p> <p>Demonstration of a hypothetical curriculum training scenario in which a progressively taller wall obstructs the path to the goal.</p> <p>[Note: The example provided above is for instructional purposes, and was based on an early version of the Wall Jump example environment. As such, it is not possible to directly replicate the results here using that environment.]</p> <p>The ML-Agents Toolkit supports modifying custom environment parameters during the training process to aid in learning. This allows elements of the environment related to difficulty or complexity to be dynamically adjusted based on training progress. The Training ML-Agents page has more information on defining training curriculums.</p>"},{"location":"ML-Agents-Overview/#training-robust-agents-using-environment-parameter-randomization","title":"Training Robust Agents using Environment Parameter Randomization","text":"<p>An agent trained on a specific environment, may be unable to generalize to any tweaks or variations in the environment (in machine learning this is referred to as overfitting). This becomes problematic in cases where environments are instantiated with varying objects or properties. One mechanism to alleviate this and train more robust agents that can generalize to unseen variations of the environment is to expose them to these variations during training. Similar to Curriculum Learning, where environments become more difficult as the agent learns, the ML-Agents Toolkit provides a way to randomly sample parameters of the environment during training. We refer to this approach as Environment Parameter Randomization. For those familiar with Reinforcement Learning research, this approach is based on the concept of Domain Randomization. By using parameter randomization during training, the agent can be better suited to adapt (with higher performance) to future unseen variations of the environment.</p> Ball scale of 0.5 Ball scale of 4 <p>Example of variations of the 3D Ball environment. The environment parameters are <code>gravity</code>, <code>ball_mass</code> and <code>ball_scale</code>.</p>"},{"location":"ML-Agents-Overview/#model-types","title":"Model Types","text":"<p>Regardless of the training method deployed, there are a few model types that users can train using the ML-Agents Toolkit. This is due to the flexibility in defining agent observations, which include vector, ray cast and visual observations. You can learn more about how to instrument an agent's observation in the Designing Agents guide.</p>"},{"location":"ML-Agents-Overview/#learning-from-vector-observations","title":"Learning from Vector Observations","text":"<p>Whether an agent's observations are ray cast or vector, the ML-Agents Toolkit provides a fully connected neural network model to learn from those observations. At training time you can configure different aspects of this model such as the number of hidden units and number of layers.</p>"},{"location":"ML-Agents-Overview/#learning-from-cameras-using-convolutional-neural-networks","title":"Learning from Cameras using Convolutional Neural Networks","text":"<p>Unlike other platforms, where the agent\u2019s observation might be limited to a single vector or image, the ML-Agents Toolkit allows multiple cameras to be used for observations per agent. This enables agents to learn to integrate information from multiple visual streams. This can be helpful in several scenarios such as training a self-driving car which requires multiple cameras with different viewpoints, or a navigational agent which might need to integrate aerial and first-person visuals. You can learn more about adding visual observations to an agent here.</p> <p>When visual observations are utilized, the ML-Agents Toolkit leverages convolutional neural networks (CNN) to learn from the input images. We offer three network architectures:</p> <ul> <li>a simple encoder which consists of two convolutional layers</li> <li>the implementation proposed by   Mnih et al., consisting of   three convolutional layers,</li> <li>the IMPALA Resnet consisting of three   stacked layers, each with two residual blocks, making a much larger network   than the other two.</li> </ul> <p>The choice of the architecture depends on the visual complexity of the scene and the available computational resources.</p>"},{"location":"ML-Agents-Overview/#learning-from-variable-length-observations-using-attention","title":"Learning from Variable Length Observations using Attention","text":"<p>Using the ML-Agents Toolkit, it is possible to have agents learn from a varying number of inputs. To do so, each agent can keep track of a buffer of vector observations. At each step, the agent will go through all the elements in the buffer and extract information but the elements in the buffer can change at every step. This can be useful in scenarios in which the agents must keep track of a varying number of elements throughout the episode. For example in a game where an agent must learn to avoid projectiles, but the projectiles can vary in numbers.</p> <p></p> <p>You can learn more about variable length observations here. When variable length observations are utilized, the ML-Agents Toolkit leverages attention networks to learn from a varying number of entities. Agents using attention will ignore entities that are deemed not relevant and pay special attention to entities relevant to the current situation based on context.</p>"},{"location":"ML-Agents-Overview/#memory-enhanced-agents-using-recurrent-neural-networks","title":"Memory-enhanced Agents using Recurrent Neural Networks","text":"<p>Have you ever entered a room to get something and immediately forgot what you were looking for? Don't let that happen to your agents.</p> <p></p> <p>In some scenarios, agents must learn to remember the past in order to take the best decision. When an agent only has partial observability of the environment, keeping track of past observations can help the agent learn. Deciding what the agents should remember in order to solve a task is not easy to do by hand, but our training algorithms can learn to keep track of what is important to remember with LSTM.</p>"},{"location":"ML-Agents-Overview/#additional-features","title":"Additional Features","text":"<p>Beyond the flexible training scenarios available, the ML-Agents Toolkit includes additional features which improve the flexibility and interpretability of the training process.</p> <ul> <li>Concurrent Unity Instances - We enable developers to run concurrent,   parallel instances of the Unity executable during training. For certain   scenarios, this should speed up training. Check out our dedicated page on   creating a Unity executable and the   Training ML-Agents   page for instructions on how to set the number of concurrent instances.</li> <li>Recording Statistics from Unity - We enable developers to   record statistics from   within their Unity environments. These statistics are aggregated and generated   during the training process.</li> <li>Custom Side Channels - We enable developers to   create custom side channels to manage data transfer   between Unity and Python that is unique to their training workflow and/or   environment.</li> <li>Custom Samplers - We enable developers to   create custom sampling methods   for Environment Parameter Randomization. This enables users to customize this   training method for their particular environment.</li> </ul>"},{"location":"ML-Agents-Overview/#summary-and-next-steps","title":"Summary and Next Steps","text":"<p>To briefly summarize: The ML-Agents Toolkit enables games and simulations built in Unity to serve as the platform for training intelligent agents. It is designed to enable a large variety of training modes and scenarios and comes packed with several features to enable researchers and developers to leverage (and enhance) machine learning within Unity.</p> <p>In terms of next steps:</p> <ul> <li>For a walkthrough of running ML-Agents with a simple scene, check out the   Getting Started guide.</li> <li>For a \"Hello World\" introduction to creating your own Learning Environment,   check out the   Making a New Learning Environment page.</li> <li>For an overview on the more complex example environments that are provided in   this toolkit, check out the   Example Environments page.</li> <li>For more information on the various training options available, check out the   Training ML-Agents page.</li> </ul>"},{"location":"ML-Agents-README/","title":"Unity ML-Agents Trainers","text":"<p>The <code>mlagents</code> Python package is part of the ML-Agents Toolkit. <code>mlagents</code> provides a set of reinforcement and imitation learning algorithms designed to be used with Unity environments. The algorithms interface with the Python API provided by the <code>mlagents_envs</code> package. See here for more information on <code>mlagents_envs</code>.</p> <p>The algorithms can be accessed using the: <code>mlagents-learn</code> access point. See here for more information on using this package.</p>"},{"location":"ML-Agents-README/#installation","title":"Installation","text":"<p>Install the <code>mlagents</code> package with:</p> <pre><code>python -m pip install mlagents==1.0.0\n</code></pre>"},{"location":"ML-Agents-README/#usage-more-information","title":"Usage &amp; More Information","text":"<p>For more information on the ML-Agents Toolkit and how to instrument a Unity scene with the ML-Agents SDK, check out the main ML-Agents Toolkit documentation.</p>"},{"location":"ML-Agents-README/#limitations","title":"Limitations","text":"<ul> <li>Resuming self-play from a checkpoint resets the reported ELO to the default   value.</li> </ul>"},{"location":"ML-Agents-Toolkit-Documentation/","title":"Unity ML-Agents Toolkit Documentation","text":""},{"location":"ML-Agents-Toolkit-Documentation/#installation-set-up","title":"Installation &amp; Set-up","text":"<ul> <li>Installation</li> <li>Using Virtual Environment</li> </ul>"},{"location":"ML-Agents-Toolkit-Documentation/#getting-started","title":"Getting Started","text":"<ul> <li>Getting Started Guide</li> <li>ML-Agents Toolkit Overview</li> <li>Background: Unity</li> <li>Background: Machine Learning</li> <li>Background: PyTorch</li> <li>Example Environments</li> </ul>"},{"location":"ML-Agents-Toolkit-Documentation/#creating-learning-environments","title":"Creating Learning Environments","text":"<ul> <li>Making a New Learning Environment</li> <li>Designing a Learning Environment</li> <li>Designing Agents</li> <li>Using an Executable Environment</li> <li>ML-Agents Package Settings</li> </ul>"},{"location":"ML-Agents-Toolkit-Documentation/#training-inference","title":"Training &amp; Inference","text":"<ul> <li>Training ML-Agents</li> <li>Training Configuration File</li> <li>Using TensorBoard to Observe Training</li> <li>Profiling Trainers</li> <li>Sentis</li> </ul>"},{"location":"ML-Agents-Toolkit-Documentation/#extending-ml-agents","title":"Extending ML-Agents","text":"<ul> <li>Creating Custom Side Channels</li> <li>Creating Custom Samplers for Environment Parameter Randomization</li> </ul>"},{"location":"ML-Agents-Toolkit-Documentation/#hugging-face-integration","title":"Hugging Face Integration","text":"<ul> <li>Using Hugging Face to download and upload trained models</li> </ul>"},{"location":"ML-Agents-Toolkit-Documentation/#python-tutorial-with-google-colab","title":"Python Tutorial with Google Colab","text":"<ul> <li>Using a UnityEnvironment</li> <li>Q-Learning with a UnityEnvironment</li> <li>Using Side Channels on a UnityEnvironment</li> </ul>"},{"location":"ML-Agents-Toolkit-Documentation/#help","title":"Help","text":"<ul> <li>Migrating from earlier versions of ML-Agents</li> <li>Frequently Asked Questions</li> <li>ML-Agents Glossary</li> <li>Limitations</li> </ul>"},{"location":"ML-Agents-Toolkit-Documentation/#api-docs","title":"API Docs","text":"<ul> <li>API Reference</li> <li>Python API Documentation</li> <li>How to use the Python API</li> <li>How to use the Unity Environment Registry</li> <li>Wrapping Learning Environment as a Gym (+Baselines/Dopamine Integration)</li> </ul>"},{"location":"ML-Agents-Toolkit-Documentation/#translations","title":"Translations","text":"<p>To make the Unity ML-Agents Toolkit accessible to the global research and Unity developer communities, we're attempting to create and maintain translations of our documentation. We've started with translating a subset of the documentation to one language (Chinese), but we hope to continue translating more pages and to other languages. Consequently, we welcome any enhancements and improvements from the community.</p> <ul> <li>Chinese</li> <li>Korean</li> </ul>"},{"location":"ML-Agents-Toolkit-Documentation/#deprecated-docs","title":"Deprecated Docs","text":"<p>We no longer use them ourselves and so they may not be up-to-date. We've decided to keep them up just in case they are helpful to you.</p> <ul> <li>Windows Anaconda Installation</li> <li>Using Docker</li> <li>Training on the Cloud with Amazon Web Services</li> <li>Training on the Cloud with Microsoft Azure</li> <li>Using the Video Recorder</li> </ul>"},{"location":"Migrating/","title":"Upgrading","text":""},{"location":"Migrating/#migrating","title":"Migrating","text":""},{"location":"Migrating/#migrating-to-the-ml-agents-envs-0300-package","title":"Migrating to the ml-agents-envs 0.30.0 package","text":"<ul> <li>Python 3.10.12 is now the minimum version of python supported due to python3.6 EOL.   Please update your python installation to 3.10.12 or higher.</li> <li>The <code>gym-unity</code> package has been refactored into the <code>ml-agents-envs</code> package. Please update your imports accordingly.</li> <li>Example:</li> <li>Before</li> </ul> <pre><code>from gym_unity.unity_gym_env import UnityToGymWrapper\n</code></pre> <ul> <li>After:</li> </ul> <pre><code>from mlagents_envs.envs.unity_gym_env import UnityToGymWrapper\n</code></pre>"},{"location":"Migrating/#migrating-the-package-to-version-2x","title":"Migrating the package to version 2.x","text":"<ul> <li>The official version of Unity ML-Agents supports is now 2022.3 LTS. If you run   into issues, please consider deleting your project's Library folder and reponening your   project.</li> <li>If you used any of the APIs that were deprecated before version 2.0, you need to use their replacement. These deprecated APIs have been removed. See the migration steps bellow for specific API replacements.</li> </ul>"},{"location":"Migrating/#deprecated-methods-removed","title":"Deprecated methods removed","text":"Deprecated API Suggested Replacement <code>IActuator ActuatorComponent.CreateActuator()</code> <code>IActuator[] ActuatorComponent.CreateActuators()</code> <code>IActionReceiver.PackActions(in float[] destination)</code> none <code>Agent.CollectDiscreteActionMasks(DiscreteActionMasker actionMasker)</code> <code>Agent.WriteDiscreteActionMask(IDiscreteActionMask actionMask)</code> <code>Agent.Heuristic(float[] actionsOut)</code> <code>Agent.Heuristic(in ActionBuffers actionsOut)</code> <code>Agent.OnActionReceived(float[] vectorAction)</code> <code>Agent.OnActionReceived(ActionBuffers actions)</code> <code>Agent.GetAction()</code> <code>Agent.GetStoredActionBuffers()</code> <code>BrainParameters.SpaceType</code>, <code>VectorActionSize</code>, <code>VectorActionSpaceType</code>, and <code>NumActions</code> <code>BrainParameters.ActionSpec</code> <code>ObservationWriter.AddRange(IEnumerable&lt;float&gt; data, int writeOffset = 0)</code> <code>ObservationWriter. AddList(IList&lt;float&gt; data, int writeOffset = 0</code> <code>SensorComponent.IsVisual()</code> and <code>IsVector()</code> none <code>VectorSensor.AddObservation(IEnumerable&lt;float&gt; observation)</code> <code>VectorSensor.AddObservation(IList&lt;float&gt; observation)</code> <code>SideChannelsManager</code> <code>SideChannelManager</code>"},{"location":"Migrating/#idiscreteactionmask-changes","title":"IDiscreteActionMask changes","text":"<ul> <li>The interface for disabling specific discrete actions has changed. <code>IDiscreteActionMask.WriteMask()</code> was removed, and replaced with <code>SetActionEnabled()</code>. Instead of returning an IEnumerable with indices to disable, you can now call <code>SetActionEnabled</code> for each index to disable (or enable). As an example, if you overrode <code>Agent.WriteDiscreteActionMask()</code> with something that looked like:</li> </ul> <pre><code>public override void WriteDiscreteActionMask(IDiscreteActionMask actionMask)\n{\n    var branch = 2;\n    var actionsToDisable = new[] {1, 3};\n    actionMask.WriteMask(branch, actionsToDisable);\n}\n</code></pre> <p>the equivalent code would now be</p> <pre><code>public override void WriteDiscreteActionMask(IDiscreteActionMask actionMask)\n{\n    var branch = 2;\n    actionMask.SetActionEnabled(branch, 1, false);\n    actionMask.SetActionEnabled(branch, 3, false);\n}\n</code></pre>"},{"location":"Migrating/#iactuator-changes","title":"IActuator changes","text":"<ul> <li>The <code>IActuator</code> interface now implements <code>IHeuristicProvider</code>.  Please add the corresponding <code>Heuristic(in ActionBuffers)</code> method to your custom Actuator classes.</li> </ul>"},{"location":"Migrating/#isensor-and-sensorcomponent-changes","title":"ISensor and SensorComponent changes","text":"<ul> <li>The <code>ISensor.GetObservationShape()</code> method and <code>ITypedSensor</code> and <code>IDimensionPropertiesSensor</code> interfaces were removed, and <code>GetObservationSpec()</code> was added. You can use <code>ObservationSpec.Vector()</code> or <code>ObservationSpec.Visual()</code> to generate <code>ObservationSpec</code>s that are equivalent to the previous shape. For example, if your old ISensor looked like:</li> </ul> <pre><code>public override int[] GetObservationShape()\n{\n    return new[] { m_Height, m_Width, m_NumChannels };\n}\n</code></pre> <p>the equivalent code would now be</p> <pre><code>public override ObservationSpec GetObservationSpec()\n{\n    return ObservationSpec.Visual(m_Height, m_Width, m_NumChannels);\n}\n</code></pre> <ul> <li>The <code>ISensor.GetCompressionType()</code> method and <code>ISparseChannelSensor</code> interface was removed, and <code>GetCompressionSpec()</code> was added. You can use <code>CompressionSpec.Default()</code> or <code>CompressionSpec.Compressed()</code> to generate <code>CompressionSpec</code>s that are  equivalent to  the previous values. For example, if your old ISensor looked like:  ```csharp public virtual SensorCompressionType GetCompressionType() {     return SensorCompressionType.None; }</li> </ul> <pre><code>\nthe equivalent code would now be\n\n```csharp\npublic CompressionSpec GetCompressionSpec()\n{\n    return CompressionSpec.Default();\n}\n</code></pre> <ul> <li>The abstract method <code>SensorComponent.GetObservationShape()</code> was removed.</li> <li>The abstract method <code>SensorComponent.CreateSensor()</code> was replaced with <code>CreateSensors()</code>, which returns an <code>ISensor[]</code>.</li> </ul>"},{"location":"Migrating/#match3-integration-changes","title":"Match3 integration changes","text":"<p>The Match-3 integration utilities were moved from <code>com.unity.ml-agents.extensions</code> to <code>com.unity.ml-agents</code>.</p> <p>The <code>AbstractBoard</code> interface was changed: * <code>AbstractBoard</code> no longer contains <code>Rows</code>, <code>Columns</code>, <code>NumCellTypes</code>, and <code>NumSpecialTypes</code> fields. * <code>public abstract BoardSize GetMaxBoardSize()</code> was added as an abstract method. <code>BoardSize</code> is a new struct that contains <code>Rows</code>, <code>Columns</code>, <code>NumCellTypes</code>, and <code>NumSpecialTypes</code> fields, with the same meanings as the old <code>AbstractBoard</code> fields. * <code>public virtual BoardSize GetCurrentBoardSize()</code> is an optional method; by default it returns <code>GetMaxBoardSize()</code>. If you wish to use a single behavior to work with multiple board sizes, override <code>GetCurrentBoardSize()</code> to return the current <code>BoardSize</code>. The values returned by <code>GetCurrentBoardSize()</code> must be less than or equal to the corresponding values from <code>GetMaxBoardSize()</code>.</p>"},{"location":"Migrating/#gridsensor-changes","title":"GridSensor changes","text":"<p>The sensor configuration has changed: * The sensor implementation has been refactored and exsisting GridSensor created from extension package will not work in newer version. Some errors might show up when loading the old sensor in the scene. You'll need to remove the old sensor and create a new GridSensor. * These parameters names have changed but still refer to the same concept in the sensor: <code>GridNumSide</code> -&gt; <code>GridSize</code>, <code>RotateToAgent</code> -&gt; <code>RotateWithAgent</code>, <code>ObserveMask</code> -&gt; <code>ColliderMask</code>, <code>DetectableObjects</code> -&gt; <code>DetectableTags</code> * <code>DepthType</code> (<code>ChanelBase</code>/<code>ChannelHot</code>) option and <code>ChannelDepth</code> are removed. Now the default is one-hot encoding for detected tag. If you were using original GridSensor without overriding any method, switching to new GridSensor will produce similar effect for training although the actual observations will be slightly different.</p> <p>For creating your GridSensor implementation with custom data: * To create custom GridSensor, derive from <code>GridSensorBase</code> instead of <code>GridSensor</code>. Besides overriding <code>GetObjectData()</code>, you will also need to consider override <code>GetCellObservationSize()</code>, <code>IsDataNormalized()</code> and <code>GetProcessCollidersMethod()</code> according to the data you collect. Also you'll need to override <code>GridSensorComponent.GetGridSensors()</code> and return your custom GridSensor. * The input argument <code>tagIndex</code> in <code>GetObjectData()</code> has changed from 1-indexed to 0-indexed and the data type changed from <code>float</code> to <code>int</code>. The index of first detectable tag will be 0 instead of 1. <code>normalizedDistance</code> was removed from input. * The observation data should be written to the input <code>dataBuffer</code> instead of creating and returning a new array. * Removed the constraint of all data required to be normalized. You should specify it in <code>IsDataNormalized()</code>. Sensors with non-normalized data cannot use PNG compression type. * The sensor will not further encode the data recieved from <code>GetObjectData()</code> anymore. The values recieved from <code>GetObjectData()</code> will be the observation sent to the trainer.</p>"},{"location":"Migrating/#lstm-models-from-previous-releases-no-longer-supported","title":"LSTM models from previous releases no longer supported","text":"<p>The way that Sentis processes LSTM (recurrent neural networks) has changed. As a result, models trained with previous versions of ML-Agents will not be usable at inference if they were trained with a <code>memory</code> setting in the <code>.yaml</code> config file. If you want to use a model that has a recurrent neural network in this release of ML-Agents, you need to train the model using the python trainer from this release.</p>"},{"location":"Migrating/#migrating-to-release-13","title":"Migrating to Release 13","text":""},{"location":"Migrating/#implementing-iheuristic-in-your-iactuator-implementations","title":"Implementing IHeuristic in your IActuator implementations","text":"<ul> <li>If you have any custom actuators, you can now implement the <code>IHeuristicProvider</code> interface to have your actuator   handle the generation of actions when an Agent is running in heuristic mode.</li> <li><code>VectorSensor.AddObservation(IEnumerable&lt;float&gt;)</code> is deprecated. Use <code>VectorSensor.AddObservation(IList&lt;float&gt;)</code>   instead.</li> <li><code>ObservationWriter.AddRange()</code> is deprecated. Use <code>ObservationWriter.AddList()</code> instead.</li> <li><code>ActuatorComponent.CreateAcuator()</code> is deprecated.  Please use override <code>ActuatorComponent.CreateActuators</code>   instead.  Since <code>ActuatorComponent.CreateActuator()</code> is abstract, you will still need to override it in your   class until it is removed.  It is only ever called if you don't override <code>ActuatorComponent.CreateActuators</code>.   You can suppress the warnings by surrounding the method with the following pragma:     <code>c#     #pragma warning disable 672     public IActuator CreateActuator() { ... }     #pragma warning restore 672</code></li> </ul>"},{"location":"Migrating/#migrating_1","title":"Migrating","text":""},{"location":"Migrating/#migrating-to-release-11","title":"Migrating to Release 11","text":""},{"location":"Migrating/#agent-virtual-method-deprecation","title":"Agent virtual method deprecation","text":"<ul> <li><code>Agent.CollectDiscreteActionMasks()</code> was deprecated and should be replaced with <code>Agent.WriteDiscreteActionMask()</code></li> <li><code>Agent.Heuristic(float[])</code> was deprecated and should be replaced with <code>Agent.Heuristic(ActionBuffers)</code>.</li> <li><code>Agent.OnActionReceived(float[])</code> was deprecated and should be replaced with <code>Agent.OnActionReceived(ActionBuffers)</code>.</li> <li><code>Agent.GetAction()</code> was deprecated and should be replaced with <code>Agent.GetStoredActionBuffers()</code>.</li> </ul> <p>The default implementation of these will continue to call the deprecated versions where appropriate. However, the deprecated versions may not be compatible with continuous and discrete actions on the same Agent.</p>"},{"location":"Migrating/#brainparameters-field-and-method-deprecation","title":"BrainParameters field and method deprecation","text":"<ul> <li><code>BrainParameters.VectorActionSize</code> was deprecated; you can now set <code>BrainParameters.ActionSpec.NumContinuousActions</code>  or <code>BrainParameters.ActionSpec.BranchSizes</code> instead.</li> <li><code>BrainParameters.VectorActionSpaceType</code> was deprecated, since both continuous and discrete actions can now be used.</li> <li><code>BrainParameters.NumActions()</code> was deprecated. Use  <code>BrainParameters.ActionSpec.NumContinuousActions</code> and  <code>BrainParameters.ActionSpec.NumDiscreteActions</code> instead.</li> </ul>"},{"location":"Migrating/#migrating-from-release-7-to-latest","title":"Migrating from Release 7 to latest","text":""},{"location":"Migrating/#important-changes","title":"Important changes","text":"<ul> <li>Some trainer files were moved. If you were using the <code>TrainerFactory</code> class, it was moved to the <code>trainers/trainer</code> folder.</li> <li>The <code>components</code> folder containing <code>bc</code> and <code>reward_signals</code> code was moved to the <code>trainers/tf</code> folder</li> </ul>"},{"location":"Migrating/#steps-to-migrate","title":"Steps to Migrate","text":"<ul> <li>Replace calls to <code>from mlagents.trainers.trainer_util import TrainerFactory</code> to <code>from mlagents.trainers.trainer import TrainerFactory</code></li> <li>Replace calls to <code>from mlagents.trainers.trainer_util import handle_existing_directories</code> to <code>from mlagents.trainers.directory_utils import validate_existing_directories</code></li> <li>Replace <code>mlagents.trainers.components</code> with <code>mlagents.trainers.tf.components</code> in your import statements.</li> </ul>"},{"location":"Migrating/#migrating-from-release-3-to-release-7","title":"Migrating from Release 3 to Release 7","text":""},{"location":"Migrating/#important-changes_1","title":"Important changes","text":"<ul> <li>The Parameter Randomization feature has been merged with the Curriculum feature. It is now possible to specify a sampler in the lesson of a Curriculum. Curriculum has been refactored and is now specified at the level of the parameter, not the behavior. More information here.(#4160)</li> </ul>"},{"location":"Migrating/#steps-to-migrate_1","title":"Steps to Migrate","text":"<ul> <li>The configuration format for curriculum and parameter randomization has changed. To upgrade your configuration files, an upgrade script has been provided. Run <code>python -m mlagents.trainers.upgrade_config -h</code> to see the script usage. Note that you will have had to upgrade to/install the current version of ML-Agents before running the script. To update manually:</li> <li>If your config file used a <code>parameter_randomization</code> section, rename that section to <code>environment_parameters</code></li> <li>If your config file used a <code>curriculum</code> section, you will need to rewrite your curriculum with this format.</li> </ul>"},{"location":"Migrating/#migrating-from-release-1-to-release-3","title":"Migrating from Release 1 to Release 3","text":""},{"location":"Migrating/#important-changes_2","title":"Important changes","text":"<ul> <li>Training artifacts (trained models, summaries) are now found under <code>results/</code>   instead of <code>summaries/</code> and <code>models/</code>.</li> <li>Trainer configuration, curriculum configuration, and parameter randomization   configuration have all been moved to a single YAML file. (#3791)</li> <li>Trainer configuration format has changed, and using a \"default\" behavior name has   been deprecated. (#3936)</li> <li><code>max_step</code> in the <code>TerminalStep</code> and <code>TerminalSteps</code> objects was renamed <code>interrupted</code>.</li> <li>On the UnityEnvironment API, <code>get_behavior_names()</code> and <code>get_behavior_specs()</code> methods were combined into the property <code>behavior_specs</code> that contains a mapping from behavior names to behavior spec.</li> <li><code>use_visual</code> and <code>allow_multiple_visual_obs</code> in the <code>UnityToGymWrapper</code> constructor were replaced by <code>allow_multiple_obs</code> which allows one or more visual observations and vector observations to be used simultaneously.</li> <li><code>--save-freq</code> has been removed from the CLI and is now configurable in the trainer configuration   file.</li> <li><code>--lesson</code> has been removed from the CLI. Lessons will resume when using <code>--resume</code>.   To start at a different lesson, modify your Curriculum configuration.</li> </ul>"},{"location":"Migrating/#steps-to-migrate_2","title":"Steps to Migrate","text":"<ul> <li>To upgrade your configuration files, an upgrade script has been provided. Run   <code>python -m mlagents.trainers.upgrade_config -h</code> to see the script usage. Note that you will have   had to upgrade to/install the current version of ML-Agents before running the script.</li> </ul> <p>To do it manually, copy your <code>&lt;BehaviorName&gt;</code> sections from <code>trainer_config.yaml</code> into a separate trainer configuration file, under a <code>behaviors</code> section.   The <code>default</code> section is no longer needed. This new file should be specific to your environment, and not contain   configurations for multiple environments (unless they have the same Behavior Names).   - You will need to reformat your trainer settings as per the example.   - If your training uses curriculum, move those configurations under a <code>curriculum</code> section.   - If your training uses parameter randomization, move   the contents of the sampler config to <code>parameter_randomization</code> in the main trainer configuration. - If you are using <code>UnityEnvironment</code> directly, replace <code>max_step</code> with <code>interrupted</code>  in the <code>TerminalStep</code> and <code>TerminalSteps</code> objects.  - Replace usage of <code>get_behavior_names()</code> and <code>get_behavior_specs()</code> in UnityEnvironment with <code>behavior_specs</code>.  - If you use the <code>UnityToGymWrapper</code>, remove <code>use_visual</code> and <code>allow_multiple_visual_obs</code>  from the constructor and add <code>allow_multiple_obs = True</code> if the environment contains either  both visual and vector observations or multiple visual observations.  - If you were setting <code>--save-freq</code> in the CLI, add a <code>checkpoint_interval</code> value in your   trainer configuration, and set it equal to <code>save-freq * n_agents_in_scene</code>.</p>"},{"location":"Migrating/#migrating-from-015-to-release-1","title":"Migrating from 0.15 to Release 1","text":""},{"location":"Migrating/#important-changes_3","title":"Important changes","text":"<ul> <li>The <code>MLAgents</code> C# namespace was renamed to <code>Unity.MLAgents</code>, and other nested   namespaces were similarly renamed (#3843).</li> <li>The <code>--load</code> and <code>--train</code> command-line flags have been deprecated and   replaced with <code>--resume</code> and <code>--inference</code>.</li> <li>Running with the same <code>--run-id</code> twice will now throw an error.</li> <li>The <code>play_against_current_self_ratio</code> self-play trainer hyperparameter has   been renamed to <code>play_against_latest_model_ratio</code></li> <li>Removed the multi-agent gym option from the gym wrapper. For multi-agent   scenarios, use the Low Level Python API.</li> <li>The low level Python API has changed. You can look at the document   Low Level Python API documentation for more information. If   you use <code>mlagents-learn</code> for training, this should be a transparent change.</li> <li>The obsolete <code>Agent</code> methods <code>GiveModel</code>, <code>Done</code>, <code>InitializeAgent</code>,   <code>AgentAction</code> and <code>AgentReset</code> have been removed.</li> <li>The signature of <code>Agent.Heuristic()</code> was changed to take a <code>float[]</code> as a   parameter, instead of returning the array. This was done to prevent a common   source of error where users would return arrays of the wrong size.</li> <li>The SideChannel API has changed (#3833, #3660) :</li> <li>Introduced the <code>SideChannelManager</code> to register, unregister and access side     channels.</li> <li><code>EnvironmentParameters</code> replaces the default <code>FloatProperties</code>. You can     access the <code>EnvironmentParameters</code> with     <code>Academy.Instance.EnvironmentParameters</code> on C#. If you were previously     creating a <code>UnityEnvironment</code> in python and passing it a     <code>FloatPropertiesChannel</code>, create an <code>EnvironmentParametersChannel</code> instead.</li> <li><code>SideChannel.OnMessageReceived</code> is now a protected method (was public)</li> <li>SideChannel IncomingMessages methods now take an optional default argument,     which is used when trying to read more data than the message contains.</li> <li>Added a feature to allow sending stats from C# environments to TensorBoard     (and other python StatsWriters). To do this from your code, use     <code>Academy.Instance.StatsRecorder.Add(key, value)</code>(#3660)</li> <li><code>num_updates</code> and <code>train_interval</code> for SAC have been replaced with   <code>steps_per_update</code>.</li> <li>The <code>UnityEnv</code> class from the <code>gym-unity</code> package was renamed   <code>UnityToGymWrapper</code> and no longer creates the <code>UnityEnvironment</code>. Instead, the   <code>UnityEnvironment</code> must be passed as input to the constructor of   <code>UnityToGymWrapper</code></li> <li>Public fields and properties on several classes were renamed to follow Unity's   C# style conventions. All public fields and properties now use \"PascalCase\"   instead of \"camelCase\"; for example, <code>Agent.maxStep</code> was renamed to   <code>Agent.MaxStep</code>. For a full list of changes, see the pull request. (#3828)</li> <li><code>WriteAdapter</code> was renamed to <code>ObservationWriter</code>. (#3834)</li> </ul>"},{"location":"Migrating/#steps-to-migrate_3","title":"Steps to Migrate","text":"<ul> <li>In C# code, replace <code>using MLAgents</code> with <code>using Unity.MLAgents</code>. Replace   other nested namespaces such as <code>using MLAgents.Sensors</code> with   <code>using Unity.MLAgents.Sensors</code></li> <li>Replace the <code>--load</code> flag with <code>--resume</code> when calling <code>mlagents-learn</code>, and   don't use the <code>--train</code> flag as training will happen by default. To run   without training, use <code>--inference</code>.</li> <li>To force-overwrite files from a pre-existing run, add the <code>--force</code>   command-line flag.</li> <li>The Jupyter notebooks have been removed from the repository.</li> <li>If your Agent class overrides <code>Heuristic()</code>, change the signature to   <code>public override void Heuristic(float[] actionsOut)</code> and assign values to   <code>actionsOut</code> instead of returning an array.</li> <li>If you used <code>SideChannels</code> you must:</li> <li>Replace <code>Academy.FloatProperties</code> with     <code>Academy.Instance.EnvironmentParameters</code>.</li> <li><code>Academy.RegisterSideChannel</code> and <code>Academy.UnregisterSideChannel</code> were     removed. Use <code>SideChannelManager.RegisterSideChannel</code> and     <code>SideChannelManager.UnregisterSideChannel</code> instead.</li> <li>Set <code>steps_per_update</code> to be around equal to the number of agents in your   environment, times <code>num_updates</code> and divided by <code>train_interval</code>.</li> <li>Replace <code>UnityEnv</code> with <code>UnityToGymWrapper</code> in your code. The constructor no   longer takes a file name as input but a fully constructed <code>UnityEnvironment</code>   instead.</li> <li>Update uses of \"camelCase\" fields and properties to \"PascalCase\".</li> </ul>"},{"location":"Migrating/#migrating-from-014-to-015","title":"Migrating from 0.14 to 0.15","text":""},{"location":"Migrating/#important-changes_4","title":"Important changes","text":"<ul> <li>The <code>Agent.CollectObservations()</code> virtual method now takes as input a   <code>VectorSensor</code> sensor as argument. The <code>Agent.AddVectorObs()</code> methods were   removed.</li> <li>The <code>SetMask</code> was renamed to <code>SetMask</code> method must now be called on the   <code>DiscreteActionMasker</code> argument of the <code>CollectDiscreteActionMasks</code> virtual   method.</li> <li>We consolidated our API for <code>DiscreteActionMasker</code>. <code>SetMask</code> takes two   arguments : the branch index and the list of masked actions for that branch.</li> <li>The <code>Monitor</code> class has been moved to the Examples Project. (It was prone to   errors during testing)</li> <li>The <code>MLAgents.Sensors</code> namespace has been introduced. All sensors classes are   part of the <code>MLAgents.Sensors</code> namespace.</li> <li>The <code>MLAgents.SideChannels</code> namespace has been introduced. All side channel   classes are part of the <code>MLAgents.SideChannels</code> namespace.</li> <li>The interface for <code>RayPerceptionSensor.PerceiveStatic()</code> was changed to take   an input class and write to an output class, and the method was renamed to   <code>Perceive()</code>.</li> <li>The <code>SetMask</code> method must now be called on the <code>DiscreteActionMasker</code> argument   of the <code>CollectDiscreteActionMasks</code> method.</li> <li>The method <code>GetStepCount()</code> on the Agent class has been replaced with the   property getter <code>StepCount</code></li> <li>The <code>--multi-gpu</code> option has been removed temporarily.</li> <li><code>AgentInfo.actionMasks</code> has been renamed to <code>AgentInfo.discreteActionMasks</code>.</li> <li><code>BrainParameters</code> and <code>SpaceType</code> have been removed from the public API</li> <li><code>BehaviorParameters</code> have been removed from the public API.</li> <li><code>DecisionRequester</code> has been made internal (you can still use the   DecisionRequesterComponent from the inspector). <code>RepeatAction</code> was renamed   <code>TakeActionsBetweenDecisions</code> for clarity.</li> <li>The following methods in the <code>Agent</code> class have been renamed. The original   method names will be removed in a later release:</li> <li><code>InitializeAgent()</code> was renamed to <code>Initialize()</code></li> <li><code>AgentAction()</code> was renamed to <code>OnActionReceived()</code></li> <li><code>AgentReset()</code> was renamed to <code>OnEpsiodeBegin()</code></li> <li><code>Done()</code> was renamed to <code>EndEpisode()</code></li> <li><code>GiveModel()</code> was renamed to <code>SetModel()</code></li> <li>The <code>IFloatProperties</code> interface has been removed.</li> <li>The interface for SideChannels was changed:</li> <li>In C#, <code>OnMessageReceived</code> now takes a <code>IncomingMessage</code> argument, and     <code>QueueMessageToSend</code> takes an <code>OutgoingMessage</code> argument.</li> <li>In python, <code>on_message_received</code> now takes a <code>IncomingMessage</code> argument, and     <code>queue_message_to_send</code> takes an <code>OutgoingMessage</code> argument.</li> <li>Automatic stepping for Academy is now controlled from the     AutomaticSteppingEnabled property.</li> </ul>"},{"location":"Migrating/#steps-to-migrate_4","title":"Steps to Migrate","text":"<ul> <li>Add the <code>using MLAgents.Sensors;</code> in addition to <code>using MLAgents;</code> on top of   your Agent's script.</li> <li>Replace your Agent's implementation of <code>CollectObservations()</code> with   <code>CollectObservations(VectorSensor sensor)</code>. In addition, replace all calls to   <code>AddVectorObs()</code> with <code>sensor.AddObservation()</code> or   <code>sensor.AddOneHotObservation()</code> on the <code>VectorSensor</code> passed as argument.</li> <li>Replace your calls to <code>SetActionMask</code> on your Agent to   <code>DiscreteActionMasker.SetActionMask</code> in <code>CollectDiscreteActionMasks</code>.</li> <li>If you call <code>RayPerceptionSensor.PerceiveStatic()</code> manually, add your inputs   to a <code>RayPerceptionInput</code>. To get the previous float array output, iterate   through <code>RayPerceptionOutput.rayOutputs</code> and call   <code>RayPerceptionOutput.RayOutput.ToFloatArray()</code>.</li> <li>Replace all calls to <code>Agent.GetStepCount()</code> with <code>Agent.StepCount</code></li> <li>We strongly recommend replacing the following methods with their new   equivalent as they will be removed in a later release:</li> <li><code>InitializeAgent()</code> to <code>Initialize()</code></li> <li><code>AgentAction()</code> to <code>OnActionReceived()</code></li> <li><code>AgentReset()</code> to <code>OnEpisodeBegin()</code></li> <li><code>Done()</code> to <code>EndEpisode()</code></li> <li><code>GiveModel()</code> to <code>SetModel()</code></li> <li>Replace <code>IFloatProperties</code> variables with <code>FloatPropertiesChannel</code> variables.</li> <li>If you implemented custom <code>SideChannels</code>, update the signatures of your   methods, and add your data to the <code>OutgoingMessage</code> or read it from the   <code>IncomingMessage</code>.</li> <li>Replace calls to Academy.EnableAutomaticStepping()/DisableAutomaticStepping()   with Academy.AutomaticSteppingEnabled = true/false.</li> </ul>"},{"location":"Migrating/#migrating-from-013-to-014","title":"Migrating from 0.13 to 0.14","text":""},{"location":"Migrating/#important-changes_5","title":"Important changes","text":"<ul> <li>The <code>UnitySDK</code> folder has been split into a Unity Package   (<code>com.unity.ml-agents</code>) and an examples project (<code>Project</code>). Please follow the   Installation Guide to get up and running with this new repo   structure.</li> <li>Several changes were made to how agents are reset and marked as done:</li> <li>Calling <code>Done()</code> on the Agent will now reset it immediately and call the     <code>AgentReset</code> virtual method. (This is to simplify the previous logic in     which the Agent had to wait for the next <code>EnvironmentStep</code> to reset)</li> <li>The \"Reset on Done\" setting in AgentParameters was removed; this is now     effectively always true. <code>AgentOnDone</code> virtual method on the Agent has been     removed.</li> <li>The <code>Decision Period</code> and <code>On Demand decision</code> checkbox have been removed from   the Agent. On demand decision is now the default (calling <code>RequestDecision</code> on   the Agent manually.)</li> <li>The Academy class was changed to a singleton, and its virtual methods were   removed.</li> <li>Trainer steps are now counted per-Agent, not per-environment as in previous   versions. For instance, if you have 10 Agents in the scene, 20 environment   steps now corresponds to 200 steps as printed in the terminal and in   Tensorboard.</li> <li>Curriculum config files are now YAML formatted and all curricula for a   training run are combined into a single file.</li> <li>The <code>--num-runs</code> command-line option has been removed from <code>mlagents-learn</code>.</li> <li>Several fields on the Agent were removed or made private in order to simplify   the interface.</li> <li>The <code>agentParameters</code> field of the Agent has been removed. (Contained only     <code>maxStep</code> information)</li> <li><code>maxStep</code> is now a public field on the Agent. (Was moved from     <code>agentParameters</code>)</li> <li>The <code>Info</code> field of the Agent has been made private. (Was only used     internally and not meant to be modified outside of the Agent)</li> <li>The <code>GetReward()</code> method on the Agent has been removed. (It was being     confused with <code>GetCumulativeReward()</code>)</li> <li>The <code>AgentAction</code> struct no longer contains a <code>value</code> field. (Value     estimates were not set during inference)</li> <li>The <code>GetValueEstimate()</code> method on the Agent has been removed.</li> <li>The <code>UpdateValueAction()</code> method on the Agent has been removed.</li> <li>The deprecated <code>RayPerception3D</code> and <code>RayPerception2D</code> classes were removed,   and the <code>legacyHitFractionBehavior</code> argument was removed from   <code>RayPerceptionSensor.PerceiveStatic()</code>.</li> <li>RayPerceptionSensor was inconsistent in how it handle scale on the Agent's   transform. It now scales the ray length and sphere size for casting as the   transform's scale changes.</li> </ul>"},{"location":"Migrating/#steps-to-migrate_5","title":"Steps to Migrate","text":"<ul> <li>Follow the instructions on how to install the <code>com.unity.ml-agents</code> package   into your project in the Installation Guide.</li> <li>If your Agent implemented <code>AgentOnDone</code> and did not have the checkbox   <code>Reset On Done</code> checked in the inspector, you must call the code that was in   <code>AgentOnDone</code> manually.</li> <li>If you give your Agent a reward or penalty at the end of an episode (e.g. for   reaching a goal or falling off of a platform), make sure you call   <code>AddReward()</code> or <code>SetReward()</code> before calling <code>Done()</code>. Previously, the   order didn't matter.</li> <li>If you were not using <code>On Demand Decision</code> for your Agent, you must add a   <code>DecisionRequester</code> component to your Agent GameObject and set its   <code>Decision Period</code> field to the old <code>Decision Period</code> of the Agent.</li> <li>If you have a class that inherits from Academy:</li> <li>If the class didn't override any of the virtual methods and didn't store any     additional data, you can just remove the old script from the scene.</li> <li>If the class had additional data, create a new MonoBehaviour and store the     data in the new MonoBehaviour instead.</li> <li>If the class overrode the virtual methods, create a new MonoBehaviour and     move the logic to it:<ul> <li>Move the InitializeAcademy code to MonoBehaviour.Awake</li> <li>Move the AcademyStep code to MonoBehaviour.FixedUpdate</li> <li>Move the OnDestroy code to MonoBehaviour.OnDestroy.</li> <li>Move the AcademyReset code to a new method and add it to the   Academy.OnEnvironmentReset action.</li> </ul> </li> <li>Multiply <code>max_steps</code> and <code>summary_freq</code> in your <code>trainer_config.yaml</code> by the   number of Agents in the scene.</li> <li>Combine curriculum configs into a single file. See   the WallJump curricula for an example of   the new curriculum config format. A tool like https://www.json2yaml.com may be   useful to help with the conversion.</li> <li>If you have a model trained which uses RayPerceptionSensor and has non-1.0   scale in the Agent's transform, it must be retrained.</li> </ul>"},{"location":"Migrating/#migrating-from-ml-agents-toolkit-v0120-to-v0130","title":"Migrating from ML-Agents Toolkit v0.12.0 to v0.13.0","text":""},{"location":"Migrating/#important-changes_6","title":"Important changes","text":"<ul> <li>The low level Python API has changed. You can look at the document   Low Level Python API documentation for more information. This   should only affect you if you're writing a custom trainer; if you use   <code>mlagents-learn</code> for training, this should be a transparent change.</li> <li><code>reset()</code> on the Low-Level Python API no longer takes a <code>train_mode</code>     argument. To modify the performance/speed of the engine, you must use an     <code>EngineConfigurationChannel</code></li> <li><code>reset()</code> on the Low-Level Python API no longer takes a <code>config</code> argument.     <code>UnityEnvironment</code> no longer has a <code>reset_parameters</code> field. To modify float     properties in the environment, you must use a <code>FloatPropertiesChannel</code>. For     more information, refer to the     Low Level Python API documentation</li> <li><code>CustomResetParameters</code> are now removed.</li> <li>The Academy no longer has a <code>Training Configuration</code> nor   <code>Inference Configuration</code> field in the inspector. To modify the configuration   from the Low-Level Python API, use an <code>EngineConfigurationChannel</code>. To modify   it during training, use the new command line arguments <code>--width</code>, <code>--height</code>,   <code>--quality-level</code>, <code>--time-scale</code> and <code>--target-frame-rate</code> in   <code>mlagents-learn</code>.</li> <li>The Academy no longer has a <code>Default Reset Parameters</code> field in the inspector.   The Academy class no longer has a <code>ResetParameters</code>. To access shared float   properties with Python, use the new <code>FloatProperties</code> field on the Academy.</li> <li>Offline Behavioral Cloning has been removed. To learn from demonstrations, use   the GAIL and Behavioral Cloning features with either PPO or SAC.</li> <li><code>mlagents.envs</code> was renamed to <code>mlagents_envs</code>. The previous repo layout   depended on PEP420, which caused   problems with some of our tooling such as mypy and pylint.</li> <li>The official version of Unity ML-Agents supports is now 2022.3 LTS. If you run   into issues, please consider deleting your library folder and reponening your   projects. You will need to install the Sentis package into your project in   order to ML-Agents to compile correctly.</li> </ul>"},{"location":"Migrating/#steps-to-migrate_6","title":"Steps to Migrate","text":"<ul> <li>If you had a custom <code>Training Configuration</code> in the Academy inspector, you   will need to pass your custom configuration at every training run using the   new command line arguments <code>--width</code>, <code>--height</code>, <code>--quality-level</code>,   <code>--time-scale</code> and <code>--target-frame-rate</code>.</li> <li>If you were using <code>--slow</code> in <code>mlagents-learn</code>, you will need to pass your old   <code>Inference Configuration</code> of the Academy inspector with the new command line   arguments <code>--width</code>, <code>--height</code>, <code>--quality-level</code>, <code>--time-scale</code> and   <code>--target-frame-rate</code> instead.</li> <li>Any imports from <code>mlagents.envs</code> should be replaced with <code>mlagents_envs</code>.</li> </ul>"},{"location":"Migrating/#migrating-from-ml-agents-toolkit-v0110-to-v0120","title":"Migrating from ML-Agents Toolkit v0.11.0 to v0.12.0","text":""},{"location":"Migrating/#important-changes_7","title":"Important Changes","text":"<ul> <li>Text actions and observations, and custom action and observation protos have   been removed.</li> <li>RayPerception3D and RayPerception2D are marked deprecated, and will be removed   in a future release. They can be replaced by RayPerceptionSensorComponent3D   and RayPerceptionSensorComponent2D.</li> <li>The <code>Use Heuristic</code> checkbox in Behavior Parameters has been replaced with a   <code>Behavior Type</code> dropdown menu. This has the following options:</li> <li><code>Default</code> corresponds to the previous unchecked behavior, meaning that     Agents will train if they connect to a python trainer, otherwise they will     perform inference.</li> <li><code>Heuristic Only</code> means the Agent will always use the <code>Heuristic()</code> method.     This corresponds to having \"Use Heuristic\" selected in 0.11.0.</li> <li><code>Inference Only</code> means the Agent will always perform inference.</li> <li>ML-Agents was upgraded to use Sentis 1.2.0-exp.2 and is installed via the package manager.</li> </ul>"},{"location":"Migrating/#steps-to-migrate_7","title":"Steps to Migrate","text":"<ul> <li>We fixed a bug in   <code>RayPerception3d.Perceive()</code> that was causing the <code>endOffset</code> to be used   incorrectly. However this may produce different behavior from previous   versions if you use a non-zero <code>startOffset</code>. To reproduce the old behavior,   you should increase the value of <code>endOffset</code> by <code>startOffset</code>. You can   verify your raycasts are performing as expected in scene view using the debug   rays.</li> <li>If you use RayPerception3D, replace it with RayPerceptionSensorComponent3D   (and similarly for 2D). The settings, such as ray angles and detectable tags,   are configured on the component now. RayPerception3D would contribute   <code>(# of rays) * (# of tags + 2)</code> to the State Size in Behavior Parameters, but   this is no longer necessary, so you should reduce the State Size by this   amount. Making this change will require retraining your model, since the   observations that RayPerceptionSensorComponent3D produces are different from   the old behavior.</li> <li>If you see messages such as   <code>The type or namespace 'Sentis' could not be found</code> or   <code>The type or namespace 'Google' could not be found</code>, you will need to   install the Sentis preview package.</li> </ul>"},{"location":"Migrating/#migrating-from-ml-agents-toolkit-v010-to-v0110","title":"Migrating from ML-Agents Toolkit v0.10 to v0.11.0","text":""},{"location":"Migrating/#important-changes_8","title":"Important Changes","text":"<ul> <li>The definition of the gRPC service has changed.</li> <li>The online BC training feature has been removed.</li> <li>The BroadcastHub has been deprecated. If there is a training Python process,   all LearningBrains in the scene will automatically be trained. If there is no   Python process, inference will be used.</li> <li>The Brain ScriptableObjects have been deprecated. The Brain Parameters are now   on the Agent and are referred to as Behavior Parameters. Make sure the   Behavior Parameters is attached to the Agent GameObject.</li> <li>To use a heuristic behavior, implement the <code>Heuristic()</code> method in the Agent   class and check the <code>use heuristic</code> checkbox in the Behavior Parameters.</li> <li>Several changes were made to the setup for visual observations (i.e. using   Cameras or RenderTextures):</li> <li>Camera resolutions are no longer stored in the Brain Parameters.</li> <li>AgentParameters no longer stores lists of Cameras and RenderTextures</li> <li>To add visual observations to an Agent, you must now attach a     CameraSensorComponent or RenderTextureComponent to the agent. The     corresponding Camera or RenderTexture can be added to these in the editor,     and the resolution and color/grayscale is configured on the component     itself.</li> </ul>"},{"location":"Migrating/#steps-to-migrate_8","title":"Steps to Migrate","text":"<ul> <li>In order to be able to train, make sure both your ML-Agents Python package and   UnitySDK code come from the v0.11 release. Training will not work, for   example, if you update the ML-Agents Python package, and only update the API   Version in UnitySDK.</li> <li>If your Agents used visual observations, you must add a CameraSensorComponent   corresponding to each old Camera in the Agent's camera list (and similarly for   RenderTextures).</li> <li>Since Brain ScriptableObjects have been removed, you will need to delete all   the Brain ScriptableObjects from your <code>Assets</code> folder. Then, add a   <code>Behavior Parameters</code> component to each <code>Agent</code> GameObject. You will then need   to complete the fields on the new <code>Behavior Parameters</code> component with the   BrainParameters of the old Brain.</li> </ul>"},{"location":"Migrating/#migrating-from-ml-agents-toolkit-v09-to-v010","title":"Migrating from ML-Agents Toolkit v0.9 to v0.10","text":""},{"location":"Migrating/#important-changes_9","title":"Important Changes","text":"<ul> <li>We have updated the C# code in our repository to be in line with Unity Coding   Conventions. This has changed the name of some public facing classes and   enums.</li> <li>The example environments have been updated. If you were using these   environments to benchmark your training, please note that the resulting   rewards may be slightly different in v0.10.</li> </ul>"},{"location":"Migrating/#steps-to-migrate_9","title":"Steps to Migrate","text":"<ul> <li><code>UnitySDK/Assets/ML-Agents/Scripts/Communicator.cs</code> and its class   <code>Communicator</code> have been renamed to   <code>UnitySDK/Assets/ML-Agents/Scripts/ICommunicator.cs</code> and <code>ICommunicator</code>   respectively.</li> <li>The <code>SpaceType</code> Enums <code>discrete</code>, and <code>continuous</code> have been renamed to   <code>Discrete</code> and <code>Continuous</code>.</li> <li>We have removed the <code>Done</code> call as well as the capacity to set <code>Max Steps</code> on   the Academy. Therefore an AcademyReset will never be triggered from C# (only   from Python). If you want to reset the simulation after a fixed number of   steps, or when an event in the simulation occurs, we recommend looking at our   multi-agent example environments (such as FoodCollector). In our examples,   groups of Agents can be reset through an \"Area\" that can reset groups of   Agents.</li> <li>The import for <code>mlagents.envs.UnityEnvironment</code> was removed. If you are using   the Python API, change <code>from mlagents_envs import UnityEnvironment</code> to   <code>from mlagents_envs.environment import UnityEnvironment</code>.</li> </ul>"},{"location":"Migrating/#migrating-from-ml-agents-toolkit-v08-to-v09","title":"Migrating from ML-Agents Toolkit v0.8 to v0.9","text":""},{"location":"Migrating/#important-changes_10","title":"Important Changes","text":"<ul> <li>We have changed the way reward signals (including Curiosity) are defined in   the <code>trainer_config.yaml</code>.</li> <li>When using multiple environments, every \"step\" is recorded in TensorBoard.</li> <li>The steps in the command line console corresponds to a single step of a single   environment. Previously, each step corresponded to one step for all   environments (i.e., <code>num_envs</code> steps).</li> </ul>"},{"location":"Migrating/#steps-to-migrate_10","title":"Steps to Migrate","text":"<ul> <li>If you were overriding any of these following parameters in your config file,   remove them from the top-level config and follow the steps below:</li> <li><code>gamma</code>: Define a new <code>extrinsic</code> reward signal and set it's <code>gamma</code> to your     new gamma.</li> <li><code>use_curiosity</code>, <code>curiosity_strength</code>, <code>curiosity_enc_size</code>: Define a     <code>curiosity</code> reward signal and set its <code>strength</code> to <code>curiosity_strength</code>,     and <code>encoding_size</code> to <code>curiosity_enc_size</code>. Give it the same <code>gamma</code> as     your <code>extrinsic</code> signal to mimic previous behavior.</li> <li>TensorBoards generated when running multiple environments in v0.8 are not   comparable to those generated in v0.9 in terms of step count. Multiply your   v0.8 step count by <code>num_envs</code> for an approximate comparison. You may need to   change <code>max_steps</code> in your config as appropriate as well.</li> </ul>"},{"location":"Migrating/#migrating-from-ml-agents-toolkit-v07-to-v08","title":"Migrating from ML-Agents Toolkit v0.7 to v0.8","text":""},{"location":"Migrating/#important-changes_11","title":"Important Changes","text":"<ul> <li>We have split the Python packages into two separate packages <code>ml-agents</code> and   <code>ml-agents-envs</code>.</li> <li><code>--worker-id</code> option of <code>learn.py</code> has been removed, use <code>--base-port</code> instead   if you'd like to run multiple instances of <code>learn.py</code>.</li> </ul>"},{"location":"Migrating/#steps-to-migrate_11","title":"Steps to Migrate","text":"<ul> <li>If you are installing via PyPI, there is no change.</li> <li>If you intend to make modifications to <code>ml-agents</code> or <code>ml-agents-envs</code> please   check the Installing for Development in the   Installation documentation.</li> </ul>"},{"location":"Migrating/#migrating-from-ml-agents-toolkit-v06-to-v07","title":"Migrating from ML-Agents Toolkit v0.6 to v0.7","text":""},{"location":"Migrating/#important-changes_12","title":"Important Changes","text":"<ul> <li>We no longer support TFS and are now using the   Sentis</li> </ul>"},{"location":"Migrating/#steps-to-migrate_12","title":"Steps to Migrate","text":"<ul> <li>Make sure to remove the <code>ENABLE_TENSORFLOW</code> flag in your Unity Project   settings</li> </ul>"},{"location":"Migrating/#migrating-from-ml-agents-toolkit-v05-to-v06","title":"Migrating from ML-Agents Toolkit v0.5 to v0.6","text":""},{"location":"Migrating/#important-changes_13","title":"Important Changes","text":"<ul> <li>Brains are now Scriptable Objects instead of MonoBehaviors.</li> <li> <p>You can no longer modify the type of a Brain. If you want to switch between   <code>PlayerBrain</code> and <code>LearningBrain</code> for multiple agents, you will need to assign   a new Brain to each agent separately. Note: You can pass the same Brain to   multiple agents in a scene by leveraging Unity's prefab system or look for all   the agents in a scene using the search bar of the <code>Hierarchy</code> window with the   word <code>Agent</code>.</p> </li> <li> <p>We replaced the Internal and External Brain with Learning Brain.   When you need to train a model, you need to drag it into the <code>Broadcast Hub</code>   inside the <code>Academy</code> and check the <code>Control</code> checkbox.</p> </li> <li>We removed the <code>Broadcast</code> checkbox of the Brain, to use the broadcast   functionality, you need to drag the Brain into the <code>Broadcast Hub</code>.</li> <li>When training multiple Brains at the same time, each model is now stored into   a separate model file rather than in the same file under different graph   scopes.</li> <li>The Learning Brain graph scope, placeholder names, output names and custom   placeholders can no longer be modified.</li> </ul>"},{"location":"Migrating/#steps-to-migrate_13","title":"Steps to Migrate","text":"<ul> <li>To update a scene from v0.5 to v0.6, you must:</li> <li>Remove the <code>Brain</code> GameObjects in the scene. (Delete all of the Brain     GameObjects under Academy in the scene.)</li> <li>Create new <code>Brain</code> Scriptable Objects using <code>Assets -&gt; Create -&gt; ML-Agents</code>     for each type of the Brain you plan to use, and put the created files under     a folder called Brains within your project.</li> <li>Edit their <code>Brain Parameters</code> to be the same as the parameters used in the     <code>Brain</code> GameObjects.</li> <li>Agents have a <code>Brain</code> field in the Inspector, you need to drag the     appropriate Brain ScriptableObject in it.</li> <li>The Academy has a <code>Broadcast Hub</code> field in the inspector, which is list of     brains used in the scene. To train or control your Brain from the     <code>mlagents-learn</code> Python script, you need to drag the relevant     <code>LearningBrain</code> ScriptableObjects used in your scene into entries into this     list.</li> </ul>"},{"location":"Migrating/#migrating-from-ml-agents-toolkit-v04-to-v05","title":"Migrating from ML-Agents Toolkit v0.4 to v0.5","text":""},{"location":"Migrating/#important","title":"Important","text":"<ul> <li>The Unity project <code>unity-environment</code> has been renamed <code>UnitySDK</code>.</li> <li>The <code>python</code> folder has been renamed to <code>ml-agents</code>. It now contains two   packages, <code>mlagents.env</code> and <code>mlagents.trainers</code>. <code>mlagents.env</code> can be used   to interact directly with a Unity environment, while <code>mlagents.trainers</code>   contains the classes for training agents.</li> <li>The supported Unity version has changed from <code>2017.1 or later</code> to   <code>2017.4 or later</code>. 2017.4 is an LTS (Long Term Support) version that helps us   maintain good quality and support. Earlier versions of Unity might still work,   but you may encounter an   error listed here.</li> </ul>"},{"location":"Migrating/#unity-api","title":"Unity API","text":"<ul> <li>Discrete Actions now use branches. You can   now specify concurrent discrete actions. You will need to update the Brain   Parameters in the Brain Inspector in all your environments that use discrete   actions. Refer to the   discrete action documentation   for more information.</li> </ul>"},{"location":"Migrating/#python-api","title":"Python API","text":"<ul> <li>In order to run a training session, you can now use the command   <code>mlagents-learn</code> instead of <code>python3 learn.py</code> after installing the <code>mlagents</code>   packages. This change is documented   here. For example, if we   previously ran</li> </ul> <p><code>sh   python3 learn.py 3DBall --train</code></p> <p>from the <code>python</code> subdirectory (which is changed to <code>ml-agents</code> subdirectory   in v0.5), we now run</p> <p><code>sh   mlagents-learn config/trainer_config.yaml --env=3DBall --train</code></p> <p>from the root directory where we installed the ML-Agents Toolkit.</p> <ul> <li>It is now required to specify the path to the yaml trainer configuration file   when running <code>mlagents-learn</code>. For an example trainer configuration file, see   trainer_config.yaml. An example of passing a   trainer configuration to <code>mlagents-learn</code> is shown above.</li> <li>The environment name is now passed through the <code>--env</code> option.</li> <li>Curriculum learning has been changed. In summary:</li> <li>Curriculum files for the same environment must now be placed into a folder.     Each curriculum file should be named after the Brain whose curriculum it     specifies.</li> <li><code>min_lesson_length</code> now specifies the minimum number of episodes in a lesson     and affects reward thresholding.</li> <li>It is no longer necessary to specify the <code>Max Steps</code> of the Academy to use     curriculum learning.</li> </ul>"},{"location":"Migrating/#migrating-from-ml-agents-toolkit-v03-to-v04","title":"Migrating from ML-Agents Toolkit v0.3 to v0.4","text":""},{"location":"Migrating/#unity-api_1","title":"Unity API","text":"<ul> <li><code>using MLAgents;</code> needs to be added in all of the C# scripts that use   ML-Agents.</li> </ul>"},{"location":"Migrating/#python-api_1","title":"Python API","text":"<ul> <li>We've changed some of the Python packages dependencies in requirement.txt   file. Make sure to run <code>pip3 install -e .</code> within your <code>ml-agents/python</code>   folder to update your Python packages.</li> </ul>"},{"location":"Migrating/#migrating-from-ml-agents-toolkit-v02-to-v03","title":"Migrating from ML-Agents Toolkit v0.2 to v0.3","text":"<p>There are a large number of new features and improvements in the ML-Agents toolkit v0.3 which change both the training process and Unity API in ways which will cause incompatibilities with environments made using older versions. This page is designed to highlight those changes for users familiar with v0.1 or v0.2 in order to ensure a smooth transition.</p>"},{"location":"Migrating/#important_1","title":"Important","text":"<ul> <li>The ML-Agents Toolkit is no longer compatible with Python 2.</li> </ul>"},{"location":"Migrating/#python-training","title":"Python Training","text":"<ul> <li>The training script <code>ppo.py</code> and <code>PPO.ipynb</code> Python notebook have been   replaced with a single <code>learn.py</code> script as the launching point for training   with ML-Agents. For more information on using <code>learn.py</code>, see   here.</li> <li>Hyperparameters for training Brains are now stored in the   <code>trainer_config.yaml</code> file. For more information on using this file, see   here.</li> </ul>"},{"location":"Migrating/#unity-api_2","title":"Unity API","text":"<ul> <li>Modifications to an Agent's rewards must now be done using either   <code>AddReward()</code> or <code>SetReward()</code>.</li> <li>Setting an Agent to done now requires the use of the <code>Done()</code> method.</li> <li><code>CollectStates()</code> has been replaced by <code>CollectObservations()</code>, which now no   longer returns a list of floats.</li> <li>To collect observations, call <code>AddVectorObs()</code> within <code>CollectObservations()</code>.   Note that you can call <code>AddVectorObs()</code> with floats, integers, lists and   arrays of floats, Vector3 and Quaternions.</li> <li><code>AgentStep()</code> has been replaced by <code>AgentAction()</code>.</li> <li><code>WaitTime()</code> has been removed.</li> <li>The <code>Frame Skip</code> field of the Academy is replaced by the Agent's   <code>Decision Frequency</code> field, enabling the Agent to make decisions at different   frequencies.</li> <li>The names of the inputs in the Internal Brain have been changed. You must   replace <code>state</code> with <code>vector_observation</code> and <code>observation</code> with   <code>visual_observation</code>. In addition, you must remove the <code>epsilon</code> placeholder.</li> </ul>"},{"location":"Migrating/#semantics","title":"Semantics","text":"<p>In order to more closely align with the terminology used in the Reinforcement Learning field, and to be more descriptive, we have changed the names of some of the concepts used in ML-Agents. The changes are highlighted in the table below.</p> Old - v0.2 and earlier New - v0.3 and later State Vector Observation Observation Visual Observation Action Vector Action N/A Text Observation N/A Text Action"},{"location":"Package-Settings/","title":"ML-Agents Package Settings","text":"<p>ML-Agents Package Settings contains settings that apply to the whole project. It allows you to configure ML-Agents-specific settings in the Editor.  These settings are available for use in both the Editor and Player.</p> <p>You can find them at <code>Edit</code> &gt; <code>Project Settings...</code> &gt; <code>ML-Agents</code>. It lists out all the available settings and their default values.</p>"},{"location":"Package-Settings/#create-custom-settings","title":"Create Custom Settings","text":"<p>In order to to use your own settings for your project, you'll need to create a settings asset.</p> <p>You can do this by clicking the <code>Create Settings Asset</code> buttom or clicking the gear on the top right and select <code>New Settings Asset...</code>. The asset file can be placed anywhere in the <code>Asset/</code> folder in your project. After Creating the settings asset, you'll be able to modify the settings for your project and your settings will be saved in the asset.</p> <p></p>"},{"location":"Package-Settings/#multiple-custom-settings-for-different-scenarios","title":"Multiple Custom Settings for Different Scenarios","text":"<p>You can create multiple settings assets in one project.</p> <p>By clicking the gear on the top right you'll see all available settings listed in the drop-down menu to choose from.</p> <p>This allows you to create different settings for different scenatios. For example, you can create two separate settings for training and inference, and specify which one you want to use according to what you're currently running.</p> <p></p>"},{"location":"Profiling-Python/","title":"Profiling in Python","text":"<p>As part of the ML-Agents Tookit, we provide a lightweight profiling system, in order to identity hotspots in the training process and help spot regressions from changes.</p> <p>Timers are hierarchical, meaning that the time tracked in a block of code can be further split into other blocks if desired. This also means that a function that is called from multiple places in the code will appear in multiple places in the timing output.</p> <p>All timers operate using a \"global\" instance by default, but this can be overridden if necessary (mainly for testing).</p>"},{"location":"Profiling-Python/#adding-profiling","title":"Adding Profiling","text":"<p>There are two ways to indicate code should be included in profiling. The simplest way is to add the <code>@timed</code> decorator to a function or method of interested.</p> <pre><code>class TrainerController:\n    # ....\n    @timed\n    def advance(self, env: EnvManager) -&gt; int:\n        # do stuff\n</code></pre> <p>You can also used the <code>hierarchical_timer</code> context manager.</p> <pre><code>with hierarchical_timer(\"communicator.exchange\"):\n    outputs = self.communicator.exchange(step_input)\n</code></pre> <p>The context manager may be easier than the <code>@timed</code> decorator for profiling different parts of a large function, or profiling calls to abstract methods that might not use decorator.</p>"},{"location":"Profiling-Python/#output","title":"Output","text":"<p>By default, at the end of training, timers are collected and written in json format to <code>{summaries_dir}/{run_id}_timers.json</code>. The output consists of node objects with the following keys:</p> <ul> <li>total (float): The total time in seconds spent in the block, including child   calls.</li> <li>count (int): The number of times the block was called.</li> <li>self (float): The total time in seconds spent in the block, excluding child   calls.</li> <li>children (dictionary): A dictionary of child nodes, keyed by the node name.</li> <li>is_parallel (bool): Indicates that the block of code was executed in multiple   threads or processes (see below). This is optional and defaults to false.</li> </ul>"},{"location":"Profiling-Python/#parallel-execution","title":"Parallel execution","text":""},{"location":"Profiling-Python/#subprocesses","title":"Subprocesses","text":"<p>For code that executes in multiple processes (for example, SubprocessEnvManager), we periodically send the timer information back to the \"main\" process, aggregate the timers there, and flush them in the subprocess. Note that (depending on the number of processes) this can result in timers where the total time may exceed the parent's total time. This is analogous to the difference between \"real\" and \"user\" values reported from the unix <code>time</code> command. In the timer output, blocks that were run in parallel are indicated by the <code>is_parallel</code> flag.</p>"},{"location":"Profiling-Python/#threads","title":"Threads","text":"<p>Timers currently use <code>time.perf_counter()</code> to track time spent, which may not give accurate results for multiple threads. If this is problematic, set <code>threaded: false</code> in your trainer configuration.</p>"},{"location":"Python-Custom-Trainer-Plugin/","title":"Unity Ml-Agents Custom trainers Plugin","text":"<p>As an attempt to bring a wider variety of reinforcement learning algorithms to our users, we have added custom trainers capabilities. we introduce an extensible plugin system to define new trainers based on the High level trainer API in <code>Ml-agents</code> Package. This will allow rerouting <code>mlagents-learn</code> CLI to custom trainers and extending the config files with hyper-parameters specific to your new trainers. We will expose a high-level extensible trainer (both on-policy, and off-policy trainers) optimizer and hyperparameter classes with documentation for the use of this plugin. For more infromation on how python plugin system works see Plugin interfaces.</p>"},{"location":"Python-Custom-Trainer-Plugin/#overview","title":"Overview","text":"<p>Model-free RL algorithms generally fall into two broad categories: on-policy and off-policy. On-policy algorithms perform updates based on data gathered from the current policy. Off-policy algorithms learn a Q function from a buffer of previous data, then use this Q function to make decisions. Off-policy algorithms have three key benefits in the context of ML-Agents: They tend to use fewer samples than on-policy as they can pull and re-use data from the buffer many times. They allow player demonstrations to be inserted in-line with RL data into the buffer, enabling new ways of doing imitation learning by streaming player data.</p> <p>To add new custom trainers to ML-agents, you would need to create a new python package. To give you an idea of how to structure your package, we have created a mlagents_trainer_plugin package ourselves as an example, with implementation of <code>A2c</code> and <code>DQN</code> algorithms. You would need a <code>setup.py</code> file to list extra requirements and register the new RL algorithm in ml-agents ecosystem and be able to call <code>mlagents-learn</code> CLI with your customized configuration.</p> <pre><code>\u251c\u2500\u2500 mlagents_trainer_plugin\n\u2502    \u251c\u2500\u2500 __init__.py\n\u2502    \u251c\u2500\u2500 a2c\n\u2502    \u2502    \u251c\u2500\u2500 __init__.py\n\u2502    \u2502    \u251c\u2500\u2500 a2c_3DBall.yaml\n\u2502    \u2502    \u251c\u2500\u2500 a2c_optimizer.py\n\u2502    \u2502    \u2514\u2500\u2500 a2c_trainer.py\n\u2502    \u2514\u2500\u2500 dqn\n\u2502        \u251c\u2500\u2500 __init__.py\n\u2502        \u251c\u2500\u2500 dqn_basic.yaml\n\u2502        \u251c\u2500\u2500 dqn_optimizer.py\n\u2502        \u2514\u2500\u2500 dqn_trainer.py\n\u2514\u2500\u2500 setup.py\n</code></pre>"},{"location":"Python-Custom-Trainer-Plugin/#installation-and-execution","title":"Installation and Execution","text":"<p>If you haven't already, follow the installation instructions. Once you have the <code>ml-agents-env</code> and <code>ml-agents</code> packages you can install the plugin package. From the repository's root directory install <code>ml-agents-trainer-plugin</code> (or replace with the name of your plugin folder).</p> <pre><code>pip3 install -e &lt;./ml-agents-trainer-plugin&gt;\n</code></pre> <p>Following the previous installations your package is added as an entrypoint and you can use a config file with new trainers:</p> <pre><code>mlagents-learn ml-agents-trainer-plugin/mlagents_trainer_plugin/a2c/a2c_3DBall.yaml --run-id &lt;run-id-name&gt;\n--env &lt;env-executable&gt;\n</code></pre>"},{"location":"Python-Custom-Trainer-Plugin/#tutorial","title":"Tutorial","text":"<p>Here\u2019s a step-by-step tutorial on how to write a setup file and extend ml-agents trainers, optimizers, and hyperparameter settings.To extend ML-agents classes see references on trainers and Optimizer.</p>"},{"location":"Python-Gym-API-Documentation/","title":"Table of Contents","text":"<ul> <li>mlagents_envs.envs.unity_gym_env</li> <li>UnityGymException</li> <li>UnityToGymWrapper<ul> <li>__init__</li> <li>reset</li> <li>step</li> <li>render</li> <li>close</li> <li>seed</li> </ul> </li> <li>ActionFlattener<ul> <li>__init__</li> <li>lookup_action</li> </ul> </li> </ul>"},{"location":"Python-Gym-API-Documentation/#mlagents_envsenvsunity_gym_env","title":"mlagents_envs.envs.unity_gym_env","text":""},{"location":"Python-Gym-API-Documentation/#unitygymexception-objects","title":"UnityGymException Objects","text":"<pre><code>class UnityGymException(error.Error)\n</code></pre> <p>Any error related to the gym wrapper of ml-agents.</p> <p></p>"},{"location":"Python-Gym-API-Documentation/#unitytogymwrapper-objects","title":"UnityToGymWrapper Objects","text":"<pre><code>class UnityToGymWrapper(gym.Env)\n</code></pre> <p>Provides Gym wrapper for Unity Learning Environments.</p> <p></p>"},{"location":"Python-Gym-API-Documentation/#__init__","title":"__init__","text":"<pre><code> | __init__(unity_env: BaseEnv, uint8_visual: bool = False, flatten_branched: bool = False, allow_multiple_obs: bool = False, action_space_seed: Optional[int] = None)\n</code></pre> <p>Environment initialization</p> <p>Arguments:</p> <ul> <li><code>unity_env</code>: The Unity BaseEnv to be wrapped in the gym. Will be closed when the UnityToGymWrapper closes.</li> <li><code>uint8_visual</code>: Return visual observations as uint8 (0-255) matrices instead of float (0.0-1.0).</li> <li><code>flatten_branched</code>: If True, turn branched discrete action spaces into a Discrete space rather than     MultiDiscrete.</li> <li><code>allow_multiple_obs</code>: If True, return a list of np.ndarrays as observations with the first elements     containing the visual observations and the last element containing the array of vector observations.     If False, returns a single np.ndarray containing either only a single visual observation or the array of     vector observations.</li> <li><code>action_space_seed</code>: If non-None, will be used to set the random seed on created gym.Space instances.</li> </ul> <p></p>"},{"location":"Python-Gym-API-Documentation/#reset","title":"reset","text":"<pre><code> | reset() -&gt; Union[List[np.ndarray], np.ndarray]\n</code></pre> <p>Resets the state of the environment and returns an initial observation. Returns: observation (object/list): the initial observation of the space.</p> <p></p>"},{"location":"Python-Gym-API-Documentation/#step","title":"step","text":"<pre><code> | step(action: List[Any]) -&gt; GymStepResult\n</code></pre> <p>Run one timestep of the environment's dynamics. When end of episode is reached, you are responsible for calling <code>reset()</code> to reset this environment's state. Accepts an action and returns a tuple (observation, reward, done, info).</p> <p>Arguments:</p> <ul> <li><code>action</code> object/list - an action provided by the environment</li> </ul> <p>Returns:</p> <ul> <li><code>observation</code> object/list - agent's observation of the current environment   reward (float/list) : amount of reward returned after previous action</li> <li><code>done</code> boolean/list - whether the episode has ended.</li> <li><code>info</code> dict - contains auxiliary diagnostic information.</li> </ul> <p></p>"},{"location":"Python-Gym-API-Documentation/#render","title":"render","text":"<pre><code> | render(mode=\"rgb_array\")\n</code></pre> <p>Return the latest visual observations. Note that it will not render a new frame of the environment.</p> <p></p>"},{"location":"Python-Gym-API-Documentation/#close","title":"close","text":"<pre><code> | close() -&gt; None\n</code></pre> <p>Override _close in your subclass to perform any necessary cleanup. Environments will automatically close() themselves when garbage collected or when the program exits.</p> <p></p>"},{"location":"Python-Gym-API-Documentation/#seed","title":"seed","text":"<pre><code> | seed(seed: Any = None) -&gt; None\n</code></pre> <p>Sets the seed for this env's random number generator(s). Currently not implemented.</p> <p></p>"},{"location":"Python-Gym-API-Documentation/#actionflattener-objects","title":"ActionFlattener Objects","text":"<pre><code>class ActionFlattener()\n</code></pre> <p>Flattens branched discrete action spaces into single-branch discrete action spaces.</p> <p></p>"},{"location":"Python-Gym-API-Documentation/#__init___1","title":"__init__","text":"<pre><code> | __init__(branched_action_space)\n</code></pre> <p>Initialize the flattener.</p> <p>Arguments:</p> <ul> <li><code>branched_action_space</code>: A List containing the sizes of each branch of the action space, e.g. [2,3,3] for three branches with size 2, 3, and 3 respectively.</li> </ul> <p></p>"},{"location":"Python-Gym-API-Documentation/#lookup_action","title":"lookup_action","text":"<pre><code> | lookup_action(action)\n</code></pre> <p>Convert a scalar discrete action into a unique set of branched actions.</p> <p>Arguments:</p> <ul> <li><code>action</code>: A scalar value representing one of the discrete actions.</li> </ul> <p>Returns:</p> <p>The List containing the branched actions.</p>"},{"location":"Python-Gym-API/","title":"Unity ML-Agents Gym Wrapper","text":"<p>A common way in which machine learning researchers interact with simulation environments is via a wrapper provided by OpenAI called <code>gym</code>. For more information on the gym interface, see here.</p> <p>We provide a gym wrapper and instructions for using it with existing machine learning algorithms which utilize gym. Our wrapper provides interfaces on top of our <code>UnityEnvironment</code> class, which is the default way of interfacing with a Unity environment via Python.</p>"},{"location":"Python-Gym-API/#installation","title":"Installation","text":"<p>The gym wrapper is part of the <code>mlgents_envs</code> package. Please refer to the mlagents_envs installation instructions.</p>"},{"location":"Python-Gym-API/#using-the-gym-wrapper","title":"Using the Gym Wrapper","text":"<p>The gym interface is available from <code>gym_unity.envs</code>. To launch an environment from the root of the project repository use:</p> <pre><code>from mlagents_envs.envs.unity_gym_env import UnityToGymWrapper\n\nenv = UnityToGymWrapper(unity_env, uint8_visual, flatten_branched, allow_multiple_obs)\n</code></pre> <ul> <li> <p><code>unity_env</code> refers to the Unity environment to be wrapped.</p> </li> <li> <p><code>uint8_visual</code> refers to whether to output visual observations as <code>uint8</code>   values (0-255). Many common Gym environments (e.g. Atari) do this. By default   they will be floats (0.0-1.0). Defaults to <code>False</code>.</p> </li> <li> <p><code>flatten_branched</code> will flatten a branched discrete action space into a Gym   Discrete. Otherwise, it will be converted into a MultiDiscrete. Defaults to   <code>False</code>.</p> </li> <li> <p><code>allow_multiple_obs</code> will return a list of observations. The first elements   contain the visual observations and the last element contains the array of   vector observations. If False the environment returns a single array (containing   a single visual observations, if present, otherwise the vector observation).   Defaults to <code>False</code>.</p> </li> <li> <p><code>action_space_seed</code> is the optional seed for action sampling. If non-None, will   be used to set the random seed on created gym.Space instances.</p> </li> </ul> <p>The returned environment <code>env</code> will function as a gym.</p>"},{"location":"Python-Gym-API/#limitations","title":"Limitations","text":"<ul> <li>It is only possible to use an environment with a single Agent.</li> <li>By default, the first visual observation is provided as the <code>observation</code>, if   present. Otherwise, vector observations are provided. You can receive all   visual and vector observations by using the <code>allow_multiple_obs=True</code> option in   the gym parameters. If set to <code>True</code>, you will receive a list of <code>observation</code>   instead of only one.</li> <li>The <code>TerminalSteps</code> or <code>DecisionSteps</code> output from the environment can still   be accessed from the <code>info</code> provided by <code>env.step(action)</code>.</li> <li>Stacked vector observations are not supported.</li> <li>Environment registration for use with <code>gym.make()</code> is currently not supported.</li> <li>Calling env.render() will not render a new frame of the environment. It will   return the latest visual observation if using visual observations.</li> </ul>"},{"location":"Python-Gym-API/#running-openai-baselines-algorithms","title":"Running OpenAI Baselines Algorithms","text":"<p>OpenAI provides a set of open-source maintained and tested Reinforcement Learning algorithms called the Baselines.</p> <p>Using the provided Gym wrapper, it is possible to train ML-Agents environments using these algorithms. This requires the creation of custom training scripts to launch each algorithm. In most cases these scripts can be created by making slight modifications to the ones provided for Atari and Mujoco environments.</p> <p>These examples were tested with baselines version 0.1.6.</p>"},{"location":"Python-Gym-API/#example-dqn-baseline","title":"Example - DQN Baseline","text":"<p>In order to train an agent to play the <code>GridWorld</code> environment using the Baselines DQN algorithm, you first need to install the baselines package using pip:</p> <pre><code>pip install git+git://github.com/openai/baselines\n</code></pre> <p>Next, create a file called <code>train_unity.py</code>. Then create an <code>/envs/</code> directory and build the environment to that directory. For more information on building Unity environments, see here. Note that because of limitations of the DQN baseline, the environment must have a single visual observation, a single discrete action and a single Agent in the scene. Add the following code to the <code>train_unity.py</code> file:</p> <pre><code>import gym\n\nfrom baselines import deepq\nfrom baselines import logger\n\nfrom mlagents_envs.environment import UnityEnvironment\nfrom mlagents_envs.envs.unity_gym_env import UnityToGymWrapper\n\n\ndef main():\n  unity_env = UnityEnvironment( &lt; path - to - environment &gt;)\n  env = UnityToGymWrapper(unity_env, uint8_visual=True)\n  logger.configure('./logs')  # Change to log in a different directory\n  act = deepq.learn(\n    env,\n    \"cnn\",  # For visual inputs\n    lr=2.5e-4,\n    total_timesteps=1000000,\n    buffer_size=50000,\n    exploration_fraction=0.05,\n    exploration_final_eps=0.1,\n    print_freq=20,\n    train_freq=5,\n    learning_starts=20000,\n    target_network_update_freq=50,\n    gamma=0.99,\n    prioritized_replay=False,\n    checkpoint_freq=1000,\n    checkpoint_path='./logs',  # Change to save model in a different directory\n    dueling=True\n  )\n  print(\"Saving model to unity_model.pkl\")\n  act.save(\"unity_model.pkl\")\n\n\nif __name__ == '__main__':\n  main()\n</code></pre> <p>To start the training process, run the following from the directory containing <code>train_unity.py</code>:</p> <pre><code>python -m train_unity\n</code></pre>"},{"location":"Python-Gym-API/#other-algorithms","title":"Other Algorithms","text":"<p>Other algorithms in the Baselines repository can be run using scripts similar to the examples from the baselines package. In most cases, the primary changes needed to use a Unity environment are to import <code>UnityToGymWrapper</code>, and to replace the environment creation code, typically <code>gym.make()</code>, with a call to <code>UnityToGymWrapper(unity_environment)</code> passing the environment as input.</p> <p>A typical rule of thumb is that for vision-based environments, modification should be done to Atari training scripts, and for vector observation environments, modification should be done to Mujoco scripts.</p> <p>Some algorithms will make use of <code>make_env()</code> or <code>make_mujoco_env()</code> functions. You can define a similar function for Unity environments. An example of such a method using the PPO2 baseline:</p> <pre><code>from mlagents_envs.environment import UnityEnvironment\nfrom mlagents_envs.envs import UnityToGymWrapper\nfrom baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\nfrom baselines.common.vec_env.dummy_vec_env import DummyVecEnv\nfrom baselines.bench import Monitor\nfrom baselines import logger\nimport baselines.ppo2.ppo2 as ppo2\n\nimport os\n\ntry:\n  from mpi4py import MPI\nexcept ImportError:\n  MPI = None\n\n\ndef make_unity_env(env_directory, num_env, visual, start_index=0):\n  \"\"\"\n  Create a wrapped, monitored Unity environment.\n  \"\"\"\n\n  def make_env(rank, use_visual=True):  # pylint: disable=C0111\n    def _thunk():\n      unity_env = UnityEnvironment(env_directory, base_port=5000 + rank)\n      env = UnityToGymWrapper(unity_env, uint8_visual=True)\n      env = Monitor(env, logger.get_dir() and os.path.join(logger.get_dir(), str(rank)))\n      return env\n\n    return _thunk\n\n  if visual:\n    return SubprocVecEnv([make_env(i + start_index) for i in range(num_env)])\n  else:\n    rank = MPI.COMM_WORLD.Get_rank() if MPI else 0\n    return DummyVecEnv([make_env(rank, use_visual=False)])\n\n\ndef main():\n  env = make_unity_env( &lt; path - to - environment &gt;, 4, True)\n  ppo2.learn(\n    network=\"mlp\",\n    env=env,\n    total_timesteps=100000,\n    lr=1e-3,\n  )\n\n\nif __name__ == '__main__':\n  main()\n</code></pre>"},{"location":"Python-Gym-API/#run-google-dopamine-algorithms","title":"Run Google Dopamine Algorithms","text":"<p>Google provides a framework Dopamine, and implementations of algorithms, e.g. DQN, Rainbow, and the C51 variant of Rainbow. Using the Gym wrapper, we can run Unity environments using Dopamine.</p> <p>First, after installing the Gym wrapper, clone the Dopamine repository.</p> <pre><code>git clone https://github.com/google/dopamine\n</code></pre> <p>Then, follow the appropriate install instructions as specified on Dopamine's homepage. Note that the Dopamine guide specifies using a virtualenv. If you choose to do so, make sure your unity_env package is also installed within the same virtualenv as Dopamine.</p>"},{"location":"Python-Gym-API/#adapting-dopamines-scripts","title":"Adapting Dopamine's Scripts","text":"<p>First, open <code>dopamine/atari/run_experiment.py</code>. Alternatively, copy the entire <code>atari</code> folder, and name it something else (e.g. <code>unity</code>). If you choose the copy approach, be sure to change the package names in the import statements in <code>train.py</code> to your new directory.</p> <p>Within <code>run_experiment.py</code>, we will need to make changes to which environment is instantiated, just as in the Baselines example. At the top of the file, insert</p> <pre><code>from mlagents_envs.environment import UnityEnvironment\nfrom mlagents_envs.envs import UnityToGymWrapper\n</code></pre> <p>to import the Gym Wrapper. Navigate to the <code>create_atari_environment</code> method in the same file, and switch to instantiating a Unity environment by replacing the method with the following code.</p> <pre><code>    game_version = 'v0' if sticky_actions else 'v4'\n    full_game_name = '{}NoFrameskip-{}'.format(game_name, game_version)\n    unity_env = UnityEnvironment(&lt;path-to-environment&gt;)\n    env = UnityToGymWrapper(unity_env, uint8_visual=True)\n    return env\n</code></pre> <p><code>&lt;path-to-environment&gt;</code> is the path to your built Unity executable. For more information on building Unity environments, see here, and note the Limitations section below.</p> <p>Note that we are not using the preprocessor from Dopamine, as it uses many Atari-specific calls. Furthermore, frame-skipping can be done from within Unity, rather than on the Python side.</p>"},{"location":"Python-Gym-API/#limitations_1","title":"Limitations","text":"<p>Since Dopamine is designed around variants of DQN, it is only compatible with discrete action spaces, and specifically the Discrete Gym space. For environments that use branched discrete action spaces, you can enable the <code>flatten_branched</code> parameter in <code>UnityToGymWrapper</code>, which treats each combination of branched actions as separate actions.</p> <p>Furthermore, when building your environments, ensure that your Agent is using visual observations with greyscale enabled, and that the dimensions of the visual observations is 84 by 84 (matches the parameter found in <code>dqn_agent.py</code> and <code>rainbow_agent.py</code>). Dopamine's agents currently do not automatically adapt to the observation dimensions or number of channels.</p>"},{"location":"Python-Gym-API/#hyperparameters","title":"Hyperparameters","text":"<p>The hyperparameters provided by Dopamine are tailored to the Atari games, and you will likely need to adjust them for ML-Agents environments. Here is a sample <code>dopamine/agents/rainbow/configs/rainbow.gin</code> file that is known to work with a simple GridWorld.</p> <pre><code>import dopamine.agents.rainbow.rainbow_agent\nimport dopamine.unity.run_experiment\nimport dopamine.replay_memory.prioritized_replay_buffer\nimport gin.tf.external_configurables\n\nRainbowAgent.num_atoms = 51\nRainbowAgent.stack_size = 1\nRainbowAgent.vmax = 10.\nRainbowAgent.gamma = 0.99\nRainbowAgent.update_horizon = 3\nRainbowAgent.min_replay_history = 20000  # agent steps\nRainbowAgent.update_period = 5\nRainbowAgent.target_update_period = 50  # agent steps\nRainbowAgent.epsilon_train = 0.1\nRainbowAgent.epsilon_eval = 0.01\nRainbowAgent.epsilon_decay_period = 50000  # agent steps\nRainbowAgent.replay_scheme = 'prioritized'\nRainbowAgent.tf_device = '/cpu:0'  # use '/cpu:*' for non-GPU version\nRainbowAgent.optimizer = @tf.train.AdamOptimizer()\n\ntf.train.AdamOptimizer.learning_rate = 0.00025\ntf.train.AdamOptimizer.epsilon = 0.0003125\n\nRunner.game_name = \"Unity\" # any name can be used here\nRunner.sticky_actions = False\nRunner.num_iterations = 200\nRunner.training_steps = 10000  # agent steps\nRunner.evaluation_steps = 500  # agent steps\nRunner.max_steps_per_episode = 27000  # agent steps\n\nWrappedPrioritizedReplayBuffer.replay_capacity = 1000000\nWrappedPrioritizedReplayBuffer.batch_size = 32\n</code></pre> <p>This example assumed you copied <code>atari</code> to a separate folder named <code>unity</code>. Replace <code>unity</code> in <code>import dopamine.unity.run_experiment</code> with the folder you copied your <code>run_experiment.py</code> and <code>trainer.py</code> files to. If you directly modified the existing files, then use <code>atari</code> here.</p>"},{"location":"Python-Gym-API/#starting-a-run","title":"Starting a Run","text":"<p>You can now run Dopamine as you would normally:</p> <pre><code>python -um dopamine.unity.train \\\n  --agent_name=rainbow \\\n  --base_dir=/tmp/dopamine \\\n  --gin_files='dopamine/agents/rainbow/configs/rainbow.gin'\n</code></pre> <p>Again, we assume that you've copied <code>atari</code> into a separate folder. Remember to replace <code>unity</code> with the directory you copied your files into. If you edited the Atari files directly, this should be <code>atari</code>.</p>"},{"location":"Python-Gym-API/#example-gridworld","title":"Example: GridWorld","text":"<p>As a baseline, here are rewards over time for the three algorithms provided with Dopamine as run on the GridWorld example environment. All Dopamine (DQN, Rainbow, C51) runs were done with the same epsilon, epsilon decay, replay history, training steps, and buffer settings as specified above. Note that the first 20000 steps are used to pre-fill the training buffer, and no learning happens.</p> <p>We provide results from our PPO implementation and the DQN from Baselines as reference. Note that all runs used the same greyscale GridWorld as Dopamine. For PPO, <code>num_layers</code> was set to 2, and all other hyperparameters are the default for GridWorld in <code>config/ppo/GridWorld.yaml</code>. For Baselines DQN, the provided hyperparameters in the previous section are used. Note that Baselines implements certain features (e.g. dueling-Q) that are not enabled in Dopamine DQN.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/","title":"Table of Contents","text":"<ul> <li>mlagents_envs.base_env</li> <li>DecisionStep</li> <li>DecisionSteps<ul> <li>agent_id_to_index</li> <li>__getitem__</li> <li>empty</li> </ul> </li> <li>TerminalStep</li> <li>TerminalSteps<ul> <li>agent_id_to_index</li> <li>__getitem__</li> <li>empty</li> </ul> </li> <li>ActionTuple<ul> <li>discrete_dtype</li> </ul> </li> <li>ActionSpec<ul> <li>is_discrete</li> <li>is_continuous</li> <li>discrete_size</li> <li>empty_action</li> <li>random_action</li> <li>create_continuous</li> <li>create_discrete</li> <li>create_hybrid</li> </ul> </li> <li>DimensionProperty<ul> <li>UNSPECIFIED</li> <li>NONE</li> <li>TRANSLATIONAL_EQUIVARIANCE</li> <li>VARIABLE_SIZE</li> </ul> </li> <li>ObservationType<ul> <li>DEFAULT</li> <li>GOAL_SIGNAL</li> </ul> </li> <li>ObservationSpec</li> <li>BehaviorSpec</li> <li>BaseEnv<ul> <li>step</li> <li>reset</li> <li>close</li> <li>behavior_specs</li> <li>set_actions</li> <li>set_action_for_agent</li> <li>get_steps</li> </ul> </li> <li>mlagents_envs.environment</li> <li>UnityEnvironment<ul> <li>__init__</li> <li>close</li> </ul> </li> <li>mlagents_envs.registry</li> <li>mlagents_envs.registry.unity_env_registry</li> <li>UnityEnvRegistry<ul> <li>register</li> <li>register_from_yaml</li> <li>clear</li> <li>__getitem__</li> </ul> </li> <li>mlagents_envs.side_channel</li> <li>mlagents_envs.side_channel.raw_bytes_channel</li> <li>RawBytesChannel<ul> <li>on_message_received</li> <li>get_and_clear_received_messages</li> <li>send_raw_data</li> </ul> </li> <li>mlagents_envs.side_channel.outgoing_message</li> <li>OutgoingMessage<ul> <li>__init__</li> <li>write_bool</li> <li>write_int32</li> <li>write_float32</li> <li>write_float32_list</li> <li>write_string</li> <li>set_raw_bytes</li> </ul> </li> <li>mlagents_envs.side_channel.engine_configuration_channel</li> <li>EngineConfigurationChannel<ul> <li>on_message_received</li> <li>set_configuration_parameters</li> <li>set_configuration</li> </ul> </li> <li>mlagents_envs.side_channel.side_channel_manager</li> <li>SideChannelManager<ul> <li>process_side_channel_message</li> <li>generate_side_channel_messages</li> </ul> </li> <li>mlagents_envs.side_channel.stats_side_channel</li> <li>StatsSideChannel<ul> <li>on_message_received</li> <li>get_and_reset_stats</li> </ul> </li> <li>mlagents_envs.side_channel.incoming_message</li> <li>IncomingMessage<ul> <li>__init__</li> <li>read_bool</li> <li>read_int32</li> <li>read_float32</li> <li>read_float32_list</li> <li>read_string</li> <li>get_raw_bytes</li> </ul> </li> <li>mlagents_envs.side_channel.float_properties_channel</li> <li>FloatPropertiesChannel<ul> <li>on_message_received</li> <li>set_property</li> <li>get_property</li> <li>list_properties</li> <li>get_property_dict_copy</li> </ul> </li> <li>mlagents_envs.side_channel.environment_parameters_channel</li> <li>EnvironmentParametersChannel<ul> <li>set_float_parameter</li> <li>set_uniform_sampler_parameters</li> <li>set_gaussian_sampler_parameters</li> <li>set_multirangeuniform_sampler_parameters</li> </ul> </li> <li>mlagents_envs.side_channel.side_channel</li> <li>SideChannel<ul> <li>queue_message_to_send</li> <li>on_message_received</li> <li>channel_id</li> </ul> </li> </ul>"},{"location":"Python-LLAPI-Documentation/#mlagents_envsbase_env","title":"mlagents_envs.base_env","text":"<p>Python Environment API for the ML-Agents Toolkit The aim of this API is to expose Agents evolving in a simulation to perform reinforcement learning on. This API supports multi-agent scenarios and groups similar Agents (same observations, actions spaces and behavior) together. These groups of Agents are identified by their BehaviorName. For performance reasons, the data of each group of agents is processed in a batched manner. Agents are identified by a unique AgentId identifier that allows tracking of Agents across simulation steps. Note that there is no guarantee that the number or order of the Agents in the state will be consistent across simulation steps. A simulation steps corresponds to moving the simulation forward until at least one agent in the simulation sends its observations to Python again. Since Agents can request decisions at different frequencies, a simulation step does not necessarily correspond to a fixed simulation time increment.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#decisionstep-objects","title":"DecisionStep Objects","text":"<pre><code>class DecisionStep(NamedTuple)\n</code></pre> <p>Contains the data a single Agent collected since the last simulation step.  - obs is a list of numpy arrays observations collected by the agent.  - reward is a float. Corresponds to the rewards collected by the agent  since the last simulation step.  - agent_id is an int and an unique identifier for the corresponding Agent.  - action_mask is an optional list of one dimensional array of booleans.  Only available when using multi-discrete actions.  Each array corresponds to an action branch. Each array contains a mask  for each action of the branch. If true, the action is not available for  the agent during this simulation step.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#decisionsteps-objects","title":"DecisionSteps Objects","text":"<pre><code>class DecisionSteps(Mapping)\n</code></pre> <p>Contains the data a batch of similar Agents collected since the last simulation step. Note that all Agents do not necessarily have new information to send at each simulation step. Therefore, the ordering of agents and the batch size of the DecisionSteps are not fixed across simulation steps.  - obs is a list of numpy arrays observations collected by the batch of  agent. Each obs has one extra dimension compared to DecisionStep: the  first dimension of the array corresponds to the batch size of the batch.  - reward is a float vector of length batch size. Corresponds to the  rewards collected by each agent since the last simulation step.  - agent_id is an int vector of length batch size containing unique  identifier for the corresponding Agent. This is used to track Agents  across simulation steps.  - action_mask is an optional list of two dimensional array of booleans.  Only available when using multi-discrete actions.  Each array corresponds to an action branch. The first dimension of each  array is the batch size and the second contains a mask for each action of  the branch. If true, the action is not available for the agent during  this simulation step.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#agent_id_to_index","title":"agent_id_to_index","text":"<pre><code> | @property\n | agent_id_to_index() -&gt; Dict[AgentId, int]\n</code></pre> <p>Returns:</p> <p>A Dict that maps agent_id to the index of those agents in this DecisionSteps.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#__getitem__","title":"__getitem__","text":"<pre><code> | __getitem__(agent_id: AgentId) -&gt; DecisionStep\n</code></pre> <p>returns the DecisionStep for a specific agent.</p> <p>Arguments:</p> <ul> <li><code>agent_id</code>: The id of the agent</li> </ul> <p>Returns:</p> <p>The DecisionStep</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#empty","title":"empty","text":"<pre><code> | @staticmethod\n | empty(spec: \"BehaviorSpec\") -&gt; \"DecisionSteps\"\n</code></pre> <p>Returns an empty DecisionSteps.</p> <p>Arguments:</p> <ul> <li><code>spec</code>: The BehaviorSpec for the DecisionSteps</li> </ul> <p></p>"},{"location":"Python-LLAPI-Documentation/#terminalstep-objects","title":"TerminalStep Objects","text":"<pre><code>class TerminalStep(NamedTuple)\n</code></pre> <p>Contains the data a single Agent collected when its episode ended.  - obs is a list of numpy arrays observations collected by the agent.  - reward is a float. Corresponds to the rewards collected by the agent  since the last simulation step.  - interrupted is a bool. Is true if the Agent was interrupted since the last  decision step. For example, if the Agent reached the maximum number of steps for  the episode.  - agent_id is an int and an unique identifier for the corresponding Agent.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#terminalsteps-objects","title":"TerminalSteps Objects","text":"<pre><code>class TerminalSteps(Mapping)\n</code></pre> <p>Contains the data a batch of Agents collected when their episode terminated. All Agents present in the TerminalSteps have ended their episode.  - obs is a list of numpy arrays observations collected by the batch of  agent. Each obs has one extra dimension compared to DecisionStep: the  first dimension of the array corresponds to the batch size of the batch.  - reward is a float vector of length batch size. Corresponds to the  rewards collected by each agent since the last simulation step.  - interrupted is an array of booleans of length batch size. Is true if the  associated Agent was interrupted since the last decision step. For example, if the  Agent reached the maximum number of steps for the episode.  - agent_id is an int vector of length batch size containing unique  identifier for the corresponding Agent. This is used to track Agents  across simulation steps.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#agent_id_to_index_1","title":"agent_id_to_index","text":"<pre><code> | @property\n | agent_id_to_index() -&gt; Dict[AgentId, int]\n</code></pre> <p>Returns:</p> <p>A Dict that maps agent_id to the index of those agents in this TerminalSteps.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#__getitem___1","title":"__getitem__","text":"<pre><code> | __getitem__(agent_id: AgentId) -&gt; TerminalStep\n</code></pre> <p>returns the TerminalStep for a specific agent.</p> <p>Arguments:</p> <ul> <li><code>agent_id</code>: The id of the agent</li> </ul> <p>Returns:</p> <p>obs, reward, done, agent_id and optional action mask for a specific agent</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#empty_1","title":"empty","text":"<pre><code> | @staticmethod\n | empty(spec: \"BehaviorSpec\") -&gt; \"TerminalSteps\"\n</code></pre> <p>Returns an empty TerminalSteps.</p> <p>Arguments:</p> <ul> <li><code>spec</code>: The BehaviorSpec for the TerminalSteps</li> </ul> <p></p>"},{"location":"Python-LLAPI-Documentation/#actiontuple-objects","title":"ActionTuple Objects","text":"<pre><code>class ActionTuple(_ActionTupleBase)\n</code></pre> <p>An object whose fields correspond to actions of different types. Continuous and discrete actions are numpy arrays of type float32 and int32, respectively and are type checked on construction. Dimensions are of (n_agents, continuous_size) and (n_agents, discrete_size), respectively. Note, this also holds when continuous or discrete size is zero.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#discrete_dtype","title":"discrete_dtype","text":"<pre><code> | @property\n | discrete_dtype() -&gt; np.dtype\n</code></pre> <p>The dtype of a discrete action.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#actionspec-objects","title":"ActionSpec Objects","text":"<pre><code>class ActionSpec(NamedTuple)\n</code></pre> <p>A NamedTuple containing utility functions and information about the action spaces for a group of Agents under the same behavior. - num_continuous_actions is an int corresponding to the number of floats which constitute the action. - discrete_branch_sizes is a Tuple of int where each int corresponds to the number of discrete actions available to the agent on an independent action branch.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#is_discrete","title":"is_discrete","text":"<pre><code> | is_discrete() -&gt; bool\n</code></pre> <p>Returns true if this Behavior uses discrete actions</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#is_continuous","title":"is_continuous","text":"<pre><code> | is_continuous() -&gt; bool\n</code></pre> <p>Returns true if this Behavior uses continuous actions</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#discrete_size","title":"discrete_size","text":"<pre><code> | @property\n | discrete_size() -&gt; int\n</code></pre> <p>Returns a an int corresponding to the number of discrete branches.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#empty_action","title":"empty_action","text":"<pre><code> | empty_action(n_agents: int) -&gt; ActionTuple\n</code></pre> <p>Generates ActionTuple corresponding to an empty action (all zeros) for a number of agents.</p> <p>Arguments:</p> <ul> <li><code>n_agents</code>: The number of agents that will have actions generated</li> </ul> <p></p>"},{"location":"Python-LLAPI-Documentation/#random_action","title":"random_action","text":"<pre><code> | random_action(n_agents: int) -&gt; ActionTuple\n</code></pre> <p>Generates ActionTuple corresponding to a random action (either discrete or continuous) for a number of agents.</p> <p>Arguments:</p> <ul> <li><code>n_agents</code>: The number of agents that will have actions generated</li> </ul> <p></p>"},{"location":"Python-LLAPI-Documentation/#create_continuous","title":"create_continuous","text":"<pre><code> | @staticmethod\n | create_continuous(continuous_size: int) -&gt; \"ActionSpec\"\n</code></pre> <p>Creates an ActionSpec that is homogenously continuous</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#create_discrete","title":"create_discrete","text":"<pre><code> | @staticmethod\n | create_discrete(discrete_branches: Tuple[int]) -&gt; \"ActionSpec\"\n</code></pre> <p>Creates an ActionSpec that is homogenously discrete</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#create_hybrid","title":"create_hybrid","text":"<pre><code> | @staticmethod\n | create_hybrid(continuous_size: int, discrete_branches: Tuple[int]) -&gt; \"ActionSpec\"\n</code></pre> <p>Creates a hybrid ActionSpace</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#dimensionproperty-objects","title":"DimensionProperty Objects","text":"<pre><code>class DimensionProperty(IntFlag)\n</code></pre> <p>The dimension property of a dimension of an observation.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#unspecified","title":"UNSPECIFIED","text":"<p>No properties specified.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#none","title":"NONE","text":"<p>No Property of the observation in that dimension. Observation can be processed with Fully connected networks.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#translational_equivariance","title":"TRANSLATIONAL_EQUIVARIANCE","text":"<p>Means it is suitable to do a convolution in this dimension.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#variable_size","title":"VARIABLE_SIZE","text":"<p>Means that there can be a variable number of observations in this dimension. The observations are unordered.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#observationtype-objects","title":"ObservationType Objects","text":"<pre><code>class ObservationType(Enum)\n</code></pre> <p>An Enum which defines the type of information carried in the observation of the agent.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#default","title":"DEFAULT","text":"<p>Observation information is generic.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#goal_signal","title":"GOAL_SIGNAL","text":"<p>Observation contains goal information for current task.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#observationspec-objects","title":"ObservationSpec Objects","text":"<pre><code>class ObservationSpec(NamedTuple)\n</code></pre> <p>A NamedTuple containing information about the observation of Agents. - shape is a Tuple of int : It corresponds to the shape of an observation's dimensions. - dimension_property is a Tuple of DimensionProperties flag, one flag for each dimension. - observation_type is an enum of ObservationType.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#behaviorspec-objects","title":"BehaviorSpec Objects","text":"<pre><code>class BehaviorSpec(NamedTuple)\n</code></pre> <p>A NamedTuple containing information about the observation and action spaces for a group of Agents under the same behavior. - observation_specs is a List of ObservationSpec NamedTuple containing information about the information of the Agent's observations such as their shapes. The order of the ObservationSpec is the same as the order of the observations of an agent. - action_spec is an ActionSpec NamedTuple.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#baseenv-objects","title":"BaseEnv Objects","text":"<pre><code>class BaseEnv(ABC)\n</code></pre>"},{"location":"Python-LLAPI-Documentation/#step","title":"step","text":"<pre><code> | @abstractmethod\n | step() -&gt; None\n</code></pre> <p>Signals the environment that it must move the simulation forward by one step.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#reset","title":"reset","text":"<pre><code> | @abstractmethod\n | reset() -&gt; None\n</code></pre> <p>Signals the environment that it must reset the simulation.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#close","title":"close","text":"<pre><code> | @abstractmethod\n | close() -&gt; None\n</code></pre> <p>Signals the environment that it must close.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#behavior_specs","title":"behavior_specs","text":"<pre><code> | @property\n | @abstractmethod\n | behavior_specs() -&gt; MappingType[str, BehaviorSpec]\n</code></pre> <p>Returns a Mapping from behavior names to behavior specs. Agents grouped under the same behavior name have the same action and observation specs, and are expected to behave similarly in the environment. Note that new keys can be added to this mapping as new policies are instantiated.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#set_actions","title":"set_actions","text":"<pre><code> | @abstractmethod\n | set_actions(behavior_name: BehaviorName, action: ActionTuple) -&gt; None\n</code></pre> <p>Sets the action for all of the agents in the simulation for the next step. The Actions must be in the same order as the order received in the DecisionSteps.</p> <p>Arguments:</p> <ul> <li><code>behavior_name</code>: The name of the behavior the agents are part of</li> <li><code>action</code>: ActionTuple tuple of continuous and/or discrete action. Actions are np.arrays with dimensions  (n_agents, continuous_size) and (n_agents, discrete_size), respectively.</li> </ul> <p></p>"},{"location":"Python-LLAPI-Documentation/#set_action_for_agent","title":"set_action_for_agent","text":"<pre><code> | @abstractmethod\n | set_action_for_agent(behavior_name: BehaviorName, agent_id: AgentId, action: ActionTuple) -&gt; None\n</code></pre> <p>Sets the action for one of the agents in the simulation for the next step.</p> <p>Arguments:</p> <ul> <li><code>behavior_name</code>: The name of the behavior the agent is part of</li> <li><code>agent_id</code>: The id of the agent the action is set for</li> <li><code>action</code>: ActionTuple tuple of continuous and/or discrete action Actions are np.arrays with dimensions  (1, continuous_size) and (1, discrete_size), respectively. Note, this initial dimensions of 1 is because this action is meant for a single agent.</li> </ul> <p></p>"},{"location":"Python-LLAPI-Documentation/#get_steps","title":"get_steps","text":"<pre><code> | @abstractmethod\n | get_steps(behavior_name: BehaviorName) -&gt; Tuple[DecisionSteps, TerminalSteps]\n</code></pre> <p>Retrieves the steps of the agents that requested a step in the simulation.</p> <p>Arguments:</p> <ul> <li><code>behavior_name</code>: The name of the behavior the agents are part of</li> </ul> <p>Returns:</p> <p>A tuple containing : - A DecisionSteps NamedTuple containing the observations, the rewards, the agent ids and the action masks for the Agents of the specified behavior. These Agents need an action this step. - A TerminalSteps NamedTuple containing the observations, rewards, agent ids and interrupted flags of the agents that had their episode terminated last step.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#mlagents_envsenvironment","title":"mlagents_envs.environment","text":""},{"location":"Python-LLAPI-Documentation/#unityenvironment-objects","title":"UnityEnvironment Objects","text":"<pre><code>class UnityEnvironment(BaseEnv)\n</code></pre>"},{"location":"Python-LLAPI-Documentation/#__init__","title":"__init__","text":"<pre><code> | __init__(file_name: Optional[str] = None, worker_id: int = 0, base_port: Optional[int] = None, seed: int = 0, no_graphics: bool = False, no_graphics_monitor: bool = False, timeout_wait: int = 60, additional_args: Optional[List[str]] = None, side_channels: Optional[List[SideChannel]] = None, log_folder: Optional[str] = None, num_areas: int = 1)\n</code></pre> <p>Starts a new unity environment and establishes a connection with the environment. Notice: Currently communication between Unity and Python takes place over an open socket without authentication. Ensure that the network where training takes place is secure.</p> <p>:string file_name: Name of Unity environment binary. :int base_port: Baseline port number to connect to Unity environment over. worker_id increments over this. If no environment is specified (i.e. file_name is None), the DEFAULT_EDITOR_PORT will be used. :int worker_id: Offset from base_port. Used for training multiple environments simultaneously. :bool no_graphics: Whether to run the Unity simulator in no-graphics mode :bool no_graphics_monitor: Whether to run the main worker in graphics mode, with the remaining in no-graphics mode :int timeout_wait: Time (in seconds) to wait for connection from environment. :list args: Addition Unity command line arguments :list side_channels: Additional side channel for no-rl communication with Unity :str log_folder: Optional folder to write the Unity Player log file into.  Requires absolute path.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#close_1","title":"close","text":"<pre><code> | close()\n</code></pre> <p>Sends a shutdown signal to the unity environment, and closes the socket connection.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#mlagents_envsregistry","title":"mlagents_envs.registry","text":""},{"location":"Python-LLAPI-Documentation/#mlagents_envsregistryunity_env_registry","title":"mlagents_envs.registry.unity_env_registry","text":""},{"location":"Python-LLAPI-Documentation/#unityenvregistry-objects","title":"UnityEnvRegistry Objects","text":"<pre><code>class UnityEnvRegistry(Mapping)\n</code></pre>"},{"location":"Python-LLAPI-Documentation/#unityenvregistry","title":"UnityEnvRegistry","text":"<p>Provides a library of Unity environments that can be launched without the need of downloading the Unity Editor. The UnityEnvRegistry implements a Map, to access an entry of the Registry, use:</p> <pre><code>registry = UnityEnvRegistry()\nentry = registry[&lt;environment_identifyier&gt;]\n</code></pre> <p>An entry has the following properties :  * <code>identifier</code> : Uniquely identifies this environment  * <code>expected_reward</code> : Corresponds to the reward an agent must obtained for the task  to be considered completed.  * <code>description</code> : A human readable description of the environment.</p> <p>To launch a Unity environment from a registry entry, use the <code>make</code> method:</p> <pre><code>registry = UnityEnvRegistry()\nenv = registry[&lt;environment_identifyier&gt;].make()\n</code></pre> <p></p>"},{"location":"Python-LLAPI-Documentation/#register","title":"register","text":"<pre><code> | register(new_entry: BaseRegistryEntry) -&gt; None\n</code></pre> <p>Registers a new BaseRegistryEntry to the registry. The BaseRegistryEntry.identifier value will be used as indexing key. If two are more environments are registered under the same key, the most recentry added will replace the others.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#register_from_yaml","title":"register_from_yaml","text":"<pre><code> | register_from_yaml(path_to_yaml: str) -&gt; None\n</code></pre> <p>Registers the environments listed in a yaml file (either local or remote). Note that the entries are registered lazily: the registration will only happen when an environment is accessed. The yaml file must have the following format :</p> <pre><code>environments:\n- &lt;identifier of the first environment&gt;:\n    expected_reward: &lt;expected reward of the environment&gt;\n    description: | &lt;a multi line description of the environment&gt;\n      &lt;continued multi line description&gt;\n    linux_url: &lt;The url for the Linux executable zip file&gt;\n    darwin_url: &lt;The url for the OSX executable zip file&gt;\n    win_url: &lt;The url for the Windows executable zip file&gt;\n\n- &lt;identifier of the second environment&gt;:\n    expected_reward: &lt;expected reward of the environment&gt;\n    description: | &lt;a multi line description of the environment&gt;\n      &lt;continued multi line description&gt;\n    linux_url: &lt;The url for the Linux executable zip file&gt;\n    darwin_url: &lt;The url for the OSX executable zip file&gt;\n    win_url: &lt;The url for the Windows executable zip file&gt;\n\n- ...\n</code></pre> <p>Arguments:</p> <ul> <li><code>path_to_yaml</code>: A local path or url to the yaml file</li> </ul> <p></p>"},{"location":"Python-LLAPI-Documentation/#clear","title":"clear","text":"<pre><code> | clear() -&gt; None\n</code></pre> <p>Deletes all entries in the registry.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#__getitem___2","title":"__getitem__","text":"<pre><code> | __getitem__(identifier: str) -&gt; BaseRegistryEntry\n</code></pre> <p>Returns the BaseRegistryEntry with the provided identifier. BaseRegistryEntry can then be used to make a Unity Environment.</p> <p>Arguments:</p> <ul> <li><code>identifier</code>: The identifier of the BaseRegistryEntry</li> </ul> <p>Returns:</p> <p>The associated BaseRegistryEntry</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#mlagents_envsside_channel","title":"mlagents_envs.side_channel","text":""},{"location":"Python-LLAPI-Documentation/#mlagents_envsside_channelraw_bytes_channel","title":"mlagents_envs.side_channel.raw_bytes_channel","text":""},{"location":"Python-LLAPI-Documentation/#rawbyteschannel-objects","title":"RawBytesChannel Objects","text":"<pre><code>class RawBytesChannel(SideChannel)\n</code></pre> <p>This is an example of what the SideChannel for raw bytes exchange would look like. Is meant to be used for general research purpose.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#on_message_received","title":"on_message_received","text":"<pre><code> | on_message_received(msg: IncomingMessage) -&gt; None\n</code></pre> <p>Is called by the environment to the side channel. Can be called multiple times per step if multiple messages are meant for that SideChannel.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#get_and_clear_received_messages","title":"get_and_clear_received_messages","text":"<pre><code> | get_and_clear_received_messages() -&gt; List[bytes]\n</code></pre> <p>returns a list of bytearray received from the environment.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#send_raw_data","title":"send_raw_data","text":"<pre><code> | send_raw_data(data: bytearray) -&gt; None\n</code></pre> <p>Queues a message to be sent by the environment at the next call to step.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#mlagents_envsside_channeloutgoing_message","title":"mlagents_envs.side_channel.outgoing_message","text":""},{"location":"Python-LLAPI-Documentation/#outgoingmessage-objects","title":"OutgoingMessage Objects","text":"<pre><code>class OutgoingMessage()\n</code></pre> <p>Utility class for forming the message that is written to a SideChannel. All data is written in little-endian format using the struct module.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#__init___1","title":"__init__","text":"<pre><code> | __init__()\n</code></pre> <p>Create an OutgoingMessage with an empty buffer.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#write_bool","title":"write_bool","text":"<pre><code> | write_bool(b: bool) -&gt; None\n</code></pre> <p>Append a boolean value.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#write_int32","title":"write_int32","text":"<pre><code> | write_int32(i: int) -&gt; None\n</code></pre> <p>Append an integer value.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#write_float32","title":"write_float32","text":"<pre><code> | write_float32(f: float) -&gt; None\n</code></pre> <p>Append a float value. It will be truncated to 32-bit precision.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#write_float32_list","title":"write_float32_list","text":"<pre><code> | write_float32_list(float_list: List[float]) -&gt; None\n</code></pre> <p>Append a list of float values. They will be truncated to 32-bit precision.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#write_string","title":"write_string","text":"<pre><code> | write_string(s: str) -&gt; None\n</code></pre> <p>Append a string value. Internally, it will be encoded to ascii, and the encoded length will also be written to the message.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#set_raw_bytes","title":"set_raw_bytes","text":"<pre><code> | set_raw_bytes(buffer: bytearray) -&gt; None\n</code></pre> <p>Set the internal buffer to a new bytearray. This will overwrite any existing data.</p> <p>Arguments:</p> <ul> <li><code>buffer</code>:</li> </ul> <p>Returns:</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#mlagents_envsside_channelengine_configuration_channel","title":"mlagents_envs.side_channel.engine_configuration_channel","text":""},{"location":"Python-LLAPI-Documentation/#engineconfigurationchannel-objects","title":"EngineConfigurationChannel Objects","text":"<pre><code>class EngineConfigurationChannel(SideChannel)\n</code></pre> <p>This is the SideChannel for engine configuration exchange. The data in the engine configuration is as follows :  - int width;  - int height;  - int qualityLevel;  - float timeScale;  - int targetFrameRate;  - int captureFrameRate;</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#on_message_received_1","title":"on_message_received","text":"<pre><code> | on_message_received(msg: IncomingMessage) -&gt; None\n</code></pre> <p>Is called by the environment to the side channel. Can be called multiple times per step if multiple messages are meant for that SideChannel. Note that Python should never receive an engine configuration from Unity</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#set_configuration_parameters","title":"set_configuration_parameters","text":"<pre><code> | set_configuration_parameters(width: Optional[int] = None, height: Optional[int] = None, quality_level: Optional[int] = None, time_scale: Optional[float] = None, target_frame_rate: Optional[int] = None, capture_frame_rate: Optional[int] = None) -&gt; None\n</code></pre> <p>Sets the engine configuration. Takes as input the configurations of the engine.</p> <p>Arguments:</p> <ul> <li><code>width</code>: Defines the width of the display. (Must be set alongside height)</li> <li><code>height</code>: Defines the height of the display. (Must be set alongside width)</li> <li><code>quality_level</code>: Defines the quality level of the simulation.</li> <li><code>time_scale</code>: Defines the multiplier for the deltatime in the simulation. If set to a higher value, time will pass faster in the simulation but the physics might break.</li> <li><code>target_frame_rate</code>: Instructs simulation to try to render at a specified frame rate.</li> <li><code>capture_frame_rate</code>: Instructs the simulation to consider time between updates to always be constant, regardless of the actual frame rate.</li> </ul> <p></p>"},{"location":"Python-LLAPI-Documentation/#set_configuration","title":"set_configuration","text":"<pre><code> | set_configuration(config: EngineConfig) -&gt; None\n</code></pre> <p>Sets the engine configuration. Takes as input an EngineConfig.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#mlagents_envsside_channelside_channel_manager","title":"mlagents_envs.side_channel.side_channel_manager","text":""},{"location":"Python-LLAPI-Documentation/#sidechannelmanager-objects","title":"SideChannelManager Objects","text":"<pre><code>class SideChannelManager()\n</code></pre>"},{"location":"Python-LLAPI-Documentation/#process_side_channel_message","title":"process_side_channel_message","text":"<pre><code> | process_side_channel_message(data: bytes) -&gt; None\n</code></pre> <p>Separates the data received from Python into individual messages for each registered side channel and calls on_message_received on them.</p> <p>Arguments:</p> <ul> <li><code>data</code>: The packed message sent by Unity</li> </ul> <p></p>"},{"location":"Python-LLAPI-Documentation/#generate_side_channel_messages","title":"generate_side_channel_messages","text":"<pre><code> | generate_side_channel_messages() -&gt; bytearray\n</code></pre> <p>Gathers the messages that the registered side channels will send to Unity and combines them into a single message ready to be sent.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#mlagents_envsside_channelstats_side_channel","title":"mlagents_envs.side_channel.stats_side_channel","text":""},{"location":"Python-LLAPI-Documentation/#statssidechannel-objects","title":"StatsSideChannel Objects","text":"<pre><code>class StatsSideChannel(SideChannel)\n</code></pre> <p>Side channel that receives (string, float) pairs from the environment, so that they can eventually be passed to a StatsReporter.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#on_message_received_2","title":"on_message_received","text":"<pre><code> | on_message_received(msg: IncomingMessage) -&gt; None\n</code></pre> <p>Receive the message from the environment, and save it for later retrieval.</p> <p>Arguments:</p> <ul> <li><code>msg</code>:</li> </ul> <p>Returns:</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#get_and_reset_stats","title":"get_and_reset_stats","text":"<pre><code> | get_and_reset_stats() -&gt; EnvironmentStats\n</code></pre> <p>Returns the current stats, and resets the internal storage of the stats.</p> <p>Returns:</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#mlagents_envsside_channelincoming_message","title":"mlagents_envs.side_channel.incoming_message","text":""},{"location":"Python-LLAPI-Documentation/#incomingmessage-objects","title":"IncomingMessage Objects","text":"<pre><code>class IncomingMessage()\n</code></pre> <p>Utility class for reading the message written to a SideChannel. Values must be read in the order they were written.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#__init___2","title":"__init__","text":"<pre><code> | __init__(buffer: bytes, offset: int = 0)\n</code></pre> <p>Create a new IncomingMessage from the bytes.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#read_bool","title":"read_bool","text":"<pre><code> | read_bool(default_value: bool = False) -&gt; bool\n</code></pre> <p>Read a boolean value from the message buffer.</p> <p>Arguments:</p> <ul> <li><code>default_value</code>: Default value to use if the end of the message is reached.</li> </ul> <p>Returns:</p> <p>The value read from the message, or the default value if the end was reached.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#read_int32","title":"read_int32","text":"<pre><code> | read_int32(default_value: int = 0) -&gt; int\n</code></pre> <p>Read an integer value from the message buffer.</p> <p>Arguments:</p> <ul> <li><code>default_value</code>: Default value to use if the end of the message is reached.</li> </ul> <p>Returns:</p> <p>The value read from the message, or the default value if the end was reached.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#read_float32","title":"read_float32","text":"<pre><code> | read_float32(default_value: float = 0.0) -&gt; float\n</code></pre> <p>Read a float value from the message buffer.</p> <p>Arguments:</p> <ul> <li><code>default_value</code>: Default value to use if the end of the message is reached.</li> </ul> <p>Returns:</p> <p>The value read from the message, or the default value if the end was reached.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#read_float32_list","title":"read_float32_list","text":"<pre><code> | read_float32_list(default_value: List[float] = None) -&gt; List[float]\n</code></pre> <p>Read a list of float values from the message buffer.</p> <p>Arguments:</p> <ul> <li><code>default_value</code>: Default value to use if the end of the message is reached.</li> </ul> <p>Returns:</p> <p>The value read from the message, or the default value if the end was reached.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#read_string","title":"read_string","text":"<pre><code> | read_string(default_value: str = \"\") -&gt; str\n</code></pre> <p>Read a string value from the message buffer.</p> <p>Arguments:</p> <ul> <li><code>default_value</code>: Default value to use if the end of the message is reached.</li> </ul> <p>Returns:</p> <p>The value read from the message, or the default value if the end was reached.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#get_raw_bytes","title":"get_raw_bytes","text":"<pre><code> | get_raw_bytes() -&gt; bytes\n</code></pre> <p>Get a copy of the internal bytes used by the message.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#mlagents_envsside_channelfloat_properties_channel","title":"mlagents_envs.side_channel.float_properties_channel","text":""},{"location":"Python-LLAPI-Documentation/#floatpropertieschannel-objects","title":"FloatPropertiesChannel Objects","text":"<pre><code>class FloatPropertiesChannel(SideChannel)\n</code></pre> <p>This is the SideChannel for float properties shared with Unity. You can modify the float properties of an environment with the commands set_property, get_property and list_properties.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#on_message_received_3","title":"on_message_received","text":"<pre><code> | on_message_received(msg: IncomingMessage) -&gt; None\n</code></pre> <p>Is called by the environment to the side channel. Can be called multiple times per step if multiple messages are meant for that SideChannel.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#set_property","title":"set_property","text":"<pre><code> | set_property(key: str, value: float) -&gt; None\n</code></pre> <p>Sets a property in the Unity Environment.</p> <p>Arguments:</p> <ul> <li><code>key</code>: The string identifier of the property.</li> <li><code>value</code>: The float value of the property.</li> </ul> <p></p>"},{"location":"Python-LLAPI-Documentation/#get_property","title":"get_property","text":"<pre><code> | get_property(key: str) -&gt; Optional[float]\n</code></pre> <p>Gets a property in the Unity Environment. If the property was not found, will return None.</p> <p>Arguments:</p> <ul> <li><code>key</code>: The string identifier of the property.</li> </ul> <p>Returns:</p> <p>The float value of the property or None.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#list_properties","title":"list_properties","text":"<pre><code> | list_properties() -&gt; List[str]\n</code></pre> <p>Returns a list of all the string identifiers of the properties currently present in the Unity Environment.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#get_property_dict_copy","title":"get_property_dict_copy","text":"<pre><code> | get_property_dict_copy() -&gt; Dict[str, float]\n</code></pre> <p>Returns a copy of the float properties.</p> <p>Returns:</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#mlagents_envsside_channelenvironment_parameters_channel","title":"mlagents_envs.side_channel.environment_parameters_channel","text":""},{"location":"Python-LLAPI-Documentation/#environmentparameterschannel-objects","title":"EnvironmentParametersChannel Objects","text":"<pre><code>class EnvironmentParametersChannel(SideChannel)\n</code></pre> <p>This is the SideChannel for sending environment parameters to Unity. You can send parameters to an environment with the command set_float_parameter.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#set_float_parameter","title":"set_float_parameter","text":"<pre><code> | set_float_parameter(key: str, value: float) -&gt; None\n</code></pre> <p>Sets a float environment parameter in the Unity Environment.</p> <p>Arguments:</p> <ul> <li><code>key</code>: The string identifier of the parameter.</li> <li><code>value</code>: The float value of the parameter.</li> </ul> <p></p>"},{"location":"Python-LLAPI-Documentation/#set_uniform_sampler_parameters","title":"set_uniform_sampler_parameters","text":"<pre><code> | set_uniform_sampler_parameters(key: str, min_value: float, max_value: float, seed: int) -&gt; None\n</code></pre> <p>Sets a uniform environment parameter sampler.</p> <p>Arguments:</p> <ul> <li><code>key</code>: The string identifier of the parameter.</li> <li><code>min_value</code>: The minimum of the sampling distribution.</li> <li><code>max_value</code>: The maximum of the sampling distribution.</li> <li><code>seed</code>: The random seed to initialize the sampler.</li> </ul> <p></p>"},{"location":"Python-LLAPI-Documentation/#set_gaussian_sampler_parameters","title":"set_gaussian_sampler_parameters","text":"<pre><code> | set_gaussian_sampler_parameters(key: str, mean: float, st_dev: float, seed: int) -&gt; None\n</code></pre> <p>Sets a gaussian environment parameter sampler.</p> <p>Arguments:</p> <ul> <li><code>key</code>: The string identifier of the parameter.</li> <li><code>mean</code>: The mean of the sampling distribution.</li> <li><code>st_dev</code>: The standard deviation of the sampling distribution.</li> <li><code>seed</code>: The random seed to initialize the sampler.</li> </ul> <p></p>"},{"location":"Python-LLAPI-Documentation/#set_multirangeuniform_sampler_parameters","title":"set_multirangeuniform_sampler_parameters","text":"<pre><code> | set_multirangeuniform_sampler_parameters(key: str, intervals: List[Tuple[float, float]], seed: int) -&gt; None\n</code></pre> <p>Sets a multirangeuniform environment parameter sampler.</p> <p>Arguments:</p> <ul> <li><code>key</code>: The string identifier of the parameter.</li> <li><code>intervals</code>: The lists of min and max that define each uniform distribution.</li> <li><code>seed</code>: The random seed to initialize the sampler.</li> </ul> <p></p>"},{"location":"Python-LLAPI-Documentation/#mlagents_envsside_channelside_channel","title":"mlagents_envs.side_channel.side_channel","text":""},{"location":"Python-LLAPI-Documentation/#sidechannel-objects","title":"SideChannel Objects","text":"<pre><code>class SideChannel(ABC)\n</code></pre> <p>The side channel just get access to a bytes buffer that will be shared between C# and Python. For example, We will create a specific side channel for properties that will be a list of string (fixed size) to float number, that can be modified by both C# and Python. All side channels are passed to the Env object at construction.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#queue_message_to_send","title":"queue_message_to_send","text":"<pre><code> | queue_message_to_send(msg: OutgoingMessage) -&gt; None\n</code></pre> <p>Queues a message to be sent by the environment at the next call to step.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#on_message_received_4","title":"on_message_received","text":"<pre><code> | @abstractmethod\n | on_message_received(msg: IncomingMessage) -&gt; None\n</code></pre> <p>Is called by the environment to the side channel. Can be called multiple times per step if multiple messages are meant for that SideChannel.</p> <p></p>"},{"location":"Python-LLAPI-Documentation/#channel_id","title":"channel_id","text":"<pre><code> | @property\n | channel_id() -&gt; uuid.UUID\n</code></pre> <p>Returns:</p> <p>The type of side channel used. Will influence how the data is processed in the environment.</p>"},{"location":"Python-LLAPI/","title":"Unity ML-Agents Python Low Level API","text":"<p>The <code>mlagents</code> Python package contains two components: a low level API which allows you to interact directly with a Unity Environment (<code>mlagents_envs</code>) and an entry point to train (<code>mlagents-learn</code>) which allows you to train agents in Unity Environments using our implementations of reinforcement learning or imitation learning. This document describes how to use the <code>mlagents_envs</code> API. For information on using <code>mlagents-learn</code>, see here. For Python Low Level API documentation, see here.</p> <p>The Python Low Level API can be used to interact directly with your Unity learning environment. As such, it can serve as the basis for developing and evaluating new learning algorithms.</p>"},{"location":"Python-LLAPI/#mlagents_envs","title":"mlagents_envs","text":"<p>The ML-Agents Toolkit Low Level API is a Python API for controlling the simulation loop of an environment or game built with Unity. This API is used by the training algorithms inside the ML-Agent Toolkit, but you can also write your own Python programs using this API.</p> <p>The key objects in the Python API include:</p> <ul> <li>UnityEnvironment \u2014 the main interface between the Unity application and   your code. Use UnityEnvironment to start and control a simulation or training   session.</li> <li>BehaviorName - is a string that identifies a behavior in the simulation.</li> <li>AgentId - is an <code>int</code> that serves as unique identifier for Agents in the   simulation.</li> <li>DecisionSteps \u2014 contains the data from Agents belonging to the same   \"Behavior\" in the simulation, such as observations and rewards. Only Agents   that requested a decision since the last call to <code>env.step()</code> are in the   DecisionSteps object.</li> <li>TerminalSteps \u2014 contains the data from Agents belonging to the same   \"Behavior\" in the simulation, such as observations and rewards. Only Agents   whose episode ended since the last call to <code>env.step()</code> are in the   TerminalSteps object.</li> <li>BehaviorSpec \u2014 describes the shape of the observation data inside   DecisionSteps and TerminalSteps as well as the expected action shapes.</li> </ul> <p>These classes are all defined in the base_env script.</p> <p>An Agent \"Behavior\" is a group of Agents identified by a <code>BehaviorName</code> that share the same observations and action types (described in their <code>BehaviorSpec</code>). You can think about Agent Behavior as a group of agents that will share the same policy. All Agents with the same behavior have the same goal and reward signals.</p> <p>To communicate with an Agent in a Unity environment from a Python program, the Agent in the simulation must have <code>Behavior Parameters</code> set to communicate. You must set the <code>Behavior Type</code> to <code>Default</code> and give it a <code>Behavior Name</code>.</p> <p>Notice: Currently communication between Unity and Python takes place over an open socket without authentication. As such, please make sure that the network where training takes place is secure. This will be addressed in a future release.</p>"},{"location":"Python-LLAPI/#loading-a-unity-environment","title":"Loading a Unity Environment","text":"<p>Python-side communication happens through <code>UnityEnvironment</code> which is located in <code>environment.py</code>. To load a Unity environment from a built binary file, put the file in the same directory as <code>envs</code>. For example, if the filename of your Unity environment is <code>3DBall</code>, in python, run:</p> <pre><code>from mlagents_envs.environment import UnityEnvironment\n# This is a non-blocking call that only loads the environment.\nenv = UnityEnvironment(file_name=\"3DBall\", seed=1, side_channels=[])\n# Start interacting with the environment.\nenv.reset()\nbehavior_names = env.behavior_specs.keys()\n...\n</code></pre> <p>NOTE: Please read Interacting with a Unity Environment to read more about how you can interact with the Unity environment from Python.</p> <ul> <li><code>file_name</code> is the name of the environment binary (located in the root   directory of the python project).</li> <li><code>worker_id</code> indicates which port to use for communication with the   environment. For use in parallel training regimes such as A3C.</li> <li><code>seed</code> indicates the seed to use when generating random numbers during the   training process. In environments which are stochastic, setting the seed   enables reproducible experimentation by ensuring that the environment and   trainers utilize the same random seed.</li> <li><code>side_channels</code> provides a way to exchange data with the Unity simulation that   is not related to the reinforcement learning loop. For example: configurations   or properties. More on them in the Side Channels doc.</li> </ul> <p>If you want to directly interact with the Editor, you need to use <code>file_name=None</code>, then press the Play button in the Editor when the message \"Start training by pressing the Play button in the Unity Editor\" is displayed on the screen</p>"},{"location":"Python-LLAPI/#interacting-with-a-unity-environment","title":"Interacting with a Unity Environment","text":""},{"location":"Python-LLAPI/#the-baseenv-interface","title":"The BaseEnv interface","text":"<p>A <code>BaseEnv</code> has the following methods:</p> <ul> <li>Reset : <code>env.reset()</code> Sends a signal to reset the environment. Returns   None.</li> <li>Step : <code>env.step()</code> Sends a signal to step the environment. Returns None.   Note that a \"step\" for Python does not correspond to either Unity <code>Update</code> nor   <code>FixedUpdate</code>. When <code>step()</code> or <code>reset()</code> is called, the Unity simulation will   move forward until an Agent in the simulation needs a input from Python to   act.</li> <li>Close : <code>env.close()</code> Sends a shutdown signal to the environment and   terminates the communication.</li> <li>Behavior Specs : <code>env.behavior_specs</code> Returns a Mapping of   <code>BehaviorName</code> to <code>BehaviorSpec</code> objects (read only).   A <code>BehaviorSpec</code> contains the observation shapes and the   <code>ActionSpec</code> (which defines the action shape). Note that   the <code>BehaviorSpec</code> for a specific group is fixed throughout the simulation.   The number of entries in the Mapping can change over time in the simulation   if new Agent behaviors are created in the simulation.</li> <li>Get Steps : <code>env.get_steps(behavior_name: str)</code> Returns a tuple   <code>DecisionSteps, TerminalSteps</code> corresponding to the behavior_name given as   input. The <code>DecisionSteps</code> contains information about the state of the agents   that need an action this step and have the behavior behavior_name. The   <code>TerminalSteps</code> contains information about the state of the agents whose   episode ended and have the behavior behavior_name. Both <code>DecisionSteps</code> and   <code>TerminalSteps</code> contain information such as the observations, the rewards and   the agent identifiers. <code>DecisionSteps</code> also contains action masks for the next   action while <code>TerminalSteps</code> contains the reason for termination (did the   Agent reach its maximum step and was interrupted). The data is in <code>np.array</code>   of which the first dimension is always the number of agents note that the   number of agents is not guaranteed to remain constant during the simulation   and it is not unusual to have either <code>DecisionSteps</code> or <code>TerminalSteps</code>   contain no Agents at all.</li> <li>Set Actions :<code>env.set_actions(behavior_name: str, action: ActionTuple)</code> Sets   the actions for a whole agent group. <code>action</code> is an <code>ActionTuple</code>, which   is made up of a 2D <code>np.array</code> of <code>dtype=np.int32</code> for discrete actions, and   <code>dtype=np.float32</code> for continuous actions. The first dimension of <code>np.array</code>   in the tuple is the number of agents that requested a decision since the   last call to <code>env.step()</code>. The second dimension is the number of discrete or   continuous actions for the corresponding array.</li> <li>Set Action for Agent :   <code>env.set_action_for_agent(agent_group: str, agent_id: int, action: ActionTuple)</code>   Sets the action for a specific Agent in an agent group. <code>agent_group</code> is the   name of the group the Agent belongs to and <code>agent_id</code> is the integer   identifier of the Agent. <code>action</code> is an <code>ActionTuple</code> as described above. Note: If no action is provided for an agent group between two calls to <code>env.step()</code> then the default action will be all zeros.</li> </ul>"},{"location":"Python-LLAPI/#decisionsteps-and-decisionstep","title":"DecisionSteps and DecisionStep","text":"<p><code>DecisionSteps</code> (with <code>s</code>) contains information about a whole batch of Agents while <code>DecisionStep</code> (no <code>s</code>) only contains information about a single Agent.</p> <p>A <code>DecisionSteps</code> has the following fields :</p> <ul> <li><code>obs</code> is a list of numpy arrays observations collected by the group of agent.   The first dimension of the array corresponds to the batch size of the group   (number of agents requesting a decision since the last call to <code>env.step()</code>).</li> <li><code>reward</code> is a float vector of length batch size. Corresponds to the rewards   collected by each agent since the last simulation step.</li> <li><code>agent_id</code> is an int vector of length batch size containing unique identifier   for the corresponding Agent. This is used to track Agents across simulation   steps.</li> <li><code>action_mask</code> is an optional list of two dimensional arrays of booleans which is only   available when using multi-discrete actions. Each array corresponds to an   action branch. The first dimension of each array is the batch size and the   second contains a mask for each action of the branch. If true, the action is   not available for the agent during this simulation step.</li> </ul> <p>It also has the two following methods:</p> <ul> <li><code>len(DecisionSteps)</code> Returns the number of agents requesting a decision since   the last call to <code>env.step()</code>.</li> <li><code>DecisionSteps[agent_id]</code> Returns a <code>DecisionStep</code> for the Agent with the   <code>agent_id</code> unique identifier.</li> </ul> <p>A <code>DecisionStep</code> has the following fields:</p> <ul> <li><code>obs</code> is a list of numpy arrays observations collected by the agent. (Each   array has one less dimension than the arrays in <code>DecisionSteps</code>)</li> <li><code>reward</code> is a float. Corresponds to the rewards collected by the agent since   the last simulation step.</li> <li><code>agent_id</code> is an int and an unique identifier for the corresponding Agent.</li> <li><code>action_mask</code> is an optional list of one dimensional arrays of booleans which is only   available when using multi-discrete actions. Each array corresponds to an   action branch. Each array contains a mask for each action of the branch. If   true, the action is not available for the agent during this simulation step.</li> </ul>"},{"location":"Python-LLAPI/#terminalsteps-and-terminalstep","title":"TerminalSteps and TerminalStep","text":"<p>Similarly to <code>DecisionSteps</code> and <code>DecisionStep</code>, <code>TerminalSteps</code> (with <code>s</code>) contains information about a whole batch of Agents while <code>TerminalStep</code> (no <code>s</code>) only contains information about a single Agent.</p> <p>A <code>TerminalSteps</code> has the following fields :</p> <ul> <li><code>obs</code> is a list of numpy arrays observations collected by the group of agent.   The first dimension of the array corresponds to the batch size of the group   (number of agents requesting a decision since the last call to <code>env.step()</code>).</li> <li><code>reward</code> is a float vector of length batch size. Corresponds to the rewards   collected by each agent since the last simulation step.</li> <li><code>agent_id</code> is an int vector of length batch size containing unique identifier   for the corresponding Agent. This is used to track Agents across simulation   steps.</li> <li><code>interrupted</code> is an array of booleans of length batch size. Is true if the  associated Agent was interrupted since the last decision step. For example,  if the Agent reached the maximum number of steps for the episode.</li> </ul> <p>It also has the two following methods:</p> <ul> <li><code>len(TerminalSteps)</code> Returns the number of agents requesting a decision since   the last call to <code>env.step()</code>.</li> <li><code>TerminalSteps[agent_id]</code> Returns a <code>TerminalStep</code> for the Agent with the   <code>agent_id</code> unique identifier.</li> </ul> <p>A <code>TerminalStep</code> has the following fields:</p> <ul> <li><code>obs</code> is a list of numpy arrays observations collected by the agent. (Each   array has one less dimension than the arrays in <code>TerminalSteps</code>)</li> <li><code>reward</code> is a float. Corresponds to the rewards collected by the agent since   the last simulation step.</li> <li><code>agent_id</code> is an int and an unique identifier for the corresponding Agent.</li> <li><code>interrupted</code> is a bool. Is true if the Agent was interrupted since the last  decision step. For example, if the Agent reached the maximum number of steps for  the episode.</li> </ul>"},{"location":"Python-LLAPI/#behaviorspec","title":"BehaviorSpec","text":"<p>A <code>BehaviorSpec</code> has the following fields :</p> <ul> <li><code>observation_specs</code> is a List of <code>ObservationSpec</code> objects : Each <code>ObservationSpec</code>   corresponds to an observation's properties: <code>shape</code> is a tuple of ints that   corresponds to the shape of the observation (without the number of agents dimension).   <code>dimension_property</code> is a tuple of flags containing extra information about how the   data should be processed in the corresponding dimension. <code>observation_type</code> is an enum   corresponding to what type of observation is generating the data (i.e., default, goal,   etc). Note that the <code>ObservationSpec</code> have the same ordering as the ordering of observations   in the DecisionSteps, DecisionStep, TerminalSteps and TerminalStep.</li> <li><code>action_spec</code> is an <code>ActionSpec</code> namedtuple that defines the number and types   of actions for the Agent.</li> </ul> <p>An <code>ActionSpec</code> has the following fields and properties: - <code>continuous_size</code> is the number of floats that constitute the continuous actions. - <code>discrete_size</code> is the number of branches (the number of independent actions) that   constitute the multi-discrete actions. - <code>discrete_branches</code> is a Tuple of ints. Each int corresponds to the number of   different options for each branch of the action. For example:   In a game direction input (no movement, left, right) and   jump input (no jump, jump) there will be two branches (direction and jump),   the first one with 3 options and the second with 2 options. (<code>discrete_size = 2</code>   and <code>discrete_action_branches = (3,2,)</code>)</p>"},{"location":"Python-LLAPI/#communicating-additional-information-with-the-environment","title":"Communicating additional information with the Environment","text":"<p>In addition to the means of communicating between Unity and python described above, we also provide methods for sharing agent-agnostic information. These additional methods are referred to as side channels. ML-Agents includes two ready-made side channels, described below. It is also possible to create custom side channels to communicate any additional data between a Unity environment and Python. Instructions for creating custom side channels can be found here.</p> <p>Side channels exist as separate classes which are instantiated, and then passed as list to the <code>side_channels</code> argument of the constructor of the <code>UnityEnvironment</code> class.</p> <pre><code>channel = MyChannel()\n\nenv = UnityEnvironment(side_channels = [channel])\n</code></pre> <p>Note : A side channel will only send/receive messages when <code>env.step</code> or <code>env.reset()</code> is called.</p>"},{"location":"Python-LLAPI/#engineconfigurationchannel","title":"EngineConfigurationChannel","text":"<p>The <code>EngineConfiguration</code> side channel allows you to modify the time-scale, resolution, and graphics quality of the environment. This can be useful for adjusting the environment to perform better during training, or be more interpretable during inference.</p> <p><code>EngineConfigurationChannel</code> has two methods :</p> <ul> <li><code>set_configuration_parameters</code> which takes the following arguments:</li> <li><code>width</code>: Defines the width of the display. (Must be set alongside height)</li> <li><code>height</code>: Defines the height of the display. (Must be set alongside width)</li> <li><code>quality_level</code>: Defines the quality level of the simulation.</li> <li><code>time_scale</code>: Defines the multiplier for the deltatime in the simulation. If     set to a higher value, time will pass faster in the simulation but the     physics may perform unpredictably.</li> <li><code>target_frame_rate</code>: Instructs simulation to try to render at a specified     frame rate.</li> <li><code>capture_frame_rate</code> Instructs the simulation to consider time between     updates to always be constant, regardless of the actual frame rate.</li> <li><code>set_configuration</code> with argument config which is an <code>EngineConfig</code> NamedTuple   object.</li> </ul> <p>For example, the following code would adjust the time-scale of the simulation to be 2x realtime.</p> <pre><code>from mlagents_envs.environment import UnityEnvironment\nfrom mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n\nchannel = EngineConfigurationChannel()\n\nenv = UnityEnvironment(side_channels=[channel])\n\nchannel.set_configuration_parameters(time_scale = 2.0)\n\ni = env.reset()\n...\n</code></pre>"},{"location":"Python-LLAPI/#environmentparameters","title":"EnvironmentParameters","text":"<p>The <code>EnvironmentParameters</code> will allow you to get and set pre-defined numerical values in the environment. This can be useful for adjusting environment-specific settings, or for reading non-agent related information from the environment. You can call <code>get_property</code> and <code>set_property</code> on the side channel to read and write properties.</p> <p><code>EnvironmentParametersChannel</code> has one methods:</p> <ul> <li><code>set_float_parameter</code> Sets a float parameter in the Unity Environment.</li> <li>key: The string identifier of the property.</li> <li>value: The float value of the property.</li> </ul> <pre><code>from mlagents_envs.environment import UnityEnvironment\nfrom mlagents_envs.side_channel.environment_parameters_channel import EnvironmentParametersChannel\n\nchannel = EnvironmentParametersChannel()\n\nenv = UnityEnvironment(side_channels=[channel])\n\nchannel.set_float_parameter(\"parameter_1\", 2.0)\n\ni = env.reset()\n...\n</code></pre> <p>Once a property has been modified in Python, you can access it in C# after the next call to <code>step</code> as follows:</p> <pre><code>var envParameters = Academy.Instance.EnvironmentParameters;\nfloat property1 = envParameters.GetWithDefault(\"parameter_1\", 0.0f);\n</code></pre>"},{"location":"Python-LLAPI/#custom-side-channels","title":"Custom side channels","text":"<p>For information on how to make custom side channels for sending additional data types, see the documentation here.</p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/","title":"Table of Contents","text":"<ul> <li>mlagents.trainers.trainer.on_policy_trainer</li> <li>OnPolicyTrainer<ul> <li>__init__</li> <li>add_policy</li> </ul> </li> <li>mlagents.trainers.trainer.off_policy_trainer</li> <li>OffPolicyTrainer<ul> <li>__init__</li> <li>save_model</li> <li>save_replay_buffer</li> <li>load_replay_buffer</li> <li>add_policy</li> </ul> </li> <li>mlagents.trainers.trainer.rl_trainer</li> <li>RLTrainer<ul> <li>end_episode</li> <li>create_optimizer</li> <li>save_model</li> <li>advance</li> </ul> </li> <li>mlagents.trainers.trainer.trainer</li> <li>Trainer<ul> <li>__init__</li> <li>stats_reporter</li> <li>parameters</li> <li>get_max_steps</li> <li>get_step</li> <li>threaded</li> <li>should_still_train</li> <li>reward_buffer</li> <li>save_model</li> <li>end_episode</li> <li>create_policy</li> <li>add_policy</li> <li>get_policy</li> <li>advance</li> <li>publish_policy_queue</li> <li>subscribe_trajectory_queue</li> </ul> </li> <li>mlagents.trainers.settings</li> <li>deep_update_dict</li> <li>RewardSignalSettings<ul> <li>structure</li> </ul> </li> <li>ParameterRandomizationSettings<ul> <li>__str__</li> <li>structure</li> <li>unstructure</li> <li>apply</li> </ul> </li> <li>ConstantSettings<ul> <li>__str__</li> <li>apply</li> </ul> </li> <li>UniformSettings<ul> <li>__str__</li> <li>apply</li> </ul> </li> <li>GaussianSettings<ul> <li>__str__</li> <li>apply</li> </ul> </li> <li>MultiRangeUniformSettings<ul> <li>__str__</li> <li>apply</li> </ul> </li> <li>CompletionCriteriaSettings<ul> <li>need_increment</li> </ul> </li> <li>Lesson</li> <li>EnvironmentParameterSettings<ul> <li>structure</li> </ul> </li> <li>TrainerSettings<ul> <li>structure</li> </ul> </li> <li>CheckpointSettings<ul> <li>prioritize_resume_init</li> </ul> </li> <li>RunOptions<ul> <li>from_argparse</li> </ul> </li> </ul>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#mlagentstrainerstraineron_policy_trainer","title":"mlagents.trainers.trainer.on_policy_trainer","text":""},{"location":"Python-On-Off-Policy-Trainer-Documentation/#onpolicytrainer-objects","title":"OnPolicyTrainer Objects","text":"<pre><code>class OnPolicyTrainer(RLTrainer)\n</code></pre> <p>The PPOTrainer is an implementation of the PPO algorithm.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#__init__","title":"__init__","text":"<pre><code> | __init__(behavior_name: str, reward_buff_cap: int, trainer_settings: TrainerSettings, training: bool, load: bool, seed: int, artifact_path: str)\n</code></pre> <p>Responsible for collecting experiences and training an on-policy model.</p> <p>Arguments:</p> <ul> <li><code>behavior_name</code>: The name of the behavior associated with trainer config</li> <li><code>reward_buff_cap</code>: Max reward history to track in the reward buffer</li> <li><code>trainer_settings</code>: The parameters for the trainer.</li> <li><code>training</code>: Whether the trainer is set for training.</li> <li><code>load</code>: Whether the model should be loaded.</li> <li><code>seed</code>: The seed the model will be initialized with</li> <li><code>artifact_path</code>: The directory within which to store artifacts from this trainer.</li> </ul> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#add_policy","title":"add_policy","text":"<pre><code> | add_policy(parsed_behavior_id: BehaviorIdentifiers, policy: Policy) -&gt; None\n</code></pre> <p>Adds policy to trainer.</p> <p>Arguments:</p> <ul> <li><code>parsed_behavior_id</code>: Behavior identifiers that the policy should belong to.</li> <li><code>policy</code>: Policy to associate with name_behavior_id.</li> </ul> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#mlagentstrainerstraineroff_policy_trainer","title":"mlagents.trainers.trainer.off_policy_trainer","text":""},{"location":"Python-On-Off-Policy-Trainer-Documentation/#offpolicytrainer-objects","title":"OffPolicyTrainer Objects","text":"<pre><code>class OffPolicyTrainer(RLTrainer)\n</code></pre> <p>The SACTrainer is an implementation of the SAC algorithm, with support for discrete actions and recurrent networks.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#__init___1","title":"__init__","text":"<pre><code> | __init__(behavior_name: str, reward_buff_cap: int, trainer_settings: TrainerSettings, training: bool, load: bool, seed: int, artifact_path: str)\n</code></pre> <p>Responsible for collecting experiences and training an off-policy model.</p> <p>Arguments:</p> <ul> <li><code>behavior_name</code>: The name of the behavior associated with trainer config</li> <li><code>reward_buff_cap</code>: Max reward history to track in the reward buffer</li> <li><code>trainer_settings</code>: The parameters for the trainer.</li> <li><code>training</code>: Whether the trainer is set for training.</li> <li><code>load</code>: Whether the model should be loaded.</li> <li><code>seed</code>: The seed the model will be initialized with</li> <li><code>artifact_path</code>: The directory within which to store artifacts from this trainer.</li> </ul> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#save_model","title":"save_model","text":"<pre><code> | save_model() -&gt; None\n</code></pre> <p>Saves the final training model to memory Overrides the default to save the replay buffer.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#save_replay_buffer","title":"save_replay_buffer","text":"<pre><code> | save_replay_buffer() -&gt; None\n</code></pre> <p>Save the training buffer's update buffer to a pickle file.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#load_replay_buffer","title":"load_replay_buffer","text":"<pre><code> | load_replay_buffer() -&gt; None\n</code></pre> <p>Loads the last saved replay buffer from a file.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#add_policy_1","title":"add_policy","text":"<pre><code> | add_policy(parsed_behavior_id: BehaviorIdentifiers, policy: Policy) -&gt; None\n</code></pre> <p>Adds policy to trainer.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#mlagentstrainerstrainerrl_trainer","title":"mlagents.trainers.trainer.rl_trainer","text":""},{"location":"Python-On-Off-Policy-Trainer-Documentation/#rltrainer-objects","title":"RLTrainer Objects","text":"<pre><code>class RLTrainer(Trainer)\n</code></pre> <p>This class is the base class for trainers that use Reward Signals.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#end_episode","title":"end_episode","text":"<pre><code> | end_episode() -&gt; None\n</code></pre> <p>A signal that the Episode has ended. The buffer must be reset. Get only called when the academy resets.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#create_optimizer","title":"create_optimizer","text":"<pre><code> | @abc.abstractmethod\n | create_optimizer() -&gt; TorchOptimizer\n</code></pre> <p>Creates an Optimizer object</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#save_model_1","title":"save_model","text":"<pre><code> | save_model() -&gt; None\n</code></pre> <p>Saves the policy associated with this trainer.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#advance","title":"advance","text":"<pre><code> | advance() -&gt; None\n</code></pre> <p>Steps the trainer, taking in trajectories and updates if ready. Will block and wait briefly if there are no trajectories.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#mlagentstrainerstrainertrainer","title":"mlagents.trainers.trainer.trainer","text":""},{"location":"Python-On-Off-Policy-Trainer-Documentation/#trainer-objects","title":"Trainer Objects","text":"<pre><code>class Trainer(abc.ABC)\n</code></pre> <p>This class is the base class for the mlagents_envs.trainers</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#__init___2","title":"__init__","text":"<pre><code> | __init__(brain_name: str, trainer_settings: TrainerSettings, training: bool, load: bool, artifact_path: str, reward_buff_cap: int = 1)\n</code></pre> <p>Responsible for collecting experiences and training a neural network model.</p> <p>Arguments:</p> <ul> <li><code>brain_name</code>: Brain name of brain to be trained.</li> <li><code>trainer_settings</code>: The parameters for the trainer (dictionary).</li> <li><code>training</code>: Whether the trainer is set for training.</li> <li><code>artifact_path</code>: The directory within which to store artifacts from this trainer</li> <li><code>reward_buff_cap</code>:</li> </ul> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#stats_reporter","title":"stats_reporter","text":"<pre><code> | @property\n | stats_reporter()\n</code></pre> <p>Returns the stats reporter associated with this Trainer.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#parameters","title":"parameters","text":"<pre><code> | @property\n | parameters() -&gt; TrainerSettings\n</code></pre> <p>Returns the trainer parameters of the trainer.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#get_max_steps","title":"get_max_steps","text":"<pre><code> | @property\n | get_max_steps() -&gt; int\n</code></pre> <p>Returns the maximum number of steps. Is used to know when the trainer should be stopped.</p> <p>Returns:</p> <p>The maximum number of steps of the trainer</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#get_step","title":"get_step","text":"<pre><code> | @property\n | get_step() -&gt; int\n</code></pre> <p>Returns the number of steps the trainer has performed</p> <p>Returns:</p> <p>the step count of the trainer</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#threaded","title":"threaded","text":"<pre><code> | @property\n | threaded() -&gt; bool\n</code></pre> <p>Whether or not to run the trainer in a thread. True allows the trainer to update the policy while the environment is taking steps. Set to False to enforce strict on-policy updates (i.e. don't update the policy when taking steps.)</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#should_still_train","title":"should_still_train","text":"<pre><code> | @property\n | should_still_train() -&gt; bool\n</code></pre> <p>Returns whether or not the trainer should train. A Trainer could stop training if it wasn't training to begin with, or if max_steps is reached.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#reward_buffer","title":"reward_buffer","text":"<pre><code> | @property\n | reward_buffer() -&gt; Deque[float]\n</code></pre> <p>Returns the reward buffer. The reward buffer contains the cumulative rewards of the most recent episodes completed by agents using this trainer.</p> <p>Returns:</p> <p>the reward buffer.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#save_model_2","title":"save_model","text":"<pre><code> | @abc.abstractmethod\n | save_model() -&gt; None\n</code></pre> <p>Saves model file(s) for the policy or policies associated with this trainer.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#end_episode_1","title":"end_episode","text":"<pre><code> | @abc.abstractmethod\n | end_episode()\n</code></pre> <p>A signal that the Episode has ended. The buffer must be reset. Get only called when the academy resets.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#create_policy","title":"create_policy","text":"<pre><code> | @abc.abstractmethod\n | create_policy(parsed_behavior_id: BehaviorIdentifiers, behavior_spec: BehaviorSpec) -&gt; Policy\n</code></pre> <p>Creates a Policy object</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#add_policy_2","title":"add_policy","text":"<pre><code> | @abc.abstractmethod\n | add_policy(parsed_behavior_id: BehaviorIdentifiers, policy: Policy) -&gt; None\n</code></pre> <p>Adds policy to trainer.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#get_policy","title":"get_policy","text":"<pre><code> | get_policy(name_behavior_id: str) -&gt; Policy\n</code></pre> <p>Gets policy associated with name_behavior_id</p> <p>Arguments:</p> <ul> <li><code>name_behavior_id</code>: Fully qualified behavior name</li> </ul> <p>Returns:</p> <p>Policy associated with name_behavior_id</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#advance_1","title":"advance","text":"<pre><code> | @abc.abstractmethod\n | advance() -&gt; None\n</code></pre> <p>Advances the trainer. Typically, this means grabbing trajectories from all subscribed trajectory queues (self.trajectory_queues), and updating a policy using the steps in them, and if needed pushing a new policy onto the right policy queues (self.policy_queues).</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#publish_policy_queue","title":"publish_policy_queue","text":"<pre><code> | publish_policy_queue(policy_queue: AgentManagerQueue[Policy]) -&gt; None\n</code></pre> <p>Adds a policy queue to the list of queues to publish to when this Trainer makes a policy update</p> <p>Arguments:</p> <ul> <li><code>policy_queue</code>: Policy queue to publish to.</li> </ul> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#subscribe_trajectory_queue","title":"subscribe_trajectory_queue","text":"<pre><code> | subscribe_trajectory_queue(trajectory_queue: AgentManagerQueue[Trajectory]) -&gt; None\n</code></pre> <p>Adds a trajectory queue to the list of queues for the trainer to ingest Trajectories from.</p> <p>Arguments:</p> <ul> <li><code>trajectory_queue</code>: Trajectory queue to read from.</li> </ul> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#mlagentstrainerssettings","title":"mlagents.trainers.settings","text":""},{"location":"Python-On-Off-Policy-Trainer-Documentation/#deep_update_dict","title":"deep_update_dict","text":"<pre><code>deep_update_dict(d: Dict, update_d: Mapping) -&gt; None\n</code></pre> <p>Similar to dict.update(), but works for nested dicts of dicts as well.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#rewardsignalsettings-objects","title":"RewardSignalSettings Objects","text":"<pre><code>@attr.s(auto_attribs=True)\nclass RewardSignalSettings()\n</code></pre>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#structure","title":"structure","text":"<pre><code> | @staticmethod\n | structure(d: Mapping, t: type) -&gt; Any\n</code></pre> <p>Helper method to structure a Dict of RewardSignalSettings class. Meant to be registered with cattr.register_structure_hook() and called with cattr.structure(). This is needed to handle the special Enum selection of RewardSignalSettings classes.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#parameterrandomizationsettings-objects","title":"ParameterRandomizationSettings Objects","text":"<pre><code>@attr.s(auto_attribs=True)\nclass ParameterRandomizationSettings(abc.ABC)\n</code></pre>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#__str__","title":"__str__","text":"<pre><code> | __str__() -&gt; str\n</code></pre> <p>Helper method to output sampler stats to console.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#structure_1","title":"structure","text":"<pre><code> | @staticmethod\n | structure(d: Union[Mapping, float], t: type) -&gt; \"ParameterRandomizationSettings\"\n</code></pre> <p>Helper method to a ParameterRandomizationSettings class. Meant to be registered with cattr.register_structure_hook() and called with cattr.structure(). This is needed to handle the special Enum selection of ParameterRandomizationSettings classes.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#unstructure","title":"unstructure","text":"<pre><code> | @staticmethod\n | unstructure(d: \"ParameterRandomizationSettings\") -&gt; Mapping\n</code></pre> <p>Helper method to a ParameterRandomizationSettings class. Meant to be registered with cattr.register_unstructure_hook() and called with cattr.unstructure().</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#apply","title":"apply","text":"<pre><code> | @abc.abstractmethod\n | apply(key: str, env_channel: EnvironmentParametersChannel) -&gt; None\n</code></pre> <p>Helper method to send sampler settings over EnvironmentParametersChannel Calls the appropriate sampler type set method.</p> <p>Arguments:</p> <ul> <li><code>key</code>: environment parameter to be sampled</li> <li><code>env_channel</code>: The EnvironmentParametersChannel to communicate sampler settings to environment</li> </ul> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#constantsettings-objects","title":"ConstantSettings Objects","text":"<pre><code>@attr.s(auto_attribs=True)\nclass ConstantSettings(ParameterRandomizationSettings)\n</code></pre>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#__str___1","title":"__str__","text":"<pre><code> | __str__() -&gt; str\n</code></pre> <p>Helper method to output sampler stats to console.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#apply_1","title":"apply","text":"<pre><code> | apply(key: str, env_channel: EnvironmentParametersChannel) -&gt; None\n</code></pre> <p>Helper method to send sampler settings over EnvironmentParametersChannel Calls the constant sampler type set method.</p> <p>Arguments:</p> <ul> <li><code>key</code>: environment parameter to be sampled</li> <li><code>env_channel</code>: The EnvironmentParametersChannel to communicate sampler settings to environment</li> </ul> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#uniformsettings-objects","title":"UniformSettings Objects","text":"<pre><code>@attr.s(auto_attribs=True)\nclass UniformSettings(ParameterRandomizationSettings)\n</code></pre>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#__str___2","title":"__str__","text":"<pre><code> | __str__() -&gt; str\n</code></pre> <p>Helper method to output sampler stats to console.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#apply_2","title":"apply","text":"<pre><code> | apply(key: str, env_channel: EnvironmentParametersChannel) -&gt; None\n</code></pre> <p>Helper method to send sampler settings over EnvironmentParametersChannel Calls the uniform sampler type set method.</p> <p>Arguments:</p> <ul> <li><code>key</code>: environment parameter to be sampled</li> <li><code>env_channel</code>: The EnvironmentParametersChannel to communicate sampler settings to environment</li> </ul> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#gaussiansettings-objects","title":"GaussianSettings Objects","text":"<pre><code>@attr.s(auto_attribs=True)\nclass GaussianSettings(ParameterRandomizationSettings)\n</code></pre>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#__str___3","title":"__str__","text":"<pre><code> | __str__() -&gt; str\n</code></pre> <p>Helper method to output sampler stats to console.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#apply_3","title":"apply","text":"<pre><code> | apply(key: str, env_channel: EnvironmentParametersChannel) -&gt; None\n</code></pre> <p>Helper method to send sampler settings over EnvironmentParametersChannel Calls the gaussian sampler type set method.</p> <p>Arguments:</p> <ul> <li><code>key</code>: environment parameter to be sampled</li> <li><code>env_channel</code>: The EnvironmentParametersChannel to communicate sampler settings to environment</li> </ul> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#multirangeuniformsettings-objects","title":"MultiRangeUniformSettings Objects","text":"<pre><code>@attr.s(auto_attribs=True)\nclass MultiRangeUniformSettings(ParameterRandomizationSettings)\n</code></pre>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#__str___4","title":"__str__","text":"<pre><code> | __str__() -&gt; str\n</code></pre> <p>Helper method to output sampler stats to console.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#apply_4","title":"apply","text":"<pre><code> | apply(key: str, env_channel: EnvironmentParametersChannel) -&gt; None\n</code></pre> <p>Helper method to send sampler settings over EnvironmentParametersChannel Calls the multirangeuniform sampler type set method.</p> <p>Arguments:</p> <ul> <li><code>key</code>: environment parameter to be sampled</li> <li><code>env_channel</code>: The EnvironmentParametersChannel to communicate sampler settings to environment</li> </ul> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#completioncriteriasettings-objects","title":"CompletionCriteriaSettings Objects","text":"<pre><code>@attr.s(auto_attribs=True)\nclass CompletionCriteriaSettings()\n</code></pre> <p>CompletionCriteriaSettings contains the information needed to figure out if the next lesson must start.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#need_increment","title":"need_increment","text":"<pre><code> | need_increment(progress: float, reward_buffer: List[float], smoothing: float) -&gt; Tuple[bool, float]\n</code></pre> <p>Given measures, this method returns a boolean indicating if the lesson needs to change now, and a float corresponding to the new smoothed value.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#lesson-objects","title":"Lesson Objects","text":"<pre><code>@attr.s(auto_attribs=True)\nclass Lesson()\n</code></pre> <p>Gathers the data of one lesson for one environment parameter including its name, the condition that must be fullfiled for the lesson to be completed and a sampler for the environment parameter. If the completion_criteria is None, then this is the last lesson in the curriculum.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#environmentparametersettings-objects","title":"EnvironmentParameterSettings Objects","text":"<pre><code>@attr.s(auto_attribs=True)\nclass EnvironmentParameterSettings()\n</code></pre> <p>EnvironmentParameterSettings is an ordered list of lessons for one environment parameter.</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#structure_2","title":"structure","text":"<pre><code> | @staticmethod\n | structure(d: Mapping, t: type) -&gt; Dict[str, \"EnvironmentParameterSettings\"]\n</code></pre> <p>Helper method to structure a Dict of EnvironmentParameterSettings class. Meant to be registered with cattr.register_structure_hook() and called with cattr.structure().</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#trainersettings-objects","title":"TrainerSettings Objects","text":"<pre><code>@attr.s(auto_attribs=True)\nclass TrainerSettings(ExportableSettings)\n</code></pre>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#structure_3","title":"structure","text":"<pre><code> | @staticmethod\n | structure(d: Mapping, t: type) -&gt; Any\n</code></pre> <p>Helper method to structure a TrainerSettings class. Meant to be registered with cattr.register_structure_hook() and called with cattr.structure().</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#checkpointsettings-objects","title":"CheckpointSettings Objects","text":"<pre><code>@attr.s(auto_attribs=True)\nclass CheckpointSettings()\n</code></pre>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#prioritize_resume_init","title":"prioritize_resume_init","text":"<pre><code> | prioritize_resume_init() -&gt; None\n</code></pre> <p>Prioritize explicit command line resume/init over conflicting yaml options. if both resume/init are set at one place use resume</p> <p></p>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#runoptions-objects","title":"RunOptions Objects","text":"<pre><code>@attr.s(auto_attribs=True)\nclass RunOptions(ExportableSettings)\n</code></pre>"},{"location":"Python-On-Off-Policy-Trainer-Documentation/#from_argparse","title":"from_argparse","text":"<pre><code> | @staticmethod\n | from_argparse(args: argparse.Namespace) -&gt; \"RunOptions\"\n</code></pre> <p>Takes an argparse.Namespace as specified in <code>parse_command_line</code>, loads input configuration files from file paths, and converts to a RunOptions instance.</p> <p>Arguments:</p> <ul> <li><code>args</code>: collection of command-line parameters passed to mlagents-learn</li> </ul> <p>Returns:</p> <p>RunOptions representing the passed in arguments, with trainer config, curriculum and sampler configs loaded from files.</p>"},{"location":"Python-Optimizer-Documentation/","title":"Table of Contents","text":"<ul> <li>mlagents.trainers.optimizer.torch_optimizer</li> <li>TorchOptimizer<ul> <li>create_reward_signals</li> <li>get_trajectory_value_estimates</li> </ul> </li> <li>mlagents.trainers.optimizer.optimizer</li> <li>Optimizer<ul> <li>update</li> </ul> </li> </ul>"},{"location":"Python-Optimizer-Documentation/#mlagentstrainersoptimizertorch_optimizer","title":"mlagents.trainers.optimizer.torch_optimizer","text":""},{"location":"Python-Optimizer-Documentation/#torchoptimizer-objects","title":"TorchOptimizer Objects","text":"<pre><code>class TorchOptimizer(Optimizer)\n</code></pre>"},{"location":"Python-Optimizer-Documentation/#create_reward_signals","title":"create_reward_signals","text":"<pre><code> | create_reward_signals(reward_signal_configs: Dict[RewardSignalType, RewardSignalSettings]) -&gt; None\n</code></pre> <p>Create reward signals</p> <p>Arguments:</p> <ul> <li><code>reward_signal_configs</code>: Reward signal config.</li> </ul> <p></p>"},{"location":"Python-Optimizer-Documentation/#get_trajectory_value_estimates","title":"get_trajectory_value_estimates","text":"<pre><code> | get_trajectory_value_estimates(batch: AgentBuffer, next_obs: List[np.ndarray], done: bool, agent_id: str = \"\") -&gt; Tuple[Dict[str, np.ndarray], Dict[str, float], Optional[AgentBufferField]]\n</code></pre> <p>Get value estimates and memories for a trajectory, in batch form.</p> <p>Arguments:</p> <ul> <li><code>batch</code>: An AgentBuffer that consists of a trajectory.</li> <li><code>next_obs</code>: the next observation (after the trajectory). Used for boostrapping     if this is not a termiinal trajectory.</li> <li><code>done</code>: Set true if this is a terminal trajectory.</li> <li><code>agent_id</code>: Agent ID of the agent that this trajectory belongs to.</li> </ul> <p>Returns:</p> <p>A Tuple of the Value Estimates as a Dict of [name, np.ndarray(trajectory_len)],     the final value estimate as a Dict of [name, float], and optionally (if using memories)     an AgentBufferField of initial critic memories to be used during update.</p> <p></p>"},{"location":"Python-Optimizer-Documentation/#mlagentstrainersoptimizeroptimizer","title":"mlagents.trainers.optimizer.optimizer","text":""},{"location":"Python-Optimizer-Documentation/#optimizer-objects","title":"Optimizer Objects","text":"<pre><code>class Optimizer(abc.ABC)\n</code></pre> <p>Creates loss functions and auxillary networks (e.g. Q or Value) needed for training. Provides methods to update the Policy.</p> <p></p>"},{"location":"Python-Optimizer-Documentation/#update","title":"update","text":"<pre><code> | @abc.abstractmethod\n | update(batch: AgentBuffer, num_sequences: int) -&gt; Dict[str, float]\n</code></pre> <p>Update the Policy based on the batch that was passed in.</p> <p>Arguments:</p> <ul> <li><code>batch</code>: AgentBuffer that contains the minibatch of data used for this update.</li> <li><code>num_sequences</code>: Number of recurrent sequences found in the minibatch.</li> </ul> <p>Returns:</p> <p>A Dict containing statistics (name, value) from the update (e.g. loss)</p>"},{"location":"Python-PettingZoo-API-Documentation/","title":"Table of Contents","text":"<ul> <li>mlagents_envs.envs.pettingzoo_env_factory</li> <li>PettingZooEnvFactory<ul> <li>env</li> </ul> </li> <li>mlagents_envs.envs.unity_aec_env</li> <li>UnityAECEnv<ul> <li>__init__</li> <li>step</li> <li>observe</li> <li>last</li> </ul> </li> <li>mlagents_envs.envs.unity_parallel_env</li> <li>UnityParallelEnv<ul> <li>__init__</li> <li>reset</li> </ul> </li> <li>mlagents_envs.envs.unity_pettingzoo_base_env</li> <li>UnityPettingzooBaseEnv<ul> <li>observation_spaces</li> <li>observation_space</li> <li>action_spaces</li> <li>action_space</li> <li>side_channel</li> <li>reset</li> <li>seed</li> <li>render</li> <li>close</li> </ul> </li> </ul>"},{"location":"Python-PettingZoo-API-Documentation/#mlagents_envsenvspettingzoo_env_factory","title":"mlagents_envs.envs.pettingzoo_env_factory","text":""},{"location":"Python-PettingZoo-API-Documentation/#pettingzooenvfactory-objects","title":"PettingZooEnvFactory Objects","text":"<pre><code>class PettingZooEnvFactory()\n</code></pre>"},{"location":"Python-PettingZoo-API-Documentation/#env","title":"env","text":"<pre><code> | env(seed: Optional[int] = None, **kwargs: Union[List, int, bool, None]) -&gt; UnityAECEnv\n</code></pre> <p>Creates the environment with env_id from unity's default_registry and wraps it in a UnityToPettingZooWrapper</p> <p>Arguments:</p> <ul> <li><code>seed</code>: The seed for the action spaces of the agents.</li> <li><code>kwargs</code>: Any argument accepted by <code>UnityEnvironment</code>class except file_name</li> </ul> <p></p>"},{"location":"Python-PettingZoo-API-Documentation/#mlagents_envsenvsunity_aec_env","title":"mlagents_envs.envs.unity_aec_env","text":""},{"location":"Python-PettingZoo-API-Documentation/#unityaecenv-objects","title":"UnityAECEnv Objects","text":"<pre><code>class UnityAECEnv(UnityPettingzooBaseEnv,  AECEnv)\n</code></pre> <p>Unity AEC (PettingZoo) environment wrapper.</p> <p></p>"},{"location":"Python-PettingZoo-API-Documentation/#__init__","title":"__init__","text":"<pre><code> | __init__(env: BaseEnv, seed: Optional[int] = None)\n</code></pre> <p>Initializes a Unity AEC environment wrapper.</p> <p>Arguments:</p> <ul> <li><code>env</code>: The UnityEnvironment that is being wrapped.</li> <li><code>seed</code>: The seed for the action spaces of the agents.</li> </ul> <p></p>"},{"location":"Python-PettingZoo-API-Documentation/#step","title":"step","text":"<pre><code> | step(action: Any) -&gt; None\n</code></pre> <p>Sets the action of the active agent and get the observation, reward, done and info of the next agent.</p> <p>Arguments:</p> <ul> <li><code>action</code>: The action for the active agent</li> </ul> <p></p>"},{"location":"Python-PettingZoo-API-Documentation/#observe","title":"observe","text":"<pre><code> | observe(agent_id)\n</code></pre> <p>Returns the observation an agent currently can make. <code>last()</code> calls this function.</p> <p></p>"},{"location":"Python-PettingZoo-API-Documentation/#last","title":"last","text":"<pre><code> | last(observe=True)\n</code></pre> <p>returns observation, cumulative reward, done, info for the current agent (specified by self.agent_selection)</p> <p></p>"},{"location":"Python-PettingZoo-API-Documentation/#mlagents_envsenvsunity_parallel_env","title":"mlagents_envs.envs.unity_parallel_env","text":""},{"location":"Python-PettingZoo-API-Documentation/#unityparallelenv-objects","title":"UnityParallelEnv Objects","text":"<pre><code>class UnityParallelEnv(UnityPettingzooBaseEnv,  ParallelEnv)\n</code></pre> <p>Unity Parallel (PettingZoo) environment wrapper.</p> <p></p>"},{"location":"Python-PettingZoo-API-Documentation/#__init___1","title":"__init__","text":"<pre><code> | __init__(env: BaseEnv, seed: Optional[int] = None)\n</code></pre> <p>Initializes a Unity Parallel environment wrapper.</p> <p>Arguments:</p> <ul> <li><code>env</code>: The UnityEnvironment that is being wrapped.</li> <li><code>seed</code>: The seed for the action spaces of the agents.</li> </ul> <p></p>"},{"location":"Python-PettingZoo-API-Documentation/#reset","title":"reset","text":"<pre><code> | reset() -&gt; Dict[str, Any]\n</code></pre> <p>Resets the environment.</p> <p></p>"},{"location":"Python-PettingZoo-API-Documentation/#mlagents_envsenvsunity_pettingzoo_base_env","title":"mlagents_envs.envs.unity_pettingzoo_base_env","text":""},{"location":"Python-PettingZoo-API-Documentation/#unitypettingzoobaseenv-objects","title":"UnityPettingzooBaseEnv Objects","text":"<pre><code>class UnityPettingzooBaseEnv()\n</code></pre> <p>Unity Petting Zoo base environment.</p> <p></p>"},{"location":"Python-PettingZoo-API-Documentation/#observation_spaces","title":"observation_spaces","text":"<pre><code> | @property\n | observation_spaces() -&gt; Dict[str, spaces.Space]\n</code></pre> <p>Return the observation spaces of all the agents.</p> <p></p>"},{"location":"Python-PettingZoo-API-Documentation/#observation_space","title":"observation_space","text":"<pre><code> | observation_space(agent: str) -&gt; Optional[spaces.Space]\n</code></pre> <p>The observation space of the current agent.</p> <p></p>"},{"location":"Python-PettingZoo-API-Documentation/#action_spaces","title":"action_spaces","text":"<pre><code> | @property\n | action_spaces() -&gt; Dict[str, spaces.Space]\n</code></pre> <p>Return the action spaces of all the agents.</p> <p></p>"},{"location":"Python-PettingZoo-API-Documentation/#action_space","title":"action_space","text":"<pre><code> | action_space(agent: str) -&gt; Optional[spaces.Space]\n</code></pre> <p>The action space of the current agent.</p> <p></p>"},{"location":"Python-PettingZoo-API-Documentation/#side_channel","title":"side_channel","text":"<pre><code> | @property\n | side_channel() -&gt; Dict[str, Any]\n</code></pre> <p>The side channels of the environment. You can access the side channels of an environment with <code>env.side_channel[&lt;name-of-channel&gt;]</code>.</p> <p></p>"},{"location":"Python-PettingZoo-API-Documentation/#reset_1","title":"reset","text":"<pre><code> | reset()\n</code></pre> <p>Resets the environment.</p> <p></p>"},{"location":"Python-PettingZoo-API-Documentation/#seed","title":"seed","text":"<pre><code> | seed(seed=None)\n</code></pre> <p>Reseeds the environment (making the resulting environment deterministic). <code>reset()</code> must be called after <code>seed()</code>, and before <code>step()</code>.</p> <p></p>"},{"location":"Python-PettingZoo-API-Documentation/#render","title":"render","text":"<pre><code> | render(mode=\"human\")\n</code></pre> <p>NOT SUPPORTED.</p> <p>Displays a rendered frame from the environment, if supported. Alternate render modes in the default environments are <code>'rgb_array'</code> which returns a numpy array and is supported by all environments outside of classic, and <code>'ansi'</code> which returns the strings printed (specific to classic environments).</p> <p></p>"},{"location":"Python-PettingZoo-API-Documentation/#close","title":"close","text":"<pre><code> | close() -&gt; None\n</code></pre> <p>Close the environment.</p>"},{"location":"Python-PettingZoo-API/","title":"Unity ML-Agents PettingZoo Wrapper","text":"<p>With the increasing interest in multi-agent training with a gym-like API, we provide a PettingZoo Wrapper around the Petting Zoo API. Our wrapper provides interfaces on top of our <code>UnityEnvironment</code> class, which is the default way of interfacing with a Unity environment via Python.</p>"},{"location":"Python-PettingZoo-API/#installation-and-examples","title":"Installation and Examples","text":"<p>The PettingZoo wrapper is part of the <code>mlgents_envs</code> package. Please refer to the mlagents_envs installation instructions.</p> <p>[Colab] PettingZoo Wrapper Example</p> <p>This colab notebook demonstrates the example usage of the wrapper, including installation, basic usages, and an example with our Striker vs Goalie environment which is a multi-agents environment with multiple different behavior names.</p>"},{"location":"Python-PettingZoo-API/#api-interface","title":"API interface","text":"<p>This wrapper is compatible with PettingZoo API. Please check out PettingZoo API page for more details. Here's an example of interacting with wrapped environment:</p> <pre><code>from mlagents_envs.environment import UnityEnvironment\nfrom mlagents_envs.envs import UnityToPettingZooWrapper\n\nunity_env = UnityEnvironment(\"StrikersVsGoalie\")\nenv = UnityToPettingZooWrapper(unity_env)\nenv.reset()\nfor agent in env.agent_iter():\n    observation, reward, done, info = env.last()\n    action = policy(observation, agent)\n    env.step(action)\n</code></pre>"},{"location":"Python-PettingZoo-API/#notes","title":"Notes","text":"<ul> <li>There is support for both AEC   and Parallel PettingZoo APIs.</li> <li>The AEC wrapper is compatible with PettingZoo (PZ) API interface but works in a slightly   different way under the hood. For the AEC API, Instead of stepping the environment in every <code>env.step(action)</code>,   the PZ wrapper will store the action, and will only perform environment stepping when all the   agents requesting for actions in the current step have been assigned an action. This is for   performance, considering that the communication between Unity and python is more efficient   when data are sent in batches.</li> <li>Since the actions for the AEC wrapper are stored without applying them to the environment until   all the actions are queued, some components of the API might behave in unexpected way. For example, a call   to <code>env.reward</code> should return the instantaneous reward for that particular step, but the true   reward would only be available when an actual environment step is performed. It's recommended that   you follow the API definition for training (access rewards from <code>env.last()</code> instead of   <code>env.reward</code>) and the underlying mechanism shouldn't affect training results.</li> <li>The environments will automatically reset when it's done, so <code>env.agent_iter(max_step)</code> will   keep going on until the specified max step is reached (default: <code>2**63</code>). There is no need to   call <code>env.reset()</code> except for the very beginning of instantiating an environment.</li> </ul>"},{"location":"Readme/","title":"Unity ML-Agents Toolkit","text":"<p>(latest release) (all releases)</p> <p>The Unity Machine Learning Agents Toolkit (ML-Agents) is an open-source project that enables games and simulations to serve as environments for training intelligent agents. We provide implementations (based on PyTorch) of state-of-the-art algorithms to enable game developers and hobbyists to easily train intelligent agents for 2D, 3D and VR/AR games. Researchers can also use the provided simple-to-use Python API to train Agents using reinforcement learning, imitation learning, neuroevolution, or any other methods. These trained agents can be used for multiple purposes, including controlling NPC behavior (in a variety of settings such as multi-agent and adversarial), automated testing of game builds and evaluating different game design decisions pre-release. The ML-Agents Toolkit is mutually beneficial for both game developers and AI researchers as it provides a central platform where advances in AI can be evaluated on Unity\u2019s rich environments and then made accessible to the wider research and game developer communities.</p>"},{"location":"Readme/#features","title":"Features","text":"<ul> <li>17+ example Unity environments</li> <li>Support for multiple environment configurations and training scenarios</li> <li>Flexible Unity SDK that can be integrated into your game or custom Unity scene</li> <li>Support for training single-agent, multi-agent cooperative, and multi-agent   competitive scenarios via several Deep Reinforcement Learning algorithms (PPO, SAC, MA-POCA, self-play).</li> <li>Support for learning from demonstrations through two Imitation Learning algorithms (BC and GAIL).</li> <li>Quickly and easily add your own custom training algorithm and/or components.</li> <li>Easily definable Curriculum Learning scenarios for complex tasks</li> <li>Train robust agents using environment randomization</li> <li>Flexible agent control with On Demand Decision Making</li> <li>Train using multiple concurrent Unity environment instances</li> <li>Utilizes the Sentis to   provide native cross-platform support</li> <li>Unity environment control from Python</li> <li>Wrap Unity learning environments as a gym environment</li> <li>Wrap Unity learning environments as a PettingZoo environment</li> </ul> <p>See our ML-Agents Overview page for detailed descriptions of all these features. Or go straight to our web docs.</p>"},{"location":"Readme/#releases-documentation","title":"Releases &amp; Documentation","text":"<p>Our latest, stable release is <code>Release 21</code>. Click here to get started with the latest release of ML-Agents.</p> <p>You can also check out our new web docs!</p> <p>The table below lists all our releases, including our <code>main</code> branch which is under active development and may be unstable. A few helpful guidelines: - The Versioning page overviews how we manage our GitHub   releases and the versioning process for each of the ML-Agents components. - The Releases page   contains details of the changes between releases. - The Migration page contains details on how to upgrade   from earlier releases of the ML-Agents Toolkit. - The Documentation links in the table below include installation and usage   instructions specific to each release. Remember to always use the   documentation that corresponds to the release version you're using. - The <code>com.unity.ml-agents</code> package is verified   for Unity 2020.1 and later. Verified packages releases are numbered 1.0.x.</p> Version Release Date Source Documentation Download Python Package Unity Package develop (unstable) -- source docs download -- -- Release 21 October 9, 2023 source docs download 1.0.0 3.0.0 <p>If you are a researcher interested in a discussion of Unity as an AI platform, see a pre-print of our reference paper on Unity and the ML-Agents Toolkit.</p> <p>If you use Unity or the ML-Agents Toolkit to conduct research, we ask that you cite the following paper as a reference:</p> <pre><code>@article{juliani2020,\n  title={Unity: A general platform for intelligent agents},\n  author={Juliani, Arthur and Berges, Vincent-Pierre and Teng, Ervin and Cohen, Andrew and Harper, Jonathan and Elion, Chris and Goy, Chris and Gao, Yuan and Henry, Hunter and Mattar, Marwan and Lange, Danny},\n  journal={arXiv preprint arXiv:1809.02627},\n  url={https://arxiv.org/pdf/1809.02627.pdf},\n  year={2020}\n}\n</code></pre> <p>Additionally, if you use the MA-POCA trainer in your research, we ask that you cite the following paper as a reference:</p> <pre><code>@article{cohen2022,\n  title={On the Use and Misuse of Absorbing States in Multi-agent Reinforcement Learning},\n  author={Cohen, Andrew and Teng, Ervin and Berges, Vincent-Pierre and Dong, Ruo-Ping and Henry, Hunter and Mattar, Marwan and Zook, Alexander and Ganguly, Sujoy},\n  journal={RL in Games Workshop AAAI 2022},\n  url={http://aaai-rlg.mlanctot.info/papers/AAAI22-RLG_paper_32.pdf},\n  year={2022}\n}\n</code></pre>"},{"location":"Readme/#additional-resources","title":"Additional Resources","text":"<p>We have a Unity Learn course, ML-Agents: Hummingbirds, that provides a gentle introduction to Unity and the ML-Agents Toolkit.</p> <p>We've also partnered with CodeMonkeyUnity to create a series of tutorial videos on how to implement and use the ML-Agents Toolkit.</p> <p>We have also published a series of blog posts that are relevant for ML-Agents:</p> <ul> <li>(July 12, 2021)   ML-Agents plays Dodgeball</li> <li>(May 5, 2021)   ML-Agents v2.0 release: Now supports training complex cooperative behaviors</li> <li>(December 28, 2020)   Happy holidays from the Unity ML-Agents team!</li> <li>(November 20, 2020)   How Eidos-Montr\u00e9al created Grid Sensors to improve observations for training agents</li> <li>(November 11, 2020)   2020 AI@Unity interns shoutout</li> <li>(May 12, 2020)   Announcing ML-Agents Unity Package v1.0!</li> <li>(February 28, 2020)   Training intelligent adversaries using self-play with ML-Agents</li> <li>(November 11, 2019)   Training your agents 7 times faster with ML-Agents</li> <li>(October 21, 2019)   The AI@Unity interns help shape the world</li> <li>(April 15, 2019)   Unity ML-Agents Toolkit v0.8: Faster training on real games</li> <li>(March 1, 2019)   Unity ML-Agents Toolkit v0.7: A leap towards cross-platform inference</li> <li>(December 17, 2018)   ML-Agents Toolkit v0.6: Improved usability of Brains and Imitation Learning</li> <li>(October 2, 2018)   Puppo, The Corgi: Cuteness Overload with the Unity ML-Agents Toolkit</li> <li>(September 11, 2018)   ML-Agents Toolkit v0.5, new resources for AI researchers available now</li> <li>(June 26, 2018)   Solving sparse-reward tasks with Curiosity</li> <li>(June 19, 2018)   Unity ML-Agents Toolkit v0.4 and Udacity Deep Reinforcement Learning Nanodegree</li> <li>(May 24, 2018)   Imitation Learning in Unity: The Workflow</li> <li>(March 15, 2018)   ML-Agents Toolkit v0.3 Beta released: Imitation Learning, feedback-driven features, and more</li> <li>(December 11, 2017)   Using Machine Learning Agents in a real game: a beginner\u2019s guide</li> <li>(December 8, 2017)   Introducing ML-Agents Toolkit v0.2: Curriculum Learning, new environments, and more</li> <li>(September 19, 2017)   Introducing: Unity Machine Learning Agents Toolkit</li> <li>Overviewing reinforcement learning concepts   (multi-armed bandit   and   Q-learning)</li> </ul>"},{"location":"Readme/#more-from-unity","title":"More from Unity","text":"<ul> <li>Unity Sentis</li> <li>Introductin Unity Muse and Sentis</li> </ul>"},{"location":"Readme/#community-and-feedback","title":"Community and Feedback","text":"<p>The ML-Agents Toolkit is an open-source project and we encourage and welcome contributions. If you wish to contribute, be sure to review our contribution guidelines and code of conduct.</p> <p>For problems with the installation and setup of the ML-Agents Toolkit, or discussions about how to best setup or train your agents, please create a new thread on the Unity ML-Agents forum and make sure to include as much detail as possible. If you run into any other problems using the ML-Agents Toolkit or have a specific feature request, please submit a GitHub issue.</p> <p>Please tell us which samples you would like to see shipped with the ML-Agents Unity package by replying to this forum thread.</p> <p>Your opinion matters a great deal to us. Only by hearing your thoughts on the Unity ML-Agents Toolkit can we continue to improve and grow. Please take a few minutes to let us know about it.</p> <p>For any other questions or feedback, connect directly with the ML-Agents team at ml-agents@unity3d.com.</p>"},{"location":"Readme/#privacy","title":"Privacy","text":"<p>In order to improve the developer experience for Unity ML-Agents Toolkit, we have added in-editor analytics. Please refer to \"Information that is passively collected by Unity\" in the Unity Privacy Policy.</p>"},{"location":"Sentis/","title":"Sentis","text":"<p>The ML-Agents Toolkit allows you to use pre-trained neural network models inside your Unity games. This support is possible thanks to the Sentis (codenamed Sentis). Sentis uses compute shaders to run the neural network within Unity.</p>"},{"location":"Sentis/#supported-devices","title":"Supported devices","text":"<p>See the Sentis documentation for a list of the supported platforms.</p> <p>Scripting Backends : Sentis is generally faster with IL2CPP than with Mono for Standalone builds. In the Editor, It is not possible to use Sentis with GPU device selected when Editor Graphics Emulation is set to OpenGL(ES) 3.0 or 2.0 emulation. Also there might be non-fatal build time errors when target platform includes Graphics API that does not support Unity Compute Shaders.</p>"},{"location":"Sentis/#using-sentis","title":"Using Sentis","text":"<p>When using a model, drag the model file into the Model field in the Inspector of the Agent. Select the Inference Device : CPU or GPU you want to use for Inference.</p> <p>Note: For most of the models generated with the ML-Agents Toolkit, CPU will be faster than GPU. You should use the GPU only if you use the ResNet visual encoder or have a large number of agents with visual observations.</p>"},{"location":"Sentis/#unsupported-use-cases","title":"Unsupported use cases","text":""},{"location":"Sentis/#externally-trained-models","title":"Externally trained models","text":"<p>The ML-Agents Toolkit only supports the models created with our trainers. Model loading expects certain conventions for constants and tensor names. While it is possible to construct a model that follows these conventions, we don't provide any additional help for this. More details can be found in TensorNames.cs and SentisModelParamLoader.cs.</p> <p>If you wish to run inference on an externally trained model, you should use Sentis directly, instead of trying to run it through ML-Agents.</p>"},{"location":"Sentis/#model-inference-outside-of-unity","title":"Model inference outside of Unity","text":"<p>We do not provide support for inference anywhere outside of Unity. The <code>.onnx</code> files produced by training use the open format ONNX; if you wish to convert a <code>.onnx</code> file to another format or run inference with them, refer to their documentation.</p>"},{"location":"Training-Configuration-File/","title":"Training Configuration File","text":"<p>Table of Contents</p> <ul> <li>Common Trainer Configurations</li> <li>Trainer-specific Configurations</li> <li>PPO-specific Configurations</li> <li>SAC-specific Configurations</li> <li>Reward Signals</li> <li>Extrinsic Rewards</li> <li>Curiosity Intrinsic Reward</li> <li>GAIL Intrinsic Reward</li> <li>RND Intrinsic Reward</li> <li>Reward Signal Settings for SAC</li> <li>Behavioral Cloning</li> <li>Memory-enhanced Agents using Recurrent Neural Networks</li> <li>Self-Play</li> <li>Note on Reward Signals</li> <li>Note on Swap Steps</li> </ul>"},{"location":"Training-Configuration-File/#common-trainer-configurations","title":"Common Trainer Configurations","text":"<p>One of the first decisions you need to make regarding your training run is which trainer to use: PPO, SAC, or POCA. There are some training configurations that are common to both trainers (which we review now) and others that depend on the choice of the trainer (which we review on subsequent sections).</p> Setting Description <code>trainer_type</code> (default = <code>ppo</code>) The type of trainer to use: <code>ppo</code>,  <code>sac</code>, or <code>poca</code>. <code>summary_freq</code> (default = <code>50000</code>) Number of experiences that needs to be collected before generating and displaying training statistics. This determines the granularity of the graphs in Tensorboard. <code>time_horizon</code> (default = <code>64</code>) How many steps of experience to collect per-agent before adding it to the experience buffer. When this limit is reached before the end of an episode, a value estimate is used to predict the overall expected reward from the agent's current state. As such, this parameter trades off between a less biased, but higher variance estimate (long time horizon) and more biased, but less varied estimate (short time horizon). In cases where there are frequent rewards within an episode, or episodes are prohibitively large, a smaller number can be more ideal. This number should be large enough to capture all the important behavior within a sequence of an agent's actions.  Typical range: <code>32</code> - <code>2048</code> <code>max_steps</code> (default = <code>500000</code>) Total number of steps (i.e., observation collected and action taken) that must be taken in the environment (or across all environments if using multiple in parallel) before ending the training process. If you have multiple agents with the same behavior name within your environment, all steps taken by those agents will contribute to the same <code>max_steps</code> count. Typical range: <code>5e5</code> - <code>1e7</code> <code>keep_checkpoints</code> (default = <code>5</code>) The maximum number of model checkpoints to keep. Checkpoints are saved after the number of steps specified by the checkpoint_interval option. Once the maximum number of checkpoints has been reached, the oldest checkpoint is deleted when saving a new checkpoint. <code>even_checkpoints</code> (default = <code>false</code>) If set to true, ignores <code>checkpoint_interval</code> and evenly distributes checkpoints throughout training based on <code>keep_checkpoints</code>and <code>max_steps</code>, i.e. <code>checkpoint_interval = max_steps / keep_checkpoints</code>. Useful for cataloging agent behavior throughout training. <code>checkpoint_interval</code> (default = <code>500000</code>) The number of experiences collected between each checkpoint by the trainer. A maximum of <code>keep_checkpoints</code> checkpoints are saved before old ones are deleted. Each checkpoint saves the <code>.onnx</code> files in <code>results/</code> folder. <code>init_path</code> (default = None) Initialize trainer from a previously saved model. Note that the prior run should have used the same trainer configurations as the current run, and have been saved with the same version of ML-Agents. You can provide either the file name or the full path to the checkpoint, e.g. <code>{checkpoint_name.pt}</code> or <code>./models/{run-id}/{behavior_name}/{checkpoint_name.pt}</code>. This option is provided in case you want to initialize different behaviors from different runs or initialize from an older checkpoint; in most cases, it is sufficient to use the <code>--initialize-from</code> CLI parameter to initialize all models from the same run. <code>threaded</code> (default = <code>false</code>) Allow environments to step while updating the model. This might result in a training speedup, especially when using SAC. For best performance, leave setting to <code>false</code> when using self-play. <code>hyperparameters -&gt; learning_rate</code> (default = <code>3e-4</code>) Initial learning rate for gradient descent. Corresponds to the strength of each gradient descent update step. This should typically be decreased if training is unstable, and the reward does not consistently increase. Typical range: <code>1e-5</code> - <code>1e-3</code> <code>hyperparameters -&gt; batch_size</code> Number of experiences in each iteration of gradient descent. This should always be multiple times smaller than <code>buffer_size</code>. If you are using continuous actions, this value should be large (on the order of 1000s). If you are using only discrete actions, this value should be smaller (on the order of 10s).  Typical range: (Continuous - PPO): <code>512</code> - <code>5120</code>; (Continuous - SAC): <code>128</code> - <code>1024</code>; (Discrete, PPO &amp; SAC): <code>32</code> - <code>512</code>. <code>hyperparameters -&gt; buffer_size</code> (default = <code>10240</code> for PPO and <code>50000</code> for SAC) PPO: Number of experiences to collect before updating the policy model. Corresponds to how many experiences should be collected before we do any learning or updating of the model. This should be multiple times larger than <code>batch_size</code>. Typically a larger <code>buffer_size</code> corresponds to more stable training updates.  SAC: The max size of the experience buffer - on the order of thousands of times longer than your episodes, so that SAC can learn from old as well as new experiences. Typical range: PPO: <code>2048</code> - <code>409600</code>; SAC: <code>50000</code> - <code>1000000</code> <code>hyperparameters -&gt; learning_rate_schedule</code> (default = <code>linear</code> for PPO and <code>constant</code> for SAC) Determines how learning rate changes over time. For PPO, we recommend decaying learning rate until max_steps so learning converges more stably. However, for some cases (e.g. training for an unknown amount of time) this feature can be disabled. For SAC, we recommend holding learning rate constant so that the agent can continue to learn until its Q function converges naturally. <code>linear</code> decays the learning_rate linearly, reaching 0 at max_steps, while <code>constant</code> keeps the learning rate constant for the entire training run. <code>network_settings -&gt; hidden_units</code> (default = <code>128</code>) Number of units in the hidden layers of the neural network. Correspond to how many units are in each fully connected layer of the neural network. For simple problems where the correct action is a straightforward combination of the observation inputs, this should be small. For problems where the action is a very complex interaction between the observation variables, this should be larger.  Typical range: <code>32</code> - <code>512</code> <code>network_settings -&gt; num_layers</code> (default = <code>2</code>) The number of hidden layers in the neural network. Corresponds to how many hidden layers are present after the observation input, or after the CNN encoding of the visual observation. For simple problems, fewer layers are likely to train faster and more efficiently. More layers may be necessary for more complex control problems.  Typical range: <code>1</code> - <code>3</code> <code>network_settings -&gt; normalize</code> (default = <code>false</code>) Whether normalization is applied to the vector observation inputs. This normalization is based on the running average and variance of the vector observation. Normalization can be helpful in cases with complex continuous control problems, but may be harmful with simpler discrete control problems. <code>network_settings -&gt; vis_encode_type</code> (default = <code>simple</code>) Encoder type for encoding visual observations.  <code>simple</code> (default) uses a simple encoder which consists of two convolutional layers, <code>nature_cnn</code> uses the CNN implementation proposed by Mnih et al., consisting of three convolutional layers, and <code>resnet</code> uses the IMPALA Resnet consisting of three stacked layers, each with two residual blocks, making a much larger network than the other two. <code>match3</code> is a smaller CNN (Gudmundsoon et al.) that can capture more granular spatial relationships and is optimized for board games. <code>fully_connected</code> uses a single fully connected dense layer as encoder without any convolutional layers.  Due to the size of convolution kernel, there is a minimum observation size limitation that each encoder type can handle - <code>simple</code>: 20x20, <code>nature_cnn</code>: 36x36, <code>resnet</code>: 15 x 15, <code>match3</code>: 5x5.  <code>fully_connected</code> doesn't have convolutional layers and thus no size limits, but since it has less representation power it should be reserved for very small inputs. Note that using the <code>match3</code> CNN with very large visual input might result in a huge observation encoding and thus potentially slow down training or cause memory issues. <code>network_settings -&gt; conditioning_type</code> (default = <code>hyper</code>) Conditioning type for the policy using goal observations.  <code>none</code> treats the goal observations as regular observations, <code>hyper</code> (default) uses a HyperNetwork with goal observations as input to generate some of the weights of the policy. Note that when using <code>hyper</code> the number of parameters of the network increases greatly. Therefore, it is recommended to reduce the number of <code>hidden_units</code> when using this <code>conditioning_type</code>"},{"location":"Training-Configuration-File/#trainer-specific-configurations","title":"Trainer-specific Configurations","text":"<p>Depending on your choice of a trainer, there are additional trainer-specific configurations. We present them below in two separate tables, but keep in mind that you only need to include the configurations for the trainer selected (i.e. the <code>trainer</code> setting above).</p>"},{"location":"Training-Configuration-File/#ppo-specific-configurations","title":"PPO-specific Configurations","text":"Setting Description <code>hyperparameters -&gt; beta</code> (default = <code>5.0e-3</code>) Strength of the entropy regularization, which makes the policy \"more random.\" This ensures that agents properly explore the action space during training. Increasing this will ensure more random actions are taken. This should be adjusted such that the entropy (measurable from TensorBoard) slowly decreases alongside increases in reward. If entropy drops too quickly, increase beta. If entropy drops too slowly, decrease <code>beta</code>. Typical range: <code>1e-4</code> - <code>1e-2</code> <code>hyperparameters -&gt; epsilon</code> (default = <code>0.2</code>) Influences how rapidly the policy can evolve during training. Corresponds to the acceptable threshold of divergence between the old and new policies during gradient descent updating. Setting this value small will result in more stable updates, but will also slow the training process. Typical range: <code>0.1</code> - <code>0.3</code> <code>hyperparameters -&gt; beta_schedule</code> (default = <code>learning_rate_schedule</code>) Determines how beta changes over time. <code>linear</code> decays beta linearly, reaching 0 at max_steps, while <code>constant</code> keeps beta constant for the entire training run. If not explicitly set, the default beta schedule will be set to <code>hyperparameters -&gt; learning_rate_schedule</code>. <code>hyperparameters -&gt; epsilon_schedule</code> (default = <code>learning_rate_schedule</code>) Determines how epsilon changes over time (PPO only). <code>linear</code> decays epsilon linearly, reaching 0 at max_steps, while <code>constant</code> keeps the epsilon constant for the entire training run. If not explicitly set, the default epsilon schedule will be set to <code>hyperparameters -&gt; learning_rate_schedule</code>. <code>hyperparameters -&gt; lambd</code> (default = <code>0.95</code>) Regularization parameter (lambda) used when calculating the Generalized Advantage Estimate (GAE). This can be thought of as how much the agent relies on its current value estimate when calculating an updated value estimate. Low values correspond to relying more on the current value estimate (which can be high bias), and high values correspond to relying more on the actual rewards received in the environment (which can be high variance). The parameter provides a trade-off between the two, and the right value can lead to a more stable training process. Typical range: <code>0.9</code> - <code>0.95</code> <code>hyperparameters -&gt; num_epoch</code> (default = <code>3</code>) Number of passes to make through the experience buffer when performing gradient descent optimization.The larger the batch_size, the larger it is acceptable to make this. Decreasing this will ensure more stable updates, at the cost of slower learning. Typical range: <code>3</code> - <code>10</code> <code>hyperparameters -&gt; shared_critic</code> (default = <code>False</code>) Whether or not the policy and value function networks share a backbone. It may be useful to use a shared backbone when learning from image observations."},{"location":"Training-Configuration-File/#sac-specific-configurations","title":"SAC-specific Configurations","text":"Setting Description <code>hyperparameters -&gt; buffer_init_steps</code> (default = <code>0</code>) Number of experiences to collect into the buffer before updating the policy model. As the untrained policy is fairly random, pre-filling the buffer with random actions is useful for exploration. Typically, at least several episodes of experiences should be pre-filled. Typical range: <code>1000</code> - <code>10000</code> <code>hyperparameters -&gt; init_entcoef</code> (default = <code>1.0</code>) How much the agent should explore in the beginning of training. Corresponds to the initial entropy coefficient set at the beginning of training. In SAC, the agent is incentivized to make its actions entropic to facilitate better exploration. The entropy coefficient weighs the true reward with a bonus entropy reward. The entropy coefficient is automatically adjusted to a preset target entropy, so the <code>init_entcoef</code> only corresponds to the starting value of the entropy bonus. Increase init_entcoef to explore more in the beginning, decrease to converge to a solution faster. Typical range: (Continuous): <code>0.5</code> - <code>1.0</code>; (Discrete): <code>0.05</code> - <code>0.5</code> <code>hyperparameters -&gt; save_replay_buffer</code> (default = <code>false</code>) Whether to save and load the experience replay buffer as well as the model when quitting and re-starting training. This may help resumes go more smoothly, as the experiences collected won't be wiped. Note that replay buffers can be very large, and will take up a considerable amount of disk space. For that reason, we disable this feature by default. <code>hyperparameters -&gt; tau</code> (default = <code>0.005</code>) How aggressively to update the target network used for bootstrapping value estimation in SAC. Corresponds to the magnitude of the target Q update during the SAC model update. In SAC, there are two neural networks: the target and the policy. The target network is used to bootstrap the policy's estimate of the future rewards at a given state, and is fixed while the policy is being updated. This target is then slowly updated according to tau. Typically, this value should be left at 0.005. For simple problems, increasing tau to 0.01 might reduce the time it takes to learn, at the cost of stability. Typical range: <code>0.005</code> - <code>0.01</code> <code>hyperparameters -&gt; steps_per_update</code> (default = <code>1</code>) Average ratio of agent steps (actions) taken to updates made of the agent's policy. In SAC, a single \"update\" corresponds to grabbing a batch of size <code>batch_size</code> from the experience replay buffer, and using this mini batch to update the models. Note that it is not guaranteed that after exactly <code>steps_per_update</code> steps an update will be made, only that the ratio will hold true over many steps. Typically, <code>steps_per_update</code> should be greater than or equal to 1. Note that setting <code>steps_per_update</code> lower will improve sample efficiency (reduce the number of steps required to train) but increase the CPU time spent performing updates. For most environments where steps are fairly fast (e.g. our example environments) <code>steps_per_update</code> equal to the number of agents in the scene is a good balance. For slow environments (steps take 0.1 seconds or more) reducing <code>steps_per_update</code> may improve training speed. We can also change <code>steps_per_update</code> to lower than 1 to update more often than once per step, though this will usually result in a slowdown unless the environment is very slow. Typical range: <code>1</code> - <code>20</code> <code>hyperparameters -&gt; reward_signal_num_update</code> (default = <code>steps_per_update</code>) Number of steps per mini batch sampled and used for updating the reward signals. By default, we update the reward signals once every time the main policy is updated. However, to imitate the training procedure in certain imitation learning papers (e.g. Kostrikov et. al, Blond\u00e9 et. al), we may want to update the reward signal (GAIL) M times for every update of the policy. We can change <code>steps_per_update</code> of SAC to N, as well as <code>reward_signal_steps_per_update</code> under <code>reward_signals</code> to N / M to accomplish this. By default, <code>reward_signal_steps_per_update</code> is set to <code>steps_per_update</code>."},{"location":"Training-Configuration-File/#ma-poca-specific-configurations","title":"MA-POCA-specific Configurations","text":"<p>MA-POCA uses the same configurations as PPO, and there are no additional POCA-specific parameters.</p> <p>NOTE: Reward signals other than Extrinsic Rewards have not been extensively tested with MA-POCA, though they can still be added and used for training on a your-mileage-may-vary basis.</p>"},{"location":"Training-Configuration-File/#reward-signals","title":"Reward Signals","text":"<p>The <code>reward_signals</code> section enables the specification of settings for both extrinsic (i.e. environment-based) and intrinsic reward signals (e.g. curiosity and GAIL). Each reward signal should define at least two parameters, <code>strength</code> and <code>gamma</code>, in addition to any class-specific hyperparameters. Note that to remove a reward signal, you should delete its entry entirely from <code>reward_signals</code>. At least one reward signal should be left defined at all times. Provide the following configurations to design the reward signal for your training run.</p>"},{"location":"Training-Configuration-File/#extrinsic-rewards","title":"Extrinsic Rewards","text":"<p>Enable these settings to ensure that your training run incorporates your environment-based reward signal:</p> Setting Description <code>extrinsic -&gt; strength</code> (default = <code>1.0</code>) Factor by which to multiply the reward given by the environment. Typical ranges will vary depending on the reward signal. Typical range: <code>1.00</code> <code>extrinsic -&gt; gamma</code> (default = <code>0.99</code>) Discount factor for future rewards coming from the environment. This can be thought of as how far into the future the agent should care about possible rewards. In situations when the agent should be acting in the present in order to prepare for rewards in the distant future, this value should be large. In cases when rewards are more immediate, it can be smaller. Must be strictly smaller than 1. Typical range: <code>0.8</code> - <code>0.995</code>"},{"location":"Training-Configuration-File/#curiosity-intrinsic-reward","title":"Curiosity Intrinsic Reward","text":"<p>To enable curiosity, provide these settings:</p> Setting Description <code>curiosity -&gt; strength</code> (default = <code>1.0</code>) Magnitude of the curiosity reward generated by the intrinsic curiosity module. This should be scaled in order to ensure it is large enough to not be overwhelmed by extrinsic reward signals in the environment. Likewise it should not be too large to overwhelm the extrinsic reward signal. Typical range: <code>0.001</code> - <code>0.1</code> <code>curiosity -&gt; gamma</code> (default = <code>0.99</code>) Discount factor for future rewards. Typical range: <code>0.8</code> - <code>0.995</code> <code>curiosity -&gt; network_settings</code> Please see the documentation for <code>network_settings</code> under Common Trainer Configurations. The network specs used by the intrinsic curiosity model. The value should of <code>hidden_units</code> should be small enough to encourage the ICM to compress the original observation, but also not too small to prevent it from learning to differentiate between expected and actual observations. Typical range: <code>64</code> - <code>256</code> <code>curiosity -&gt; learning_rate</code> (default = <code>3e-4</code>) Learning rate used to update the intrinsic curiosity module. This should typically be decreased if training is unstable, and the curiosity loss is unstable. Typical range: <code>1e-5</code> - <code>1e-3</code>"},{"location":"Training-Configuration-File/#gail-intrinsic-reward","title":"GAIL Intrinsic Reward","text":"<p>To enable GAIL (assuming you have recorded demonstrations), provide these settings:</p> Setting Description <code>gail -&gt; strength</code> (default = <code>1.0</code>) Factor by which to multiply the raw reward. Note that when using GAIL with an Extrinsic Signal, this value should be set lower if your demonstrations are suboptimal (e.g. from a human), so that a trained agent will focus on receiving extrinsic rewards instead of exactly copying the demonstrations. Keep the strength below about 0.1 in those cases. Typical range: <code>0.01</code> - <code>1.0</code> <code>gail -&gt; gamma</code> (default = <code>0.99</code>) Discount factor for future rewards. Typical range: <code>0.8</code> - <code>0.9</code> <code>gail -&gt; demo_path</code> (Required, no default) The path to your .demo file or directory of .demo files. <code>gail -&gt; network_settings</code> Please see the documentation for <code>network_settings</code> under Common Trainer Configurations. The network specs for the GAIL discriminator. The value of <code>hidden_units</code> should be small enough to encourage the discriminator to compress the original observation, but also not too small to prevent it from learning to differentiate between demonstrated and actual behavior. Dramatically increasing this size will also negatively affect training times. Typical range: <code>64</code> - <code>256</code> <code>gail -&gt; learning_rate</code> (Optional, default = <code>3e-4</code>) Learning rate used to update the discriminator. This should typically be decreased if training is unstable, and the GAIL loss is unstable. Typical range: <code>1e-5</code> - <code>1e-3</code> <code>gail -&gt; use_actions</code> (default = <code>false</code>) Determines whether the discriminator should discriminate based on both observations and actions, or just observations. Set to True if you want the agent to mimic the actions from the demonstrations, and False if you'd rather have the agent visit the same states as in the demonstrations but with possibly different actions. Setting to False is more likely to be stable, especially with imperfect demonstrations, but may learn slower. <code>gail -&gt; use_vail</code> (default = <code>false</code>) Enables a variational bottleneck within the GAIL discriminator. This forces the discriminator to learn a more general representation and reduces its tendency to be \"too good\" at discriminating, making learning more stable. However, it does increase training time. Enable this if you notice your imitation learning is unstable, or unable to learn the task at hand."},{"location":"Training-Configuration-File/#rnd-intrinsic-reward","title":"RND Intrinsic Reward","text":"<p>Random Network Distillation (RND) is only available for the PyTorch trainers. To enable RND, provide these settings:</p> Setting Description <code>rnd -&gt; strength</code> (default = <code>1.0</code>) Magnitude of the curiosity reward generated by the intrinsic rnd module. This should be scaled in order to ensure it is large enough to not be overwhelmed by extrinsic reward signals in the environment. Likewise it should not be too large to overwhelm the extrinsic reward signal. Typical range: <code>0.001</code> - <code>0.01</code> <code>rnd -&gt; gamma</code> (default = <code>0.99</code>) Discount factor for future rewards. Typical range: <code>0.8</code> - <code>0.995</code> <code>rnd -&gt; network_settings</code> Please see the documentation for <code>network_settings</code> under Common Trainer Configurations. The network specs for the RND model. <code>curiosity -&gt; learning_rate</code> (default = <code>3e-4</code>) Learning rate used to update the RND module. This should be large enough for the RND module to quickly learn the state representation, but small enough to allow for stable learning. Typical range: <code>1e-5</code> - <code>1e-3</code>"},{"location":"Training-Configuration-File/#behavioral-cloning","title":"Behavioral Cloning","text":"<p>To enable Behavioral Cloning as a pre-training option (assuming you have recorded demonstrations), provide the following configurations under the <code>behavioral_cloning</code> section:</p> Setting Description <code>demo_path</code> (Required, no default) The path to your .demo file or directory of .demo files. <code>strength</code> (default = <code>1.0</code>) Learning rate of the imitation relative to the learning rate of PPO, and roughly corresponds to how strongly we allow BC to influence the policy. Typical range: <code>0.1</code> - <code>0.5</code> <code>steps</code> (default = <code>0</code>) During BC, it is often desirable to stop using demonstrations after the agent has \"seen\" rewards, and allow it to optimize past the available demonstrations and/or generalize outside of the provided demonstrations. steps corresponds to the training steps over which BC is active. The learning rate of BC will anneal over the steps. Set the steps to 0 for constant imitation over the entire training run. <code>batch_size</code> (default = <code>batch_size</code> of trainer) Number of demonstration experiences used for one iteration of a gradient descent update. If not specified, it will default to the <code>batch_size</code> of the trainer. Typical range: (Continuous): <code>512</code> - <code>5120</code>; (Discrete): <code>32</code> - <code>512</code> <code>num_epoch</code> (default = <code>num_epoch</code> of trainer) Number of passes through the experience buffer during gradient descent. If not specified, it will default to the number of epochs set for PPO. Typical range: <code>3</code> - <code>10</code> <code>samples_per_update</code> (default = <code>0</code>) Maximum number of samples to use during each imitation update. You may want to lower this if your demonstration dataset is very large to avoid overfitting the policy on demonstrations. Set to 0 to train over all of the demonstrations at each update step. Typical range: <code>buffer_size</code>"},{"location":"Training-Configuration-File/#memory-enhanced-agents-using-recurrent-neural-networks","title":"Memory-enhanced Agents using Recurrent Neural Networks","text":"<p>You can enable your agents to use memory by adding a <code>memory</code> section under <code>network_settings</code>, and setting <code>memory_size</code> and <code>sequence_length</code>:</p> Setting Description <code>network_settings -&gt; memory -&gt; memory_size</code> (default = <code>128</code>) Size of the memory an agent must keep. In order to use a LSTM, training requires a sequence of experiences instead of single experiences. Corresponds to the size of the array of floating point numbers used to store the hidden state of the recurrent neural network of the policy. This value must be a multiple of 2, and should scale with the amount of information you expect the agent will need to remember in order to successfully complete the task. Typical range: <code>32</code> - <code>256</code> <code>network_settings -&gt; memory -&gt; sequence_length</code> (default = <code>64</code>) Defines how long the sequences of experiences must be while training. Note that if this number is too small, the agent will not be able to remember things over longer periods of time. If this number is too large, the neural network will take longer to train. Typical range: <code>4</code> - <code>128</code> <p>A few considerations when deciding to use memory:</p> <ul> <li>LSTM does not work well with continuous actions. Please use   discrete actions for better results.</li> <li>Adding a recurrent layer increases the complexity of the neural network, it is   recommended to decrease <code>num_layers</code> when using recurrent.</li> <li>It is required that <code>memory_size</code> be divisible by 2.</li> </ul>"},{"location":"Training-Configuration-File/#self-play","title":"Self-Play","text":"<p>Training with self-play adds additional confounding factors to the usual issues faced by reinforcement learning. In general, the tradeoff is between the skill level and generality of the final policy and the stability of learning. Training against a set of slowly or unchanging adversaries with low diversity results in a more stable learning process than training against a set of quickly changing adversaries with high diversity. With this context, this guide discusses the exposed self-play hyperparameters and intuitions for tuning them.</p> <p>If your environment contains multiple agents that are divided into teams, you can leverage our self-play training option by providing these configurations for each Behavior:</p> Setting Description <code>save_steps</code> (default = <code>20000</code>) Number of trainer steps between snapshots. For example, if <code>save_steps=10000</code> then a snapshot of the current policy will be saved every <code>10000</code> trainer steps. Note, trainer steps are counted per agent. For more information, please see the migration doc after v0.13. A larger value of <code>save_steps</code> will yield a set of opponents that cover a wider range of skill levels and possibly play styles since the policy receives more training. As a result, the agent trains against a wider variety of opponents. Learning a policy to defeat more diverse opponents is a harder problem and so may require more overall training steps but also may lead to more general and robust policy at the end of training. This value is also dependent on how intrinsically difficult the environment is for the agent.  Typical range: <code>10000</code> - <code>100000</code> <code>team_change</code> (default = <code>5 * save_steps</code>) Number of trainer_steps between switching the learning team. This is the number of trainer steps the teams associated with a specific ghost trainer will train before a different team becomes the new learning team. It is possible that, in asymmetric games, opposing teams require fewer trainer steps to make similar performance gains. This enables users to train a more complicated team of agents for more trainer steps than a simpler team of agents per team switch. A larger value of <code>team-change</code> will allow the agent to train longer against it's opponents. The longer an agent trains against the same set of opponents the more able it will be to defeat them. However, training against them for too long may result in overfitting to the particular opponent strategies and so the agent may fail against the next batch of opponents.  The value of <code>team-change</code> will determine how many snapshots of the agent's policy are saved to be used as opponents for the other team. So, we recommend setting this value as a function of the <code>save_steps</code> parameter discussed previously.  Typical range: 4x-10x where x=<code>save_steps</code> <code>swap_steps</code> (default = <code>10000</code>) Number of ghost steps (not trainer steps) between swapping the opponents policy with a different snapshot. A 'ghost step' refers to a step taken by an agent that is following a fixed policy and not learning. The reason for this distinction is that in asymmetric games, we may have teams with an unequal number of agents e.g. a 2v1 scenario like our Strikers Vs Goalie example environment. The team with two agents collects twice as many agent steps per environment step as the team with one agent. Thus, these two values will need to be distinct to ensure that the same number of trainer steps corresponds to the same number of opponent swaps for each team. The formula for <code>swap_steps</code> if a user desires <code>x</code> swaps of a team with <code>num_agents</code> agents against an opponent team with <code>num_opponent_agents</code> agents during <code>team-change</code> total steps is: <code>(num_agents / num_opponent_agents) * (team_change / x)</code>  Typical range: <code>10000</code> - <code>100000</code> <code>play_against_latest_model_ratio</code> (default = <code>0.5</code>) Probability an agent will play against the latest opponent policy. With probability 1 - <code>play_against_latest_model_ratio</code>, the agent will play against a snapshot of its opponent from a past iteration.  A larger value of <code>play_against_latest_model_ratio</code> indicates that an agent will be playing against the current opponent more often. Since the agent is updating it's policy, the opponent will be different from iteration to iteration. This can lead to an unstable learning environment, but poses the agent with an auto-curricula of more increasingly challenging situations which may lead to a stronger final policy.  Typical range: <code>0.0</code> - <code>1.0</code> <code>window</code> (default = <code>10</code>) Size of the sliding window of past snapshots from which the agent's opponents are sampled. For example, a <code>window</code> size of 5 will save the last 5 snapshots taken. Each time a new snapshot is taken, the oldest is discarded. A larger value of <code>window</code> means that an agent's pool of opponents will contain a larger diversity of behaviors since it will contain policies from earlier in the training run. Like in the <code>save_steps</code> hyperparameter, the agent trains against a wider variety of opponents. Learning a policy to defeat more diverse opponents is a harder problem and so may require more overall training steps but also may lead to more general and robust policy at the end of training.  Typical range: <code>5</code> - <code>30</code>"},{"location":"Training-Configuration-File/#note-on-reward-signals","title":"Note on Reward Signals","text":"<p>We make the assumption that the final reward in a trajectory corresponds to the outcome of an episode. A final reward of +1 indicates winning, -1 indicates losing and 0 indicates a draw. The ELO calculation (discussed below) depends on this final reward being either +1, 0, -1.</p> <p>The reward signal should still be used as described in the documentation for the other trainers. However, we encourage users to be a bit more conservative when shaping reward functions due to the instability and non-stationarity of learning in adversarial games. Specifically, we encourage users to begin with the simplest possible reward function (+1 winning, -1 losing) and to allow for more iterations of training to compensate for the sparsity of reward.</p>"},{"location":"Training-Configuration-File/#note-on-swap-steps","title":"Note on Swap Steps","text":"<p>As an example, in a 2v1 scenario, if we want the swap to occur x=4 times during team-change=200000 steps, the swap_steps for the team of one agent is:</p> <p>swap_steps = (1 / 2) * (200000 / 4) = 25000 The swap_steps for the team of two agents is:</p> <p>swap_steps = (2 / 1) * (200000 / 4) = 100000 Note, with equal team sizes, the first term is equal to 1 and swap_steps can be calculated by just dividing the total steps by the desired number of swaps.</p> <p>A larger value of swap_steps means that an agent will play against the same fixed opponent for a longer number of training iterations. This results in a more stable training scenario, but leaves the agent open to the risk of overfitting it's behavior for this particular opponent. Thus, when a new opponent is swapped, the agent may lose more often than expected.</p>"},{"location":"Training-ML-Agents/","title":"Training ML-Agents","text":"<p>Table of Contents</p> <ul> <li>Training ML-Agents</li> <li>Training with mlagents-learn<ul> <li>Starting Training</li> <li>Observing Training</li> <li>Stopping and Resuming Training</li> <li>Loading an Existing Model</li> </ul> </li> <li>Training Configurations<ul> <li>Adding CLI Arguments to the Training Configuration file</li> <li>Environment settings</li> <li>Engine settings</li> <li>Checkpoint settings</li> <li>Torch settings:</li> <li>Behavior Configurations</li> <li>Default Behavior Settings</li> <li>Environment Parameters</li> <li>Environment Parameter Randomization<ul> <li>Supported Sampler Types</li> <li>Training with Environment Parameter Randomization</li> </ul> </li> <li>Curriculum<ul> <li>Training with a Curriculum</li> </ul> </li> <li>Training Using Concurrent Unity Instances</li> </ul> </li> </ul> <p>For a broad overview of reinforcement learning, imitation learning and all the training scenarios, methods and options within the ML-Agents Toolkit, see ML-Agents Toolkit Overview.</p> <p>Once your learning environment has been created and is ready for training, the next step is to initiate a training run. Training in the ML-Agents Toolkit is powered by a dedicated Python package, <code>mlagents</code>. This package exposes a command <code>mlagents-learn</code> that is the single entry point for all training workflows (e.g. reinforcement leaning, imitation learning, curriculum learning). Its implementation can be found at ml-agents/mlagents/trainers/learn.py.</p>"},{"location":"Training-ML-Agents/#training-with-mlagents-learn","title":"Training with mlagents-learn","text":""},{"location":"Training-ML-Agents/#starting-training","title":"Starting Training","text":"<p><code>mlagents-learn</code> is the main training utility provided by the ML-Agents Toolkit. It accepts a number of CLI options in addition to a YAML configuration file that contains all the configurations and hyperparameters to be used during training. The set of configurations and hyperparameters to include in this file depend on the agents in your environment and the specific training method you wish to utilize. Keep in mind that the hyperparameter values can have a big impact on the training performance (i.e. your agent's ability to learn a policy that solves the task). In this page, we will review all the hyperparameters for all training methods and provide guidelines and advice on their values.</p> <p>To view a description of all the CLI options accepted by <code>mlagents-learn</code>, use the <code>--help</code>:</p> <pre><code>mlagents-learn --help\n</code></pre> <p>The basic command for training is:</p> <pre><code>mlagents-learn &lt;trainer-config-file&gt; --env=&lt;env_name&gt; --run-id=&lt;run-identifier&gt;\n</code></pre> <p>where</p> <ul> <li><code>&lt;trainer-config-file&gt;</code> is the file path of the trainer configuration YAML.   This contains all the hyperparameter values. We offer a detailed guide on the   structure of this file and the meaning of the hyperparameters (and advice on   how to set them) in the dedicated   Training Configurations section below.</li> <li><code>&lt;env_name&gt;</code>(Optional) is the name (including path) of your   Unity executable containing the agents   to be trained. If <code>&lt;env_name&gt;</code> is not passed, the training will happen in the   Editor. Press the Play button in Unity when the message \"Start training   by pressing the Play button in the Unity Editor\" is displayed on the screen.</li> <li><code>&lt;run-identifier&gt;</code> is a unique name you can use to identify the results of   your training runs.</li> </ul> <p>See the Getting Started Guide for a sample execution of the <code>mlagents-learn</code> command.</p>"},{"location":"Training-ML-Agents/#observing-training","title":"Observing Training","text":"<p>Regardless of which training methods, configurations or hyperparameters you provide, the training process will always generate three artifacts, all found in the <code>results/&lt;run-identifier&gt;</code> folder:</p> <ol> <li>Summaries: these are training metrics that    are updated throughout the training process. They are helpful to monitor your    training performance and may help inform how to update your hyperparameter    values. See Using TensorBoard for more details on how    to visualize the training metrics.</li> <li>Models: these contain the model checkpoints that    are updated throughout training and the final model file (<code>.onnx</code>). This final    model file is generated once either when training completes or is    interrupted.</li> <li>Timers file (under <code>results/&lt;run-identifier&gt;/run_logs</code>): this contains aggregated    metrics on your training process, including time spent on specific code    blocks. See Profiling in Python for more information    on the timers generated.</li> </ol> <p>These artifacts are updated throughout the training process and finalized when training is completed or is interrupted.</p>"},{"location":"Training-ML-Agents/#stopping-and-resuming-training","title":"Stopping and Resuming Training","text":"<p>To interrupt training and save the current progress, hit <code>Ctrl+C</code> once and wait for the model(s) to be saved out.</p> <p>To resume a previously interrupted or completed training run, use the <code>--resume</code> flag and make sure to specify the previously used run ID.</p> <p>If you would like to re-run a previously interrupted or completed training run and re-use the same run ID (in this case, overwriting the previously generated artifacts), then use the <code>--force</code> flag.</p>"},{"location":"Training-ML-Agents/#loading-an-existing-model","title":"Loading an Existing Model","text":"<p>You can also use this mode to run inference of an already-trained model in Python by using both the <code>--resume</code> and <code>--inference</code> flags. Note that if you want to run inference in Unity, you should use the Sentis.</p> <p>Additionally, if the network architecture changes, you may still load an existing model, but ML-Agents will only load the parts of the model it can load and ignore all others. For instance, if you add a new reward signal, the existing model will load but the new reward signal will be initialized from scratch. If you have a model with a visual encoder (CNN) but change the <code>hidden_units</code>, the CNN will be loaded but the body of the network will be initialized from scratch.</p> <p>Alternatively, you might want to start a new training run but initialize it using an already-trained model. You may want to do this, for instance, if your environment changed and you want a new model, but the old behavior is still better than random. You can do this by specifying <code>--initialize-from=&lt;run-identifier&gt;</code>, where <code>&lt;run-identifier&gt;</code> is the old run ID.</p>"},{"location":"Training-ML-Agents/#training-configurations","title":"Training Configurations","text":"<p>The Unity ML-Agents Toolkit provides a wide range of training scenarios, methods and options. As such, specific training runs may require different training configurations and may generate different artifacts and TensorBoard statistics. This section offers a detailed guide into how to manage the different training set-ups withing the toolkit.</p> <p>More specifically, this section offers a detailed guide on the command-line flags for <code>mlagents-learn</code> that control the training configurations:</p> <ul> <li><code>&lt;trainer-config-file&gt;</code>: defines the training hyperparameters for each   Behavior in the scene, and the set-ups for the environment parameters   (Curriculum Learning and Environment Parameter Randomization)</li> </ul> <p>It is important to highlight that successfully training a Behavior in the ML-Agents Toolkit involves tuning the training hyperparameters and configuration. This guide contains some best practices for tuning the training process when the default parameters don't seem to be giving the level of performance you would like. We provide sample configuration files for our example environments in the config/ directory. The <code>config/ppo/3DBall.yaml</code> was used to train the 3D Balance Ball in the Getting Started guide. That configuration file uses the PPO trainer, but we also have configuration files for SAC and GAIL.</p> <p>Additionally, the set of configurations you provide depend on the training functionalities you use (see ML-Agents Toolkit Overview for a description of all the training functionalities). Each functionality you add typically has its own training configurations. For instance:</p> <ul> <li>Use PPO or SAC?</li> <li>Use Recurrent Neural Networks for adding memory to your agents?</li> <li>Use the intrinsic curiosity module?</li> <li>Ignore the environment reward signal?</li> <li>Pre-train using behavioral cloning? (Assuming you have recorded   demonstrations.)</li> <li>Include the GAIL intrinsic reward signals? (Assuming you have recorded   demonstrations.)</li> <li>Use self-play? (Assuming your environment includes multiple agents.)</li> </ul> <p>The trainer config file, <code>&lt;trainer-config-file&gt;</code>, determines the features you will use during training, and the answers to the above questions will dictate its contents. The rest of this guide breaks down the different sub-sections of the trainer config file and explains the possible settings for each. If you need a list of all the trainer configurations, please see Training Configuration File.</p> <p>NOTE: The configuration file format has been changed between 0.17.0 and 0.18.0 and between 0.18.0 and onwards. To convert an old set of configuration files (trainer config, curriculum, and sampler files) to the new format, a script has been provided. Run <code>python -m mlagents.trainers.upgrade_config -h</code> in your console to see the script's usage.</p>"},{"location":"Training-ML-Agents/#adding-cli-arguments-to-the-training-configuration-file","title":"Adding CLI Arguments to the Training Configuration file","text":"<p>Additionally, within the training configuration YAML file, you can also add the CLI arguments (such as <code>--num-envs</code>).</p> <p>Reminder that a detailed description of all the CLI arguments can be found by using the help utility:</p> <pre><code>mlagents-learn --help\n</code></pre> <p>These additional CLI arguments are grouped into environment, engine, checkpoint and torch. The available settings and example values are shown below.</p>"},{"location":"Training-ML-Agents/#environment-settings","title":"Environment settings","text":"<pre><code>env_settings:\n  env_path: FoodCollector\n  env_args: null\n  base_port: 5005\n  num_envs: 1\n  timeout_wait: 10\n  seed: -1\n  max_lifetime_restarts: 10\n  restarts_rate_limit_n: 1\n  restarts_rate_limit_period_s: 60\n</code></pre>"},{"location":"Training-ML-Agents/#engine-settings","title":"Engine settings","text":"<pre><code>engine_settings:\n  width: 84\n  height: 84\n  quality_level: 5\n  time_scale: 20\n  target_frame_rate: -1\n  capture_frame_rate: 60\n  no_graphics: false\n</code></pre>"},{"location":"Training-ML-Agents/#checkpoint-settings","title":"Checkpoint settings","text":"<pre><code>checkpoint_settings:\n  run_id: foodtorch\n  initialize_from: null\n  load_model: false\n  resume: false\n  force: true\n  train_model: false\n  inference: false\n</code></pre>"},{"location":"Training-ML-Agents/#torch-settings","title":"Torch settings:","text":"<pre><code>torch_settings:\n  device: cpu\n</code></pre>"},{"location":"Training-ML-Agents/#behavior-configurations","title":"Behavior Configurations","text":"<p>The primary section of the trainer config file is a set of configurations for each Behavior in your scene. These are defined under the sub-section <code>behaviors</code> in your trainer config file. Some of the configurations are required while others are optional. To help us get started, below is a sample file that includes all the possible settings if we're using a PPO trainer with all the possible training functionalities enabled (memory, behavioral cloning, curiosity, GAIL and self-play). You will notice that curriculum and environment parameter randomization settings are not part of the <code>behaviors</code> configuration, but in their own section called <code>environment_parameters</code>.</p> <pre><code>behaviors:\n  BehaviorPPO:\n    trainer_type: ppo\n\n    hyperparameters:\n      # Hyperparameters common to PPO and SAC\n      batch_size: 1024\n      buffer_size: 10240\n      learning_rate: 3.0e-4\n      learning_rate_schedule: linear\n\n      # PPO-specific hyperparameters\n      beta: 5.0e-3\n      beta_schedule: constant\n      epsilon: 0.2\n      epsilon_schedule: linear\n      lambd: 0.95\n      num_epoch: 3\n      shared_critic: False\n\n    # Configuration of the neural network (common to PPO/SAC)\n    network_settings:\n      vis_encode_type: simple\n      normalize: false\n      hidden_units: 128\n      num_layers: 2\n      # memory\n      memory:\n        sequence_length: 64\n        memory_size: 256\n\n    # Trainer configurations common to all trainers\n    max_steps: 5.0e5\n    time_horizon: 64\n    summary_freq: 10000\n    keep_checkpoints: 5\n    checkpoint_interval: 50000\n    threaded: false\n    init_path: null\n\n    # behavior cloning\n    behavioral_cloning:\n      demo_path: Project/Assets/ML-Agents/Examples/Pyramids/Demos/ExpertPyramid.demo\n      strength: 0.5\n      steps: 150000\n      batch_size: 512\n      num_epoch: 3\n      samples_per_update: 0\n\n    reward_signals:\n      # environment reward (default)\n      extrinsic:\n        strength: 1.0\n        gamma: 0.99\n\n      # curiosity module\n      curiosity:\n        strength: 0.02\n        gamma: 0.99\n        encoding_size: 256\n        learning_rate: 3.0e-4\n\n      # GAIL\n      gail:\n        strength: 0.01\n        gamma: 0.99\n        encoding_size: 128\n        demo_path: Project/Assets/ML-Agents/Examples/Pyramids/Demos/ExpertPyramid.demo\n        learning_rate: 3.0e-4\n        use_actions: false\n        use_vail: false\n\n    # self-play\n    self_play:\n      window: 10\n      play_against_latest_model_ratio: 0.5\n      save_steps: 50000\n      swap_steps: 2000\n      team_change: 100000\n</code></pre> <p>Here is an equivalent file if we use an SAC trainer instead. Notice that the configurations for the additional functionalities (memory, behavioral cloning, curiosity and self-play) remain unchanged.</p> <pre><code>behaviors:\n  BehaviorSAC:\n    trainer_type: sac\n\n    # Trainer configs common to PPO/SAC (excluding reward signals)\n    # same as PPO config\n\n    # SAC-specific configs (replaces the hyperparameters section above)\n    hyperparameters:\n      # Hyperparameters common to PPO and SAC\n      # Same as PPO config\n\n      # SAC-specific hyperparameters\n      # Replaces the \"PPO-specific hyperparameters\" section above\n      buffer_init_steps: 0\n      tau: 0.005\n      steps_per_update: 10.0\n      save_replay_buffer: false\n      init_entcoef: 0.5\n      reward_signal_steps_per_update: 10.0\n\n    # Configuration of the neural network (common to PPO/SAC)\n    network_settings:\n      # Same as PPO config\n\n    # Trainer configurations common to all trainers\n      # &lt;Same as PPO config&gt;\n\n    # pre-training using behavior cloning\n    behavioral_cloning:\n      # same as PPO config\n\n    reward_signals:\n      # environment reward\n      extrinsic:\n        # same as PPO config\n\n      # curiosity module\n      curiosity:\n        # same as PPO config\n\n      # GAIL\n      gail:\n        # same as PPO config\n\n    # self-play\n    self_play:\n      # same as PPO config\n</code></pre> <p>We now break apart the components of the configuration file and describe what each of these parameters mean and provide guidelines on how to set them. See Training Configuration File for a detailed description of all the configurations listed above, along with their defaults. Unless otherwise specified, omitting a configuration will revert it to its default.</p>"},{"location":"Training-ML-Agents/#default-behavior-settings","title":"Default Behavior Settings","text":"<p>In some cases, you may want to specify a set of default configurations for your Behaviors. This may be useful, for instance, if your Behavior names are generated procedurally by the environment and not known before runtime, or if you have many Behaviors with very similar settings. To specify a default configuraton, insert a <code>default_settings</code> section in your YAML. This section should be formatted exactly like a configuration for a Behavior.</p> <pre><code>default_settings:\n  # &lt; Same as Behavior configuration &gt;\nbehaviors:\n  # &lt; Same as above &gt;\n</code></pre> <p>Behaviors found in the environment that aren't specified in the YAML will now use the <code>default_settings</code>, and unspecified settings in behavior configurations will default to the values in <code>default_settings</code> if specified there.</p>"},{"location":"Training-ML-Agents/#environment-parameters","title":"Environment Parameters","text":"<p>In order to control the <code>EnvironmentParameters</code> in the Unity simulation during training, you need to add a section called <code>environment_parameters</code>. For example you can set the value of an <code>EnvironmentParameter</code> called <code>my_environment_parameter</code> to <code>3.0</code> with the following code :</p> <pre><code>behaviors:\n  BehaviorY:\n    # &lt; Same as above &gt;\n\n# Add this section\nenvironment_parameters:\n  my_environment_parameter: 3.0\n</code></pre> <p>Inside the Unity simulation, you can access your Environment Parameters by doing :</p> <pre><code>Academy.Instance.EnvironmentParameters.GetWithDefault(\"my_environment_parameter\", 0.0f);\n</code></pre>"},{"location":"Training-ML-Agents/#environment-parameter-randomization","title":"Environment Parameter Randomization","text":"<p>To enable environment parameter randomization, you need to edit the <code>environment_parameters</code> section of your training configuration yaml file. Instead of providing a single float value for your environment parameter, you can specify a sampler instead. Here is an example with three environment parameters called <code>mass</code>, <code>length</code> and <code>scale</code>:</p> <pre><code>behaviors:\n  BehaviorY:\n    # &lt; Same as above &gt;\n\n# Add this section\nenvironment_parameters:\n  mass:\n    sampler_type: uniform\n    sampler_parameters:\n        min_value: 0.5\n        max_value: 10\n\n  length:\n    sampler_type: multirangeuniform\n    sampler_parameters:\n        intervals: [[7, 10], [15, 20]]\n\n  scale:\n    sampler_type: gaussian\n    sampler_parameters:\n        mean: 2\n        st_dev: .3\n</code></pre> Setting Description <code>sampler_type</code> A string identifier for the type of sampler to use for this <code>Environment Parameter</code>. <code>sampler_parameters</code> The parameters for a given <code>sampler_type</code>. Samplers of different types can have different <code>sampler_parameters</code>"},{"location":"Training-ML-Agents/#supported-sampler-types","title":"Supported Sampler Types","text":"<p>Below is a list of the <code>sampler_type</code> values supported by the toolkit.</p> <ul> <li><code>uniform</code> - Uniform sampler</li> <li>Uniformly samples a single float value from a range with a given minimum     and maximum value (inclusive).</li> <li>parameters - <code>min_value</code>, <code>max_value</code></li> <li><code>gaussian</code> - Gaussian sampler</li> <li>Samples a single float value from a normal distribution with a given mean     and standard deviation.</li> <li>parameters - <code>mean</code>, <code>st_dev</code></li> <li><code>multirange_uniform</code> - Multirange uniform sampler</li> <li>First, samples an interval from a set of intervals in proportion to relative     length of the intervals. Then, uniformly samples a single float value from the     sampled interval (inclusive). This sampler can take an arbitrary number of     intervals in a list in the following format:     [[<code>interval_1_min</code>, <code>interval_1_max</code>], [<code>interval_2_min</code>,     <code>interval_2_max</code>], ...]</li> <li>parameters - <code>intervals</code></li> </ul> <p>The implementation of the samplers can be found in the Samplers.cs file.</p>"},{"location":"Training-ML-Agents/#training-with-environment-parameter-randomization","title":"Training with Environment Parameter Randomization","text":"<p>After the sampler configuration is defined, we proceed by launching <code>mlagents-learn</code> and specify trainer configuration with  parameter randomization enabled. For example, if we wanted to train the 3D ball agent with parameter randomization, we would run</p> <pre><code>mlagents-learn config/ppo/3DBall_randomize.yaml --run-id=3D-Ball-randomize\n</code></pre> <p>We can observe progress and metrics via TensorBoard.</p>"},{"location":"Training-ML-Agents/#curriculum","title":"Curriculum","text":"<p>To enable curriculum learning, you need to add a <code>curriculum</code> sub-section to your environment parameter. Here is one example with the environment parameter <code>my_environment_parameter</code> :</p> <pre><code>behaviors:\n  BehaviorY:\n    # &lt; Same as above &gt;\n\n# Add this section\nenvironment_parameters:\n  my_environment_parameter:\n    curriculum:\n      - name: MyFirstLesson # The '-' is important as this is a list\n        completion_criteria:\n          measure: progress\n          behavior: my_behavior\n          signal_smoothing: true\n          min_lesson_length: 100\n          threshold: 0.2\n        value: 0.0\n      - name: MySecondLesson # This is the start of the second lesson\n        completion_criteria:\n          measure: progress\n          behavior: my_behavior\n          signal_smoothing: true\n          min_lesson_length: 100\n          threshold: 0.6\n          require_reset: true\n        value:\n          sampler_type: uniform\n          sampler_parameters:\n            min_value: 4.0\n            max_value: 7.0\n      - name: MyLastLesson\n        value: 8.0\n</code></pre> <p>Note that this curriculum only applies to <code>my_environment_parameter</code>. The <code>curriculum</code> section contains a list of <code>Lessons</code>. In the example, the lessons are named <code>MyFirstLesson</code>, <code>MySecondLesson</code> and <code>MyLastLesson</code>. Each <code>Lesson</code> has 3 fields :</p> <ul> <li><code>name</code> which is a user defined name for the lesson (The name of the lesson will be displayed in  the console when the lesson changes)</li> <li><code>completion_criteria</code> which determines what needs to happen in the simulation before the lesson  can be considered complete. When that condition is met, the curriculum moves on to the next  <code>Lesson</code>. Note that you do not need to specify a <code>completion_criteria</code> for the last <code>Lesson</code></li> <li><code>value</code> which is the value the environment parameter will take during the lesson. Note that this  can be a float or a sampler.</li> </ul> <p>There are the different settings of the <code>completion_criteria</code> :</p> Setting Description <code>measure</code> What to measure learning progress, and advancement in lessons by. <code>reward</code> uses a measure of received reward, <code>progress</code> uses the ratio of steps/max_steps, while <code>Elo</code> is available only for self-play situations and uses Elo score as a curriculum completion measure. <code>behavior</code> Specifies which behavior is being tracked. There can be multiple behaviors with different names, each at different points of training. This setting allows the curriculum to track only one of them. <code>threshold</code> Determines at what point in value of <code>measure</code> the lesson should be increased. <code>min_lesson_length</code> The minimum number of episodes that should be completed before the lesson can change. If <code>measure</code> is set to <code>reward</code>, the average cumulative reward of the last <code>min_lesson_length</code> episodes will be used to determine if the lesson should change. Must be nonnegative.  Important: the average reward that is compared to the thresholds is different than the mean reward that is logged to the console. For example, if <code>min_lesson_length</code> is <code>100</code>, the lesson will increment after the average cumulative reward of the last <code>100</code> episodes exceeds the current threshold. The mean reward logged to the console is dictated by the <code>summary_freq</code> parameter defined above. <code>signal_smoothing</code> Whether to weight the current progress measure by previous values. <code>require_reset</code> Whether changing lesson requires the environment to reset (default: false) ##### Training with a Curriculum <p>Once we have specified our metacurriculum and curricula, we can launch <code>mlagents-learn</code> to point to the config file containing our curricula and PPO will train using Curriculum Learning. For example, to train agents in the Wall Jump environment with curriculum learning, we can run:</p> <pre><code>mlagents-learn config/ppo/WallJump_curriculum.yaml --run-id=wall-jump-curriculum\n</code></pre> <p>We can then keep track of the current lessons and progresses via TensorBoard. If you've terminated the run, you can resume it using <code>--resume</code> and lesson progress will start off where it ended.</p>"},{"location":"Training-ML-Agents/#training-using-concurrent-unity-instances","title":"Training Using Concurrent Unity Instances","text":"<p>In order to run concurrent Unity instances during training, set the number of environment instances using the command line option <code>--num-envs=&lt;n&gt;</code> when you invoke <code>mlagents-learn</code>. Optionally, you can also set the <code>--base-port</code>, which is the starting port used for the concurrent Unity instances.</p> <p>Some considerations:</p> <ul> <li>Buffer Size - If you are having trouble getting an agent to train, even   with multiple concurrent Unity instances, you could increase <code>buffer_size</code> in   the trainer config file. A common practice is to multiply   <code>buffer_size</code> by <code>num-envs</code>.</li> <li>Resource Constraints - Invoking concurrent Unity instances is constrained   by the resources on the machine. Please use discretion when setting   <code>--num-envs=&lt;n&gt;</code>.</li> <li>Result Variation Using Concurrent Unity Instances - If you keep all the   hyperparameters the same, but change <code>--num-envs=&lt;n&gt;</code>, the results and model   would likely change.</li> </ul>"},{"location":"Training-Plugins/","title":"Customizing Training via Plugins","text":"<p>ML-Agents provides support for running your own python implementations of specific interfaces during the training process. These interfaces are currently fairly limited, but will be expanded in the future.</p> <p>Note: Plugin interfaces should currently be considered \"in beta\", and they may change in future releases.</p>"},{"location":"Training-Plugins/#how-to-write-your-own-plugin","title":"How to Write Your Own Plugin","text":"<p>This video explains the basics of how to create a plugin system using setuptools, and is the same approach that ML-Agents' plugin system is based on.</p> <p>The <code>ml-agents-plugin-examples</code> directory contains a reference implementation of each plugin interface, so it's a good starting point.</p>"},{"location":"Training-Plugins/#setuppy","title":"setup.py","text":"<p>If you don't already have a <code>setup.py</code> file for your python code, you'll need to add one. <code>ml-agents-plugin-examples</code> has a minimal example of this.</p> <p>In the call to <code>setup()</code>, you'll need to add to the <code>entry_points</code> dictionary for each plugin interface that you implement. The form of this is <code>{entry point name}={plugin module}:{plugin function}</code>. For example, in  <code>ml-agents-plugin-examples</code>:</p> <pre><code>entry_points={\n    ML_AGENTS_STATS_WRITER: [\n        \"example=mlagents_plugin_examples.example_stats_writer:get_example_stats_writer\"\n    ]\n}\n</code></pre> <ul> <li><code>ML_AGENTS_STATS_WRITER</code> (which is a string constant, <code>mlagents.stats_writer</code>) is the name of the plugin interface. This must be one of the provided interfaces (see below).</li> <li><code>example</code> is the plugin implementation name. This can be anything.</li> <li><code>mlagents_plugin_examples.example_stats_writer</code> is the plugin module. This points to the module where the plugin registration function is defined.</li> <li><code>get_example_stats_writer</code> is the plugin registration function. This is called when running <code>mlagents-learn</code>. The arguments and expected return type for this are different for each plugin interface.</li> </ul>"},{"location":"Training-Plugins/#local-installation","title":"Local Installation","text":"<p>Once you've defined <code>entry_points</code> in your <code>setup.py</code>, you will need to run</p> <pre><code>pip install -e [path to your plugin code]\n</code></pre> <p>in the same python virtual environment that you have <code>mlagents</code> installed.</p>"},{"location":"Training-Plugins/#plugin-interfaces","title":"Plugin Interfaces","text":""},{"location":"Training-Plugins/#statswriter","title":"StatsWriter","text":"<p>The StatsWriter class receives various information from the training process, such as the average Agent reward in each summary period. By default, we log this information to the console and write it to TensorBoard.</p>"},{"location":"Training-Plugins/#interface","title":"Interface","text":"<p>The <code>StatsWriter.write_stats()</code> method must be implemented in any derived classes. It takes a \"category\" parameter, which typically is the behavior name of the Agents being trained, and a dictionary of <code>StatSummary</code> values with string keys. Additionally, <code>StatsWriter.on_add_stat()</code> may be extended to register a callback handler for each stat emission.</p>"},{"location":"Training-Plugins/#registration","title":"Registration","text":"<p>The <code>StatsWriter</code> registration function takes a <code>RunOptions</code> argument and returns a list of <code>StatsWriter</code>s. An example implementation is provided in <code>mlagents_plugin_examples</code></p>"},{"location":"Training-on-Amazon-Web-Service/","title":"Training on Amazon Web Service","text":"<p>:warning: Note: We no longer use this guide ourselves and so it may not work correctly. We've decided to keep it up just in case it is helpful to you.</p> <p>This page contains instructions for setting up an EC2 instance on Amazon Web Service for training ML-Agents environments.</p>"},{"location":"Training-on-Amazon-Web-Service/#pre-configured-ami","title":"Pre-configured AMI","text":"<p>We've prepared a pre-configured AMI for you with the ID: <code>ami-016ff5559334f8619</code> in the <code>us-east-1</code> region. It was created as a modification of Deep Learning AMI (Ubuntu). The AMI has been tested with p2.xlarge instance. Furthermore, if you want to train without headless mode, you need to enable X Server.</p> <p>After launching your EC2 instance using the ami and ssh into it, run the following commands to enable it:</p> <pre><code># Start the X Server, press Enter to come to the command line\n$ sudo /usr/bin/X :0 &amp;\n\n# Check if Xorg process is running\n# You will have a list of processes running on the GPU, Xorg should be in the\n# list, as shown below\n$ nvidia-smi\n\n# Thu Jun 14 20:27:26 2018\n# +-----------------------------------------------------------------------------+\n# | NVIDIA-SMI 390.67                 Driver Version: 390.67                    |\n# |-------------------------------+----------------------+----------------------+\n# | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n# | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n# |===============================+======================+======================|\n# |   0  Tesla K80           On   | 00000000:00:1E.0 Off |                    0 |\n# | N/A   35C    P8    31W / 149W |      9MiB / 11441MiB |      0%      Default |\n# +-------------------------------+----------------------+----------------------+\n#\n# +-----------------------------------------------------------------------------+\n# | Processes:                                                       GPU Memory |\n# |  GPU       PID   Type   Process name                             Usage      |\n# |=============================================================================|\n# |    0      2331      G   /usr/lib/xorg/Xorg                             8MiB |\n# +-----------------------------------------------------------------------------+\n\n# Make the ubuntu use X Server for display\n$ export DISPLAY=:0\n</code></pre>"},{"location":"Training-on-Amazon-Web-Service/#configuring-your-own-instance","title":"Configuring your own instance","text":"<p>You could also choose to configure your own instance. To begin with, you will need an EC2 instance which contains the latest Nvidia drivers, CUDA9, and cuDNN. In this tutorial we used the Deep Learning AMI (Ubuntu) listed under AWS Marketplace with a p2.xlarge instance.</p>"},{"location":"Training-on-Amazon-Web-Service/#installing-the-ml-agents-toolkit-on-the-instance","title":"Installing the ML-Agents Toolkit on the instance","text":"<p>After launching your EC2 instance using the ami and ssh into it:</p> <ol> <li>Activate the python3 environment</li> </ol> <p><code>sh    source activate python3</code></p> <ol> <li>Clone the ML-Agents repo and install the required Python packages</li> </ol> <p><code>sh    git clone --branch release_21 https://github.com/Unity-Technologies/ml-agents.git    cd ml-agents/ml-agents/    pip3 install -e .</code></p>"},{"location":"Training-on-Amazon-Web-Service/#setting-up-x-server-optional","title":"Setting up X Server (optional)","text":"<p>X Server setup is only necessary if you want to do training that requires visual observation input. Instructions here are adapted from this Medium post on running general Unity applications in the cloud.</p> <p>Current limitations of the Unity Engine require that a screen be available to render to when using visual observations. In order to make this possible when training on a remote server, a virtual screen is required. We can do this by installing Xorg and creating a virtual screen. Once installed and created, we can display the Unity environment in the virtual environment, and train as we would on a local machine. Ensure that <code>headless</code> mode is disabled when building linux executables which use visual observations.</p>"},{"location":"Training-on-Amazon-Web-Service/#install-and-setup-xorg","title":"Install and setup Xorg:","text":"<pre><code>```sh\n# Install Xorg\n$ sudo apt-get update\n$ sudo apt-get install -y xserver-xorg mesa-utils\n$ sudo nvidia-xconfig -a --use-display-device=None --virtual=1280x1024\n\n# Get the BusID information\n$ nvidia-xconfig --query-gpu-info\n\n# Add the BusID information to your /etc/X11/xorg.conf file\n$ sudo sed -i 's/    BoardName      \"Tesla K80\"/    BoardName      \"Tesla K80\"\\n    BusID          \"0:30:0\"/g' /etc/X11/xorg.conf\n\n# Remove the Section \"Files\" from the /etc/X11/xorg.conf file\n# And remove two lines that contain Section \"Files\" and EndSection\n$ sudo vim /etc/X11/xorg.conf\n```\n</code></pre>"},{"location":"Training-on-Amazon-Web-Service/#update-and-setup-nvidia-driver","title":"Update and setup Nvidia driver:","text":"<pre><code>```sh\n# Download and install the latest Nvidia driver for ubuntu\n# Please refer to http://download.nvidia.com/XFree86/Linux-#x86_64/latest.txt\n$ wget http://download.nvidia.com/XFree86/Linux-x86_64/390.87/NVIDIA-Linux-x86_64-390.87.run\n$ sudo /bin/bash ./NVIDIA-Linux-x86_64-390.87.run --accept-license --no-questions --ui=none\n\n# Disable Nouveau as it will clash with the Nvidia driver\n$ sudo echo 'blacklist nouveau'  | sudo tee -a /etc/modprobe.d/blacklist.conf\n$ sudo echo 'options nouveau modeset=0'  | sudo tee -a /etc/modprobe.d/blacklist.conf\n$ sudo echo options nouveau modeset=0 | sudo tee -a /etc/modprobe.d/nouveau-kms.conf\n$ sudo update-initramfs -u\n```\n</code></pre>"},{"location":"Training-on-Amazon-Web-Service/#restart-the-ec2-instance","title":"Restart the EC2 instance:","text":"<pre><code>```sh\nsudo reboot now\n```\n</code></pre>"},{"location":"Training-on-Amazon-Web-Service/#make-sure-there-are-no-xorg-processes-running","title":"Make sure there are no Xorg processes running:","text":"<pre><code># Kill any possible running Xorg processes\n# Note that you might have to run this command multiple times depending on\n# how Xorg is configured.\n$ sudo killall Xorg\n\n# Check if there is any Xorg process left\n# You will have a list of processes running on the GPU, Xorg should not be in\n# the list, as shown below.\n$ nvidia-smi\n\n# Thu Jun 14 20:21:11 2018\n# +-----------------------------------------------------------------------------+\n# | NVIDIA-SMI 390.67                 Driver Version: 390.67                    |\n# |-------------------------------+----------------------+----------------------+\n# | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n# | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n# |===============================+======================+======================|\n# |   0  Tesla K80           On   | 00000000:00:1E.0 Off |                    0 |\n# | N/A   37C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n# +-------------------------------+----------------------+----------------------+\n#\n# +-----------------------------------------------------------------------------+\n# | Processes:                                                       GPU Memory |\n# |  GPU       PID   Type   Process name                             Usage      |\n# |=============================================================================|\n# |  No running processes found                                                 |\n# +-----------------------------------------------------------------------------+\n\n</code></pre>"},{"location":"Training-on-Amazon-Web-Service/#start-x-server-and-make-the-ubuntu-use-x-server-for-display","title":"Start X Server and make the ubuntu use X Server for display:","text":"<pre><code>```console\n# Start the X Server, press Enter to come back to the command line\n$ sudo /usr/bin/X :0 &amp;\n\n# Check if Xorg process is running\n# You will have a list of processes running on the GPU, Xorg should be in the list.\n$ nvidia-smi\n\n# Make the ubuntu use X Server for display\n$ export DISPLAY=:0\n```\n</code></pre>"},{"location":"Training-on-Amazon-Web-Service/#ensure-the-xorg-is-correctly-configured","title":"Ensure the Xorg is correctly configured:","text":"<pre><code>```sh\n# For more information on glxgears, see ftp://www.x.org/pub/X11R6.8.1/doc/glxgears.1.html.\n$ glxgears\n# If Xorg is configured correctly, you should see the following message\n\n# Running synchronized to the vertical refresh.  The framerate should be\n# approximately the same as the monitor refresh rate.\n# 137296 frames in 5.0 seconds = 27459.053 FPS\n# 141674 frames in 5.0 seconds = 28334.779 FPS\n# 141490 frames in 5.0 seconds = 28297.875 FPS\n\n```\n</code></pre>"},{"location":"Training-on-Amazon-Web-Service/#training-on-ec2-instance","title":"Training on EC2 instance","text":"<ol> <li>In the Unity Editor, load a project containing an ML-Agents environment (you    can use one of the example environments if you have not created your own).</li> <li>Open the Build Settings window (menu: File &gt; Build Settings).</li> <li>Select Linux as the Target Platform, and x86_64 as the target architecture    (the default x86 currently does not work).</li> <li>Check Headless Mode if you have not setup the X Server. (If you do not use    Headless Mode, you have to setup the X Server to enable training.)</li> <li>Click Build to build the Unity environment executable.</li> <li>Upload the executable to your EC2 instance within <code>ml-agents</code> folder.</li> <li>Change the permissions of the executable.</li> </ol> <p><code>sh    chmod +x &lt;your_env&gt;.x86_64</code></p> <ol> <li>(Without Headless Mode) Start X Server and use it for display:</li> </ol> <p>```sh    # Start the X Server, press Enter to come back to the command line    $ sudo /usr/bin/X :0 &amp;</p> <p># Check if Xorg process is running    # You will have a list of processes running on the GPU, Xorg should be in the list.    $ nvidia-smi</p> <p># Make the ubuntu use X Server for display    $ export DISPLAY=:0    ```</p> <ol> <li>Test the instance setup from Python using:</li> </ol> <p>```python    from mlagents_envs.environment import UnityEnvironment</p> <p>env = UnityEnvironment()    ``` <p>Where <code>&lt;your_env&gt;</code> corresponds to the path to your environment executable.</p> <p>You should receive a message confirming that the environment was loaded    successfully.</p> <ol> <li> <p>Train your models</p> <p><code>console mlagents-learn &lt;trainer-config-file&gt; --env=&lt;your_env&gt; --train</code></p> </li> </ol>"},{"location":"Training-on-Amazon-Web-Service/#faq","title":"FAQ","text":""},{"location":"Training-on-Amazon-Web-Service/#the-_data-folder-hasnt-been-copied-cover","title":"The _Data folder hasn't been copied cover <p>If you've built your Linux executable, but forget to copy over the corresponding _Data folder, you will see error message like the following: <pre><code>Set current directory to /home/ubuntu/ml-agents/ml-agents\nFound path: /home/ubuntu/ml-agents/ml-agents/3dball_linux.x86_64\nno boot config - using default values\n\n(Filename:  Line: 403)\n\nThere is no data folder\n</code></pre>","text":""},{"location":"Training-on-Amazon-Web-Service/#unity-environment-not-responding","title":"Unity Environment not responding <p>If you didn't setup X Server or hasn't launched it properly, or your environment somehow crashes, or you haven't <code>chmod +x</code> your Unity Environment, all of these will cause connection between Unity and Python to fail. Then you will see something like this:</p> <pre><code>Logging to /home/ubuntu/.config/unity3d/&lt;Some_Path&gt;/Player.log\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/home/ubuntu/ml-agents/ml-agents/mlagents_envs/environment.py\", line 63, in __init__\n    aca_params = self.send_academy_parameters(rl_init_parameters_in)\n  File \"/home/ubuntu/ml-agents/ml-agents/mlagents_envs/environment.py\", line 489, in send_academy_parameters\n    return self.communicator.initialize(inputs).rl_initialization_output\n  File \"/home/ubuntu/ml-agents/ml-agents/mlagents_envs/rpc_communicator.py\", line 60, in initialize\nmlagents_envs.exception.UnityTimeOutException: The Unity environment took too long to respond. Make sure that :\n         The environment does not need user interaction to launch\n         The environment and the Python interface have compatible versions.\n</code></pre> <p>It would be also really helpful to check your /home/ubuntu/.config/unity3d//Player.log to see what happens with your Unity environment.","text":""},{"location":"Training-on-Amazon-Web-Service/#could-not-launch-x-server","title":"Could not launch X Server <p>When you execute:</p> <pre><code>sudo /usr/bin/X :0 &amp;\n</code></pre> <p>You might see something like:</p> <pre><code>X.Org X Server 1.18.4\n...\n(==) Log file: \"/var/log/Xorg.0.log\", Time: Thu Oct 11 21:10:38 2018\n(==) Using config file: \"/etc/X11/xorg.conf\"\n(==) Using system config directory \"/usr/share/X11/xorg.conf.d\"\n(EE)\nFatal server error:\n(EE) no screens found(EE)\n(EE)\nPlease consult the X.Org Foundation support\n         at http://wiki.x.org\n for help.\n(EE) Please also check the log file at \"/var/log/Xorg.0.log\" for additional information.\n(EE)\n(EE) Server terminated with error (1). Closing log file.\n</code></pre> <p>And when you execute:</p> <pre><code>nvidia-smi\n</code></pre> <p>You might see something like:</p> <pre><code>NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n</code></pre> <p>This means the NVIDIA's driver needs to be updated. Refer to this section for more information.</p>","text":""},{"location":"Training-on-Microsoft-Azure/","title":"Training on Microsoft Azure (works with ML-Agents Toolkit v0.3)","text":"<p>:warning: Note: We no longer use this guide ourselves and so it may not work correctly. We've decided to keep it up just in case it is helpful to you.</p> <p>This page contains instructions for setting up training on Microsoft Azure through either Azure Container Instances or Virtual Machines. Non \"headless\" training has not yet been tested to verify support.</p>"},{"location":"Training-on-Microsoft-Azure/#pre-configured-azure-virtual-machine","title":"Pre-Configured Azure Virtual Machine","text":"<p>A pre-configured virtual machine image is available in the Azure Marketplace and is nearly completely ready for training. You can start by deploying the Data Science Virtual Machine for Linux (Ubuntu) into your Azure subscription.</p> <p>Note that, if you choose to deploy the image to an N-Series GPU optimized VM, training will, by default, run on the GPU. If you choose any other type of VM, training will run on the CPU.</p>"},{"location":"Training-on-Microsoft-Azure/#configuring-your-own-instance","title":"Configuring your own Instance","text":"<p>Setting up your own instance requires a number of package installations. Please view the documentation for doing so here.</p>"},{"location":"Training-on-Microsoft-Azure/#installing-ml-agents","title":"Installing ML-Agents","text":"<ol> <li>Move    the <code>ml-agents</code> sub-folder of this ml-agents repo to the remote Azure    instance, and set it as the working directory.</li> <li>Install the required packages:    Torch: <code>pip3 install torch==1.7.0 -f https://download.pytorch.org/whl/torch_stable.html</code> and    MLAgents: <code>python -m pip install mlagents==1.0.0</code></li> </ol>"},{"location":"Training-on-Microsoft-Azure/#testing","title":"Testing","text":"<p>To verify that all steps worked correctly:</p> <ol> <li>In the Unity Editor, load a project containing an ML-Agents environment (you    can use one of the example environments if you have not created your own).</li> <li>Open the Build Settings window (menu: File &gt; Build Settings).</li> <li>Select Linux as the Target Platform, and x86_64 as the target architecture.</li> <li>Check Headless Mode.</li> <li>Click Build to build the Unity environment executable.</li> <li>Upload the resulting files to your Azure instance.</li> <li>Test the instance setup from Python using:</li> </ol> <pre><code>from mlagents_envs.environment import UnityEnvironment\n\nenv = UnityEnvironment(file_name=\"&lt;your_env&gt;\", seed=1, side_channels=[])\n</code></pre> <p>Where <code>&lt;your_env&gt;</code> corresponds to the path to your environment executable (i.e. <code>/home/UserName/Build/yourFile</code>).</p> <p>You should receive a message confirming that the environment was loaded successfully.</p> <p>Note: When running your environment in headless mode, you must append <code>--no-graphics</code> to your mlagents-learn command, as it won't train otherwise. You can test this simply by aborting a training and check if it says \"Model Saved\" or \"Aborted\", or see if it generated the .onnx in the result folder.</p>"},{"location":"Training-on-Microsoft-Azure/#running-training-on-your-virtual-machine","title":"Running Training on your Virtual Machine","text":"<p>To run your training on the VM:</p> <ol> <li>Move    your built Unity application to your Virtual Machine.</li> <li>Set the directory where the ML-Agents Toolkit was installed to your working    directory.</li> <li>Run the following command:</li> </ol> <pre><code>mlagents-learn &lt;trainer_config&gt; --env=&lt;your_app&gt; --run-id=&lt;run_id&gt; --train\n</code></pre> <p>Where <code>&lt;your_app&gt;</code> is the path to your app (i.e. <code>~/unity-volume/3DBallHeadless</code>) and <code>&lt;run_id&gt;</code> is an identifier you would like to identify your training run with.</p> <p>If you've selected to run on a N-Series VM with GPU support, you can verify that the GPU is being used by running <code>nvidia-smi</code> from the command line.</p>"},{"location":"Training-on-Microsoft-Azure/#monitoring-your-training-run-with-tensorboard","title":"Monitoring your Training Run with TensorBoard","text":"<p>Once you have started training, you can use TensorBoard to observe the training.</p> <ol> <li> <p>Start by    opening the appropriate port for web traffic to connect to your VM.</p> </li> <li> <p>Note that you don't need to generate a new <code>Network Security Group</code> but      instead, go to the Networking tab under Settings for your VM.</p> </li> <li> <p>As an example, you could use the following settings to open the Port with      the following Inbound Rule settings:</p> <ul> <li>Source: Any</li> <li>Source Port Ranges: *</li> <li>Destination: Any</li> <li>Destination Port Ranges: 6006</li> <li>Protocol: Any</li> <li>Action: Allow</li> <li>Priority: (Leave as default)</li> </ul> </li> <li> <p>Unless you started the training as a background process, connect to your VM    from another terminal instance.</p> </li> <li>Run the following command from your terminal    <code>tensorboard --logdir results --host 0.0.0.0</code></li> <li>You should now be able to open a browser and navigate to    <code>&lt;Your_VM_IP_Address&gt;:6060</code> to view the TensorBoard report.</li> </ol>"},{"location":"Training-on-Microsoft-Azure/#running-on-azure-container-instances","title":"Running on Azure Container Instances","text":"<p>Azure Container Instances allow you to spin up a container, on demand, that will run your training and then be shut down. This ensures you aren't leaving a billable VM running when it isn't needed. Using ACI enables you to offload training of your models without needing to install Python and TensorFlow on your own computer.</p>"},{"location":"Training-on-Microsoft-Azure/#custom-instances","title":"Custom Instances","text":"<p>This page contains instructions for setting up a custom Virtual Machine on Microsoft Azure so you can running ML-Agents training in the cloud.</p> <ol> <li>Start by    deploying an Azure VM    with Ubuntu Linux (tests were done with 16.04 LTS). To use GPU support, use a    N-Series VM.</li> <li>SSH into your VM.</li> <li>Start with the following commands to install the Nvidia driver:</li> </ol> <p>```sh    wget http://us.download.nvidia.com/tesla/375.66/nvidia-diag-driver-local-repo-ubuntu1604_375.66-1_amd64.deb</p> <p>sudo dpkg -i nvidia-diag-driver-local-repo-ubuntu1604_375.66-1_amd64.deb</p> <p>sudo apt-get update</p> <p>sudo apt-get install cuda-drivers</p> <p>sudo reboot    ```</p> <ol> <li>After a minute you should be able to reconnect to your VM and install the    CUDA toolkit:</li> </ol> <p>```sh    wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_8.0.61-1_amd64.deb</p> <p>sudo dpkg -i cuda-repo-ubuntu1604_8.0.61-1_amd64.deb</p> <p>sudo apt-get update</p> <p>sudo apt-get install cuda-8-0    ```</p> <ol> <li> <p>You'll next need to download cuDNN from the Nvidia developer site. This    requires a registered account.</p> </li> <li> <p>Navigate to http://developer.nvidia.com and    create an account and verify it.</p> </li> <li> <p>Download (to your own computer) cuDNN from    this url.</p> </li> <li> <p>Copy the deb package to your VM:</p> </li> </ol> <p><code>sh    scp libcudnn6_6.0.21-1+cuda8.0_amd64.deb &lt;VMUserName&gt;@&lt;VMIPAddress&gt;:libcudnn6_6.0.21-1+cuda8.0_amd64.deb</code></p> <ol> <li>SSH back to your VM and execute the following:</li> </ol> <p>```console    sudo dpkg -i libcudnn6_6.0.21-1+cuda8.0_amd64.deb</p> <p>export LD_LIBRARY_PATH=/usr/local/cuda/lib64/:/usr/lib/x86_64-linux-gnu/:$LD_LIBRARY_PATH    . ~/.profile</p> <p>sudo reboot    ```</p> <ol> <li> <p>After a minute, you should be able to SSH back into your VM. After doing so,     run the following:</p> <p><code>sh sudo apt install python-pip sudo apt install python3-pip</code></p> </li> <li> <p>At this point, you need to install TensorFlow. The version you install     should be tied to if you are using GPU to train:</p> <p><code>sh pip3 install tensorflow-gpu==1.4.0 keras==2.0.6</code></p> <p>Or CPU to train:</p> <p><code>sh pip3 install tensorflow==1.4.0 keras==2.0.6</code></p> </li> <li> <p>You'll then need to install additional dependencies:</p> <p><code>sh pip3 install pillow pip3 install numpy</code></p> </li> </ol>"},{"location":"Tutorial-Custom-Trainer-Plugin/","title":"Custom Trainer Plugin","text":""},{"location":"Tutorial-Custom-Trainer-Plugin/#how-to-write-a-custom-trainer-plugin","title":"How to write a custom trainer plugin","text":""},{"location":"Tutorial-Custom-Trainer-Plugin/#step-1-write-your-custom-trainer-class","title":"Step 1: Write your custom trainer class","text":"<p>Before you start writing your code, make sure to use your favorite environment management tool(e.g. <code>venv</code> or <code>conda</code>) to create and activate a Python virtual environment. The following command uses <code>conda</code>, but other tools work similarly:</p> <pre><code>conda create -n trainer-env python=3.10.12\nconda activate trainer-env\n</code></pre> <p>Users of the plug-in system are responsible for implementing the trainer class subject to the API standard. Let us follow an example by implementing a custom trainer named \"YourCustomTrainer\". You can either extend <code>OnPolicyTrainer</code> or <code>OffPolicyTrainer</code> classes depending on the training strategies you choose.</p> <p>Please refer to the internal PPO implementation for a complete code example. We will not provide a workable code in the document. The purpose of the tutorial is to introduce you to the core components and interfaces of our plugin framework. We use code snippets and patterns to demonstrate the control and data flow.</p> <p>Your custom trainers are responsible for collecting experiences and training the models. Your custom trainer class acts like a co-ordinator to the policy and optimizer. To start implementing methods in the class, create a policy class objects from method <code>create_policy</code>:</p> <pre><code>def create_policy(\n    self, parsed_behavior_id: BehaviorIdentifiers, behavior_spec: BehaviorSpec\n) -&gt; TorchPolicy:\n\n    actor_cls: Union[Type[SimpleActor], Type[SharedActorCritic]] = SimpleActor\n    actor_kwargs: Dict[str, Any] = {\n        \"conditional_sigma\": False,\n        \"tanh_squash\": False,\n    }\n    if self.shared_critic:\n        reward_signal_configs = self.trainer_settings.reward_signals\n        reward_signal_names = [\n            key.value for key, _ in reward_signal_configs.items()\n        ]\n        actor_cls = SharedActorCritic\n        actor_kwargs.update({\"stream_names\": reward_signal_names})\n\n    policy = TorchPolicy(\n        self.seed,\n        behavior_spec,\n        self.trainer_settings.network_settings,\n        actor_cls,\n        actor_kwargs,\n    )\n    return policy\n\n</code></pre> <p>Depending on whether you use shared or separate network architecture for your policy, we provide <code>SimpleActor</code> and <code>SharedActorCritic</code> from <code>mlagents.trainers.torch_entities.networks</code> that you can choose from. In our example above, we use a <code>SimpleActor</code>.</p> <p>Next, create an optimizer class object from <code>create_optimizer</code> method and connect it to the policy object you created above:</p> <pre><code>def create_optimizer(self) -&gt; TorchOptimizer:\n    return TorchPPOOptimizer(  # type: ignore\n        cast(TorchPolicy, self.policy), self.trainer_settings  # type: ignore\n    )  # type: ignore\n\n</code></pre> <p>There are a couple of abstract methods(<code>_process_trajectory</code> and <code>_update_policy</code>) inherited from <code>RLTrainer</code> that you need to implement in your custom trainer class. <code>_process_trajectory</code> takes a trajectory and processes it, putting it into the update buffer. Processing involves calculating value and advantage targets for the model updating step. Given input <code>trajectory: Trajectory</code>, users are responsible for processing the data in the trajectory and append <code>agent_buffer_trajectory</code> to the back of the update buffer by calling <code>self._append_to_update_buffer(agent_buffer_trajectory)</code>, whose output will be used in updating the model in <code>optimizer</code> class.</p> <p>A typical <code>_process_trajectory</code> function(incomplete) will convert a trajectory object to an agent buffer then get all value estimates from the trajectory by calling <code>self.optimizer.get_trajectory_value_estimates</code>. From the returned dictionary of value estimates we extract reward signals keyed by their names:</p> <pre><code>def _process_trajectory(self, trajectory: Trajectory) -&gt; None:\n    super()._process_trajectory(trajectory)\n    agent_id = trajectory.agent_id  # All the agents should have the same ID\n\n    agent_buffer_trajectory = trajectory.to_agentbuffer()\n\n    # Get all value estimates\n    (\n        value_estimates,\n        value_next,\n        value_memories,\n    ) =  self.optimizer.get_trajectory_value_estimates(\n        agent_buffer_trajectory,\n        trajectory.next_obs,\n        trajectory.done_reached and not trajectory.interrupted,\n    )\n\n    for name, v in value_estimates.items():\n        agent_buffer_trajectory[RewardSignalUtil.value_estimates_key(name)].extend(\n            v\n        )\n        self._stats_reporter.add_stat(\n            f\"Policy/{self.optimizer.reward_signals[name].name.capitalize()} Value Estimate\",\n            np.mean(v),\n        )\n\n    # Evaluate all reward functions\n    self.collected_rewards[\"environment\"][agent_id] += np.sum(\n        agent_buffer_trajectory[BufferKey.ENVIRONMENT_REWARDS]\n    )\n    for name, reward_signal in self.optimizer.reward_signals.items():\n        evaluate_result = (\n            reward_signal.evaluate(agent_buffer_trajectory) * reward_signal.strength\n        )\n        agent_buffer_trajectory[RewardSignalUtil.rewards_key(name)].extend(\n            evaluate_result\n        )\n        # Report the reward signals\n        self.collected_rewards[name][agent_id] += np.sum(evaluate_result)\n\n    self._append_to_update_buffer(agent_buffer_trajectory)\n\n</code></pre> <p>A trajectory will be a list of dictionaries of strings mapped to <code>Anything</code>. When calling <code>forward</code> on a policy, the argument will include an \u201cexperience\u201d dictionary from the last step. The <code>forward</code> method will generate an action and the next \u201cexperience\u201d dictionary. Examples of fields in the \u201cexperience\u201d dictionary include observation, action, reward, done status, group_reward, LSTM memory state, etc.</p>"},{"location":"Tutorial-Custom-Trainer-Plugin/#step-2-implement-your-custom-optimizer-for-the-trainer","title":"Step 2: implement your custom optimizer for the trainer.","text":"<p>We will show you an example we implemented - <code>class TorchPPOOptimizer(TorchOptimizer)</code>, which takes a Policy and a Dict of trainer parameters and creates an Optimizer that connects to the policy. Your optimizer should include a value estimator and a loss function in the <code>update</code> method.</p> <p>Before writing your optimizer class, first define setting class <code>class PPOSettings(OnPolicyHyperparamSettings)</code> for your custom optimizer:</p> <pre><code>class PPOSettings(OnPolicyHyperparamSettings):\n    beta: float = 5.0e-3\n    epsilon: float = 0.2\n    lambd: float = 0.95\n    num_epoch: int = 3\n    shared_critic: bool = False\n    learning_rate_schedule: ScheduleType = ScheduleType.LINEAR\n    beta_schedule: ScheduleType = ScheduleType.LINEAR\n    epsilon_schedule: ScheduleType = ScheduleType.LINEAR\n\n</code></pre> <p>You should implement <code>update</code> function following interface:</p> <pre><code>def update(self, batch: AgentBuffer, num_sequences: int) -&gt; Dict[str, float]:\n\n</code></pre> <p>In which losses and other metrics are calculated from an <code>AgentBuffer</code> that is generated from your trainer class, depending on which model you choose to implement the loss functions will be different. In our case we calculate value loss from critic and trust region policy loss. A typical pattern(incomplete) of the calculations will look like the following:</p> <pre><code>run_out = self.policy.actor.get_stats(\n    current_obs,\n    actions,\n    masks=act_masks,\n    memories=memories,\n    sequence_length=self.policy.sequence_length,\n)\n\nlog_probs = run_out[\"log_probs\"]\nentropy = run_out[\"entropy\"]\n\nvalues, _ = self.critic.critic_pass(\n    current_obs,\n    memories=value_memories,\n    sequence_length=self.policy.sequence_length,\n)\npolicy_loss = ModelUtils.trust_region_policy_loss(\n    ModelUtils.list_to_tensor(batch[BufferKey.ADVANTAGES]),\n    log_probs,\n    old_log_probs,\n    loss_masks,\n    decay_eps,\n)\nloss = (\n    policy_loss\n    + 0.5 * value_loss\n    - decay_bet * ModelUtils.masked_mean(entropy, loss_masks)\n)\n\n</code></pre> <p>Finally update the model and return the a dictionary including calculated losses and updated decay learning rate:</p> <pre><code>ModelUtils.update_learning_rate(self.optimizer, decay_lr)\nself.optimizer.zero_grad()\nloss.backward()\n\nself.optimizer.step()\nupdate_stats = {\n    \"Losses/Policy Loss\": torch.abs(policy_loss).item(),\n    \"Losses/Value Loss\": value_loss.item(),\n    \"Policy/Learning Rate\": decay_lr,\n    \"Policy/Epsilon\": decay_eps,\n    \"Policy/Beta\": decay_bet,\n}\n\n</code></pre>"},{"location":"Tutorial-Custom-Trainer-Plugin/#step-3-integrate-your-custom-trainer-into-the-plugin-system","title":"Step 3: Integrate your custom trainer into the plugin system","text":"<p>By integrating a custom trainer into the plugin system, a user can use their published packages which have their implementations. To do that, you need to add a setup.py file. In the call to setup(), you'll need to add to the entry_points dictionary for each plugin interface that you implement. The form of this is {entry point name}={plugin module}:{plugin function}. For example:</p> <pre><code>entry_points={\n        ML_AGENTS_TRAINER_TYPE: [\n            \"your_trainer_type=your_package.your_custom_trainer:get_type_and_setting\"\n        ]\n    },\n</code></pre> <p>Some key elements in the code:</p> <pre><code>ML_AGENTS_TRAINER_TYPE: a string constant for trainer type\nyour_trainer_type: name your trainer type, used in configuration file\nyour_package: your pip installable package containing custom trainer implementation\n</code></pre> <p>Also define <code>get_type_and_setting</code> method in <code>YourCustomTrainer</code> class:</p> <pre><code>def get_type_and_setting():\n    return {YourCustomTrainer.get_trainer_name(): YourCustomTrainer}, {\n        YourCustomTrainer.get_trainer_name(): YourCustomSetting\n    }\n\n</code></pre> <p>Finally, specify trainer type in the config file:</p> <pre><code>behaviors:\n  3DBall:\n    trainer_type: your_trainer_type\n...\n</code></pre>"},{"location":"Tutorial-Custom-Trainer-Plugin/#step-4-install-your-custom-trainer-and-run-training","title":"Step 4: Install your custom trainer and run training:","text":"<p>Before installing your custom trainer package, make sure you have <code>ml-agents-env</code> and <code>ml-agents</code> installed</p> <pre><code>pip3 install -e ./ml-agents-envs &amp;&amp; pip3 install -e ./ml-agents\n</code></pre> <p>Install your cutom trainer package(if your package is pip installable):</p> <pre><code>pip3 install your_custom_package\n</code></pre> <p>Or follow our internal implementations:</p> <pre><code>pip3 install -e ./ml-agents-trainer-plugin\n</code></pre> <p>Following the previous installations your package is added as an entrypoint and you can use a config file with new trainers:</p> <pre><code>mlagents-learn ml-agents-trainer-plugin/mlagents_trainer_plugin/a2c/a2c_3DBall.yaml --run-id &lt;run-id-name&gt;\n--env &lt;env-executable&gt;\n</code></pre>"},{"location":"Tutorial-Custom-Trainer-Plugin/#validate-your-implementations","title":"Validate your implementations:","text":"<p>Create a clean Python environment with Python 3.10.12 and activate it before you start, if you haven't done so already:</p> <pre><code>conda create -n trainer-env python=3.10.12\nconda activate trainer-env\n</code></pre> <p>Make sure you follow previous steps and install all required packages. We are testing internal implementations in this tutorial, but ML-Agents users can run similar validations once they have their own implementations installed:</p> <pre><code>pip3 install -e ./ml-agents-envs &amp;&amp; pip3 install -e ./ml-agents\npip3 install -e ./ml-agents-trainer-plugin\n</code></pre> <p>Once your package is added as an <code>entrypoint</code>, you can add to the config file the new trainer type. Check if trainer type is specified in the config file <code>a2c_3DBall.yaml</code>:</p> <pre><code>trainer_type: a2c\n</code></pre> <p>Test if custom trainer package is installed by running:</p> <pre><code>mlagents-learn ml-agents-trainer-plugin/mlagents_trainer_plugin/a2c/a2c_3DBall.yaml --run-id test-trainer\n</code></pre> <p>You can also list all trainers installed in the registry. Type <code>python</code> in your shell to open a REPL session. Run the python code below, you should be able to see all trainer types currently installed:</p> <pre><code>&gt;&gt;&gt; import pkg_resources\n&gt;&gt;&gt; for entry in pkg_resources.iter_entry_points('mlagents.trainer_type'):\n...     print(entry)\n...\ndefault = mlagents.plugins.trainer_type:get_default_trainer_types\na2c = mlagents_trainer_plugin.a2c.a2c_trainer:get_type_and_setting\ndqn = mlagents_trainer_plugin.dqn.dqn_trainer:get_type_and_setting\n</code></pre> <p>If it is properly installed, you will see Unity logo and message indicating training will start:</p> <pre><code>[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.\n</code></pre> <p>If you see the following error message, it could be due to trainer type is wrong or the trainer type specified is not installed:</p> <pre><code>mlagents.trainers.exception.TrainerConfigError: Invalid trainer type a2c was found\n</code></pre>"},{"location":"Unity-Environment-Registry/","title":"Unity Environment Registry [Experimental]","text":"<p>The Unity Environment Registry is a database of pre-built Unity environments that can be easily used without having to install the Unity Editor. It is a great way to get started with our UnityEnvironment API.</p>"},{"location":"Unity-Environment-Registry/#loading-an-environment-from-the-registry","title":"Loading an Environment from the Registry","text":"<p>To get started, you can access the default registry we provide with our Example Environments. The Unity Environment Registry implements a Mapping, therefore, you can access an entry with its identifier with the square brackets <code>[ ]</code>. Use the following code to list all of the environment identifiers present in the default registry:</p> <pre><code>from mlagents_envs.registry import default_registry\n\nenvironment_names = list(default_registry.keys())\nfor name in environment_names:\n   print(name)\n</code></pre> <p>The <code>make()</code> method on a registry value will return a <code>UnityEnvironment</code> ready to be used. All arguments passed to the make method will be passed to the constructor of the <code>UnityEnvironment</code> as well. Refer to the documentation on the Python-API for more information about the arguments of the <code>UnityEnvironment</code> constructor. For example, the following code will create the environment under the identifier <code>\"my-env\"</code>, reset it, perform a few steps and finally close it:</p> <pre><code>from mlagents_envs.registry import default_registry\n\nenv = default_registry[\"my-env\"].make()\nenv.reset()\nfor _ in range(10):\n  env.step()\nenv.close()\n</code></pre>"},{"location":"Unity-Environment-Registry/#create-and-share-your-own-registry","title":"Create and share your own registry","text":"<p>In order to share the <code>UnityEnvironemnt</code> you created, you must :  - Create a Unity executable of your environment for each platform (Linux, OSX and/or Windows)  - Place each executable in a <code>zip</code> compressed folder  - Upload each zip file online to your preferred hosting platform  - Create a <code>yaml</code> file that will contain the description and path to your environment  - Upload the <code>yaml</code> file online The <code>yaml</code> file must have the following format :</p> <pre><code>environments:\n  - &lt;environment-identifier&gt;:\n     expected_reward: &lt;expected-reward-float&gt;\n     description: &lt;description-of-the-environment&gt;\n     linux_url: &lt;url-to-the-linux-zip-folder&gt;\n     darwin_url: &lt;url-to-the-osx-zip-folder&gt;\n     win_url: &lt;url-to-the-windows-zip-folder&gt;\n     additional_args:\n      - &lt;an-optional-list-of-command-line-arguments-for-the-executable&gt;\n      - ...\n</code></pre> <p>Your users can now use your environment with the following code :</p> <pre><code>from mlagents_envs.registry import UnityEnvRegistry\n\nregistry = UnityEnvRegistry()\nregistry.register_from_yaml(\"url-or-path-to-your-yaml-file\")\n</code></pre> <p>Note: The <code>\"url-or-path-to-your-yaml-file\"</code> can be either a url or a local path.</p>"},{"location":"Using-Docker/","title":"Using Docker For ML-Agents (Deprecated)","text":"<p>:warning: Note: We no longer use this guide ourselves and so it may not work correctly. We've decided to keep it up just in case it is helpful to you.</p> <p>We currently offer a solution for Windows and Mac users who would like to do training or inference using Docker. This option may be appealing to those who would like to avoid installing Python and TensorFlow themselves. The current setup forces both TensorFlow and Unity to only rely on the CPU for computations. Consequently, our Docker simulation does not use a GPU and uses <code>Xvfb</code> to do visual rendering. <code>Xvfb</code> is a utility that enables <code>ML-Agents</code> (or any other application) to do rendering virtually i.e. it does not assume that the machine running <code>ML-Agents</code> has a GPU or a display attached to it. This means that rich environments which involve agents using camera-based visual observations might be slower.</p>"},{"location":"Using-Docker/#requirements","title":"Requirements","text":"<ul> <li>Docker</li> <li>Unity Linux Build Support Component. Make sure to select the Linux Build   Support component when installing Unity.</li> </ul>"},{"location":"Using-Docker/#setup","title":"Setup","text":"<ul> <li> <p>Download the Unity Installer and add   the Linux Build Support Component</p> </li> <li> <p>Download and install   Docker if you don't have it setup on your machine.</p> </li> <li> <p>Since Docker runs a container in an environment that is isolated from the host   machine, a mounted directory in your host machine is used to share data, e.g.   the trainer configuration file, Unity executable and   TensorFlow graph. For convenience, we created an empty <code>unity-volume</code>   directory at the root of the repository for this purpose, but feel free to use   any other directory. The remainder of this guide assumes that the   <code>unity-volume</code> directory is the one used.</p> </li> </ul>"},{"location":"Using-Docker/#usage","title":"Usage","text":"<p>Using Docker for ML-Agents involves three steps: building the Unity environment with specific flags, building a Docker container and, finally, running the container. If you are not familiar with building a Unity environment for ML-Agents, please read through our Getting Started with the 3D Balance Ball Example guide first.</p>"},{"location":"Using-Docker/#build-the-environment-optional","title":"Build the Environment (Optional)","text":"<p>If you want to used the Editor to perform training, you can skip this step.</p> <p>Since Docker typically runs a container sharing a (linux) kernel with the host machine, the Unity environment has to be built for the linux platform. When building a Unity environment, please select the following options from the the Build Settings window:</p> <ul> <li>Set the Target Platform to <code>Linux</code></li> <li>Set the Architecture to <code>x86_64</code></li> </ul> <p>Then click <code>Build</code>, pick an environment name (e.g. <code>3DBall</code>) and set the output directory to <code>unity-volume</code>. After building, ensure that the file <code>&lt;environment-name&gt;.x86_64</code> and subdirectory <code>&lt;environment-name&gt;_Data/</code> are created under <code>unity-volume</code>.</p> <p></p>"},{"location":"Using-Docker/#build-the-docker-container","title":"Build the Docker Container","text":"<p>First, make sure the Docker engine is running on your machine. Then build the Docker container by calling the following command at the top-level of the repository:</p> <pre><code>docker build -t &lt;image-name&gt; .\n</code></pre> <p>Replace <code>&lt;image-name&gt;</code> with a name for the Docker image, e.g. <code>balance.ball.v0.1</code>.</p>"},{"location":"Using-Docker/#run-the-docker-container","title":"Run the Docker Container","text":"<p>Run the Docker container by calling the following command at the top-level of the repository:</p> <pre><code>docker run -it --name &lt;container-name&gt; \\\n           --mount type=bind,source=\"$(pwd)\"/unity-volume,target=/unity-volume \\\n           -p 5005:5005 \\\n           -p 6006:6006 \\\n           &lt;image-name&gt;:latest \\\n           &lt;trainer-config-file&gt; \\\n           --env=&lt;environment-name&gt; \\\n           --train \\\n           --run-id=&lt;run-id&gt;\n</code></pre> <p>Notes on argument values:</p> <ul> <li><code>&lt;container-name&gt;</code> is used to identify the container (in case you want to   interrupt and terminate it). This is optional and Docker will generate a   random name if this is not set. Note that this must be unique for every run   of a Docker image.</li> <li><code>&lt;image-name&gt;</code> references the image name used when building the container.</li> <li><code>&lt;environment-name&gt;</code> (Optional): If you are training with a linux   executable, this is the name of the executable. If you are training in the   Editor, do not pass a <code>&lt;environment-name&gt;</code> argument and press the Play   button in Unity when the message \"Start training by pressing the Play button   in the Unity Editor\" is displayed on the screen.</li> <li><code>source</code>: Reference to the path in your host OS where you will store the Unity   executable.</li> <li><code>target</code>: Tells Docker to mount the <code>source</code> path as a disk with this name.</li> <li><code>trainer-config-file</code>, <code>train</code>, <code>run-id</code>: ML-Agents arguments passed to   <code>mlagents-learn</code>. <code>trainer-config-file</code> is the filename of the trainer config   file, <code>train</code> trains the algorithm, and <code>run-id</code> is used to tag each   experiment with a unique identifier. We recommend placing the trainer-config   file inside <code>unity-volume</code> so that the container has access to the file.</li> </ul> <p>To train with a <code>3DBall</code> environment executable, the command would be:</p> <pre><code>docker run -it --name 3DBallContainer.first.trial \\\n           --mount type=bind,source=\"$(pwd)\"/unity-volume,target=/unity-volume \\\n           -p 5005:5005 \\\n           -p 6006:6006 \\\n           balance.ball.v0.1:latest 3DBall \\\n           /unity-volume/trainer_config.yaml \\\n           --env=/unity-volume/3DBall \\\n           --train \\\n           --run-id=3dball_first_trial\n</code></pre> <p>For more detail on Docker mounts, check out these docs from Docker.</p> <p>NOTE If you are training using docker for environments that use visual observations, you may need to increase the default memory that Docker allocates for the container. For example, see here for instructions for Docker for Mac.</p>"},{"location":"Using-Docker/#running-tensorboard","title":"Running Tensorboard","text":"<p>You can run Tensorboard to monitor your training instance on http://localhost:6006:</p> <pre><code>docker exec -it &lt;container-name&gt; tensorboard --logdir /unity-volume/results --host 0.0.0.0\n</code></pre> <p>With our previous 3DBall example, this command would look like this:</p> <pre><code>docker exec -it 3DBallContainer.first.trial tensorboard --logdir /unity-volume/results --host 0.0.0.0\n</code></pre> <p>For more details on Tensorboard, check out the documentation about Using Tensorboard.</p>"},{"location":"Using-Docker/#stopping-container-and-saving-state","title":"Stopping Container and Saving State","text":"<p>If you are satisfied with the training progress, you can stop the Docker container while saving state by either using <code>Ctrl+C</code> or <code>\u2318+C</code> (Mac) or by using the following command:</p> <pre><code>docker kill --signal=SIGINT &lt;container-name&gt;\n</code></pre> <p><code>&lt;container-name&gt;</code> is the name of the container specified in the earlier <code>docker run</code> command. If you didn't specify one, you can find the randomly generated identifier by running <code>docker container ls</code>.</p>"},{"location":"Using-Tensorboard/","title":"Using TensorBoard to Observe Training","text":"<p>The ML-Agents Toolkit saves statistics during learning session that you can view with a TensorFlow utility named, TensorBoard.</p> <p>The <code>mlagents-learn</code> command saves training statistics to a folder named <code>results</code>, organized by the <code>run-id</code> value you assign to a training session.</p> <p>In order to observe the training process, either during training or afterward, start TensorBoard:</p> <ol> <li>Open a terminal or console window:</li> <li>Navigate to the directory where the ML-Agents Toolkit is installed.</li> <li>From the command line run: <code>tensorboard --logdir results --port 6006</code></li> <li>Open a browser window and navigate to    localhost:6006.</li> </ol> <p>Note: The default port TensorBoard uses is 6006. If there is an existing session running on port 6006 a new session can be launched on an open port using the --port option.</p> <p>Note: If you don't assign a <code>run-id</code> identifier, <code>mlagents-learn</code> uses the default string, \"ppo\". You can delete the folders under the <code>results</code> directory to clear out old statistics.</p> <p>On the left side of the TensorBoard window, you can select which of the training runs you want to display. You can select multiple run-ids to compare statistics. The TensorBoard window also provides options for how to display and smooth graphs.</p>"},{"location":"Using-Tensorboard/#the-ml-agents-toolkit-training-statistics","title":"The ML-Agents Toolkit training statistics","text":"<p>The ML-Agents training program saves the following statistics:</p> <p></p>"},{"location":"Using-Tensorboard/#environment-statistics","title":"Environment Statistics","text":"<ul> <li> <p><code>Environment/Lesson</code> - Plots the progress from lesson to lesson. Only   interesting when performing curriculum training.</p> </li> <li> <p><code>Environment/Cumulative Reward</code> - The mean cumulative episode reward over all   agents. Should increase during a successful training session.</p> </li> <li> <p><code>Environment/Episode Length</code> - The mean length of each episode in the   environment for all agents.</p> </li> </ul>"},{"location":"Using-Tensorboard/#is-training","title":"Is Training","text":"<ul> <li><code>Is Training</code> - A boolean indicating if the agent is updating its model.</li> </ul>"},{"location":"Using-Tensorboard/#policy-statistics","title":"Policy Statistics","text":"<ul> <li> <p><code>Policy/Entropy</code> (PPO; SAC) - How random the decisions of the model are.   Should slowly decrease during a successful training process. If it decreases   too quickly, the <code>beta</code> hyperparameter should be increased.</p> </li> <li> <p><code>Policy/Learning Rate</code> (PPO; SAC) - How large a step the training algorithm   takes as it searches for the optimal policy. Should decrease over time.</p> </li> <li> <p><code>Policy/Entropy Coefficient</code> (SAC) - Determines the relative importance of the   entropy term. This value is adjusted automatically so that the agent retains   some amount of randomness during training.</p> </li> <li> <p><code>Policy/Extrinsic Reward</code> (PPO; SAC) - This corresponds to the mean cumulative   reward received from the environment per-episode.</p> </li> <li> <p><code>Policy/Value Estimate</code> (PPO; SAC) - The mean value estimate for all states   visited by the agent. Should increase during a successful training session.</p> </li> <li> <p><code>Policy/Curiosity Reward</code> (PPO/SAC+Curiosity) - This corresponds to the mean   cumulative intrinsic reward generated per-episode.</p> </li> <li> <p><code>Policy/Curiosity Value Estimate</code> (PPO/SAC+Curiosity) - The agent's value   estimate for the curiosity reward.</p> </li> <li> <p><code>Policy/GAIL Reward</code> (PPO/SAC+GAIL) - This corresponds to the mean cumulative   discriminator-based reward generated per-episode.</p> </li> <li> <p><code>Policy/GAIL Value Estimate</code> (PPO/SAC+GAIL) - The agent's value estimate for   the GAIL reward.</p> </li> <li> <p><code>Policy/GAIL Policy Estimate</code> (PPO/SAC+GAIL) - The discriminator's estimate   for states and actions generated by the policy.</p> </li> <li> <p><code>Policy/GAIL Expert Estimate</code> (PPO/SAC+GAIL) - The discriminator's estimate   for states and actions drawn from expert demonstrations.</p> </li> </ul>"},{"location":"Using-Tensorboard/#learning-loss-functions","title":"Learning Loss Functions","text":"<ul> <li> <p><code>Losses/Policy Loss</code> (PPO; SAC) - The mean magnitude of policy loss function.   Correlates to how much the policy (process for deciding actions) is changing.   The magnitude of this should decrease during a successful training session.</p> </li> <li> <p><code>Losses/Value Loss</code> (PPO; SAC) - The mean loss of the value function update.   Correlates to how well the model is able to predict the value of each state.   This should increase while the agent is learning, and then decrease once the   reward stabilizes.</p> </li> <li> <p><code>Losses/Forward Loss</code> (PPO/SAC+Curiosity) - The mean magnitude of the forward   model loss function. Corresponds to how well the model is able to predict the   new observation encoding.</p> </li> <li> <p><code>Losses/Inverse Loss</code> (PPO/SAC+Curiosity) - The mean magnitude of the inverse   model loss function. Corresponds to how well the model is able to predict the   action taken between two observations.</p> </li> <li> <p><code>Losses/Pretraining Loss</code> (BC) - The mean magnitude of the behavioral cloning   loss. Corresponds to how well the model imitates the demonstration data.</p> </li> <li> <p><code>Losses/GAIL Loss</code> (GAIL) - The mean magnitude of the GAIL discriminator loss.   Corresponds to how well the model imitates the demonstration data.</p> </li> </ul>"},{"location":"Using-Tensorboard/#self-play","title":"Self-Play","text":"<ul> <li><code>Self-Play/ELO</code> (Self-Play) -   ELO measures the relative   skill level between two players. In a proper training run, the ELO of the   agent should steadily increase.</li> </ul>"},{"location":"Using-Tensorboard/#exporting-data-from-tensorboard","title":"Exporting Data from TensorBoard","text":"<p>To export timeseries data in CSV or JSON format, check the \"Show data download links\" in the upper left. This will enable download links below each chart.</p> <p></p>"},{"location":"Using-Tensorboard/#custom-metrics-from-unity","title":"Custom Metrics from Unity","text":"<p>To get custom metrics from a C# environment into TensorBoard, you can use the <code>StatsRecorder</code>:</p> <pre><code>var statsRecorder = Academy.Instance.StatsRecorder;\nstatsRecorder.Add(\"MyMetric\", 1.0);\n</code></pre>"},{"location":"Using-Virtual-Environment/","title":"Using Virtual Environment","text":""},{"location":"Using-Virtual-Environment/#what-is-a-virtual-environment","title":"What is a Virtual Environment?","text":"<p>A Virtual Environment is a self contained directory tree that contains a Python installation for a particular version of Python, plus a number of additional packages. To learn more about Virtual Environments see here.</p>"},{"location":"Using-Virtual-Environment/#why-should-i-use-a-virtual-environment","title":"Why should I use a Virtual Environment?","text":"<p>A Virtual Environment keeps all dependencies for the Python project separate from dependencies of other projects. This has a few advantages:</p> <ol> <li>It makes dependency management for the project easy.</li> <li>It enables using and testing of different library versions by quickly    spinning up a new environment and verifying the compatibility of the code    with the different version.</li> </ol>"},{"location":"Using-Virtual-Environment/#python-version-requirement-required","title":"Python Version Requirement (Required)","text":"<p>This guide has been tested with Python 3.10.12. Newer versions might not have support for the dependent libraries, so are not recommended.</p>"},{"location":"Using-Virtual-Environment/#use-conda-or-mamba","title":"Use Conda (or Mamba)","text":"<p>While there are many options for setting up virtual environments for python, by far the most</p>"},{"location":"Using-Virtual-Environment/#installing-pip-required","title":"Installing Pip (Required)","text":"<ol> <li>Download the <code>get-pip.py</code> file using the command    <code>curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py</code></li> <li>Run the following <code>python3 get-pip.py</code></li> <li>Check pip version using <code>pip3 -V</code></li> </ol> <p>Note (for Ubuntu users): If the <code>ModuleNotFoundError: No module named 'distutils.util'</code> error is encountered, then python3-distutils needs to be installed. Install python3-distutils using <code>sudo apt-get install python3-distutils</code></p>"},{"location":"Using-Virtual-Environment/#mac-os-x-setup","title":"Mac OS X Setup","text":"<ol> <li>Create a folder where the virtual environments will reside    <code>$ mkdir ~/python-envs</code></li> <li>To create a new environment named <code>sample-env</code> execute    <code>$ python3 -m venv ~/python-envs/sample-env</code></li> <li>To activate the environment execute    <code>$ source ~/python-envs/sample-env/bin/activate</code></li> <li>Upgrade to the latest pip version using <code>$ pip3 install --upgrade pip</code></li> <li>Upgrade to the latest setuptools version using    <code>$ pip3 install --upgrade setuptools</code></li> <li>To deactivate the environment execute <code>$ deactivate</code> (you can reactivate the    environment using the same <code>activate</code> command listed above)</li> </ol>"},{"location":"Using-Virtual-Environment/#ubuntu-setup","title":"Ubuntu Setup","text":"<ol> <li>Install the python3-venv package using <code>$ sudo apt-get install python3-venv</code></li> <li>Follow the steps in the Mac OS X installation.</li> </ol>"},{"location":"Using-Virtual-Environment/#windows-setup","title":"Windows Setup","text":"<ol> <li>Create a folder where the virtual environments will reside <code>md python-envs</code></li> <li>To create a new environment named <code>sample-env</code> execute    <code>python -m venv python-envs\\sample-env</code></li> <li>To activate the environment execute <code>python-envs\\sample-env\\Scripts\\activate</code></li> <li>Upgrade to the latest pip version using <code>pip install --upgrade pip</code></li> <li>To deactivate the environment execute <code>deactivate</code> (you can reactivate the    environment using the same <code>activate</code> command listed above)</li> </ol> <p>Note: - Verify that you are using Python version 3.10.12. Launch a   command prompt using <code>cmd</code> and execute <code>python --version</code> to verify the version. - Python3 installation may require admin privileges on Windows. - This guide is for Windows 10 using a 64-bit architecture only.</p>"},{"location":"Versioning/","title":"ML-Agents Versioning","text":""},{"location":"Versioning/#context","title":"Context","text":"<p>As the ML-Agents project evolves into a more mature product, we want to communicate the process we use to version our packages and the data that flows into, through, and out of them clearly. Our project now has four packages (1 Unity, 3 Python) along with artifacts that are produced as well as consumed.  This document covers the versioning for these packages and artifacts.</p>"},{"location":"Versioning/#github-releases","title":"GitHub Releases","text":"<p>Up until now, all packages were in lockstep in-terms of versioning. As a result, the GitHub releases were tagged with the version of all those packages (e.g. v0.15.0, v0.15.1) and labeled accordingly. With the decoupling of package versions, we now need to revisit our GitHub release tagging. The proposal is that we move towards an integer release numbering for our repo and each such release will call out specific version upgrades of each package. For instance, with the April 30th release, we will have: - GitHub Release 1 (branch name: release_1_branch)   - com.unity.ml-agents release 1.0.0   - ml-agents release 0.16.0   - ml-agents-envs release 0.16.0   - gym-unity release 0.16.0</p> <p>Our release cadence will not be affected by these versioning changes.  We will keep having monthly releases to fix bugs and release new features.</p>"},{"location":"Versioning/#packages","title":"Packages","text":"<p>All of the software packages, and their generated artifacts will be versioned.  Any automation tools will not be versioned.</p>"},{"location":"Versioning/#unity-package","title":"Unity package","text":"<p>Package name: com.unity.ml-agents - Versioned following Semantic Versioning Guidelines - This package consumes an artifact of the training process: the <code>.nn</code> file.  These files     are integer versioned and currently at version 2. The com.unity.ml-agents package     will need to support the version of <code>.nn</code> files which existed at its 1.0.0 release.     For example, consider that com.unity.ml-agents is at version 1.0.0 and the NN files     are at version 2.  If the NN files change to version 3, the next release of     com.unity.ml-agents at version 1.1.0 guarantees it will be able to read both of these     formats.  If the NN files were to change to version 4 and com.unity.ml-agents to     version 2.0.0, support for NN versions 2 and 3 could be dropped for com.unity.ml-agents     version 2.0.0. - This package produces one artifact, the <code>.demo</code> files.  These files will have integer     versioning. This means their version will increment by 1 at each change.  The     com.unity.ml-agents package must be backward compatible with version changes     that occur between minor versions. - To summarize, the artifacts produced and consumed by com.unity.ml-agents are guaranteed     to be supported for 1.x.x versions of com.unity.ml-agents.  We intend to provide stability     for our users by moving to a 1.0.0 release of com.unity.ml-agents.</p>"},{"location":"Versioning/#python-packages","title":"Python Packages","text":"<p>Package names: ml-agents / ml-agents-envs / gym-unity - The python packages remain in \"Beta.\"  This means that breaking changes to the public     API of the python packages can change without having to have a major version bump.     Historically, the python and C# packages were in version lockstep.  This is no longer     the case.  The python packages will remain in lockstep with each other for now, while the     C# package will follow its own versioning as is appropriate.  However, the python package     versions may diverge in the future. - While the python packages will remain in Beta for now, we acknowledge that the most     heavily used portion of our python interface is the <code>mlagents-learn</code> CLI and strive     to make this part of our API backward compatible. We are actively working on this and     expect to have a stable CLI in the next few weeks.</p>"},{"location":"Versioning/#communicator","title":"Communicator","text":"<p>Packages which communicate: com.unity.ml-agents / ml-agents-envs</p> <p>Another entity of the ML-Agents Toolkit that requires versioning is the communication layer between C# and Python, which will follow also semantic versioning.  This guarantees a level of backward compatibility between different versions of C# and Python packages which communicate. Any Communicator version 1.x.x of the Unity package should be compatible with any 1.x.x Communicator Version in Python.</p> <p>An RLCapabilities struct keeps track of which features exist. This struct is passed from C# to Python, and another from Python to C#.  With this feature level granularity, we can notify users more specifically about feature limitations based on what's available in both C# and Python. These notifications will be logged to the python terminal, or to the Unity Editor Console.</p>"},{"location":"Versioning/#side-channels","title":"Side Channels","text":"<p>The communicator is what manages data transfer between Unity and Python for the core training loop. Side Channels are another means of data transfer between Unity and Python. Side Channels are not versioned, but have been designed to support backward compatibility for what they are. As of today, we provide 4 side channels: - FloatProperties: shared float data between Unity - Python (bidirectional) - RawBytes: raw data that can be sent Unity - Python (bidirectional) - EngineConfig: a set of numeric fields in a pre-defined order sent from Python to Unity - Stats: (name, value, agg) messages sent from Unity to Python</p> <p>Aside from the specific implementations of side channels we provide (and use ourselves), the Side Channel interface is made available for users to create their own custom side channels. As such, we guarantee that the built in SideChannel interface between Unity and Python is backward compatible in packages that share the same major version.</p>"},{"location":"com.unity.ml-agents.extensions/","title":"About ML-Agents Extensions package (<code>com.unity.ml-agents.extensions</code>)","text":"<p>The Unity ML-Agents Extensions package contains optional add-ons to the C# SDK for the Unity ML-Agents Toolkit.</p> <p>These extensions are all considered experimental, and their API or behavior may change between versions.</p>"},{"location":"com.unity.ml-agents.extensions/#package-contents","title":"Package contents","text":"<p>The following table describes the package folder structure:</p> Location Description Documentation~ Contains the documentation for the Unity package. Editor Contains utilities for Editor windows and drawers. Runtime Contains core C# APIs for integrating ML-Agents into your Unity scene. Tests Contains the unit tests for the package. <p>The Runtime directory currently contains these features:  * Physics-based sensors  * Input System Package Integration  * Custom Grid-based Sensors</p>"},{"location":"com.unity.ml-agents.extensions/#installation","title":"Installation","text":"<p>The ML-Agents Extensions package is not currently available in the Package Manager. There are two recommended ways to install the package:</p>"},{"location":"com.unity.ml-agents.extensions/#local-installation","title":"Local Installation","text":"<p>Clone the repository and follow the Local Installation for Development directions (substituting <code>com.unity.ml-agents.extensions</code> for the package name).</p>"},{"location":"com.unity.ml-agents.extensions/#github-via-package-manager","title":"Github via Package Manager","text":"<p>In Unity 2019.4 or later, open the Package Manager, hit the \"+\" button, and select \"Add package from git URL\".</p> <p></p> <p>In the dialog that appears, enter  ``` git+https://github.com/Unity-Technologies/ml-agents.git?path=com.unity.ml-agents.extensions#release_21</p> <pre><code>\nYou can also edit your project's `manifest.json` directly and add the following line to the `dependencies`\nsection:\n</code></pre> <p>\"com.unity.ml-agents.extensions\": \"git+https://github.com/Unity-Technologies/ml-agents.git?path=com.unity.ml-agents.extensions#release_21\", ``` See Git dependencies for more information. Note that this may take several minutes to resolve the packages the first time that you add it.</p>"},{"location":"com.unity.ml-agents.extensions/#requirements","title":"Requirements","text":"<p>This version of the Unity ML-Agents package is compatible with the following versions of the Unity Editor:</p> <ul> <li>2019.4 and later</li> </ul> <p>If using the <code>InputActuatorComponent</code> - install the <code>com.unity.inputsystem</code> package version <code>1.1.0-preview.3</code> or later.</p>"},{"location":"com.unity.ml-agents.extensions/#known-limitations","title":"Known Limitations","text":"<ul> <li>For the <code>InputActuatorComponent</code><ul> <li>Limited implementation of <code>InputControls</code></li> <li>No way to customize the action space of the <code>InputActuatorComponent</code></li> </ul> </li> </ul>"},{"location":"com.unity.ml-agents.extensions/#need-help","title":"Need Help?","text":"<p>The main README contains links for contacting the team or getting support.</p>"},{"location":"com.unity.ml-agents/","title":"About ML-Agents package (<code>com.unity.ml-agents</code>)","text":"<p>The ML-Agents package contains the primary C# SDK for the Unity ML-Agents Toolkit.</p> <p>The package allows you to convert any Unity scene into a learning environment and train character behaviors using a variety of machine learning algorithms. Additionally, it allows you to embed these trained behaviors back into Unity scenes to control your characters. More specifically, the package provides the following core functionalities:</p> <ul> <li>Define Agents: entities, or characters, whose behavior will be learned. Agents   are entities that generate observations (through sensors), take actions, and   receive rewards from the environment.</li> <li>Define Behaviors: entities that specify how an agent should act. Multiple   agents can share the same Behavior and a scene may have multiple Behaviors.</li> <li>Record demonstrations of an agent within the Editor. You can use   demonstrations to help train a behavior for that agent.</li> <li>Embedding a trained behavior into the scene via the Unity Inference Engine.   Embedded behaviors allow you to switch an Agent between learning and   inference.</li> </ul> <p>Note that the ML-Agents package does not contain the machine learning algorithms for training behaviors. The ML-Agents package only supports instrumenting a Unity scene, setting it up for training, and then embedding the trained model back into your Unity scene. The machine learning algorithms that orchestrate training are part of the companion Python package.</p> <p>Note that we also provide an ML-Agents Extensions package (<code>com.unity.ml-agents.extensions</code>) that contains early/experimental features that you may find useful. This package is only available from the ML-Agents GitHub repo.</p>"},{"location":"com.unity.ml-agents/#package-contents","title":"Package contents","text":"<p>The following table describes the package folder structure:</p> Location Description Documentation~ Contains the documentation for the Unity package. Editor Contains utilities for Editor windows and drawers. Plugins Contains third-party DLLs. Runtime Contains core C# APIs for integrating ML-Agents into your Unity scene. Runtime/Integrations Contains utilities for integrating ML-Agents into specific game genres. Tests Contains the unit tests for the package. <p></p>"},{"location":"com.unity.ml-agents/#installation","title":"Installation","text":"<p>To install this ML-Agents package, follow the instructions in the Package Manager documentation.</p> <p>To install the companion Python package to enable training behaviors, follow the installation instructions on our GitHub repository.</p>"},{"location":"com.unity.ml-agents/#advanced-installation","title":"Advanced Installation","text":"<p>With the changes to Unity Package Manager in 2021, experimental packages will not show up in the package list and have to be installed manually. There are two recommended ways to install the package manually:</p>"},{"location":"com.unity.ml-agents/#github-via-package-manager","title":"Github via Package Manager","text":"<p>In Unity 2019.4 or later, open the Package Manager, hit the \"+\" button, and select \"Add package from git URL\".</p> <p></p> <p>In the dialog that appears, enter</p> <pre><code>git+https://github.com/Unity-Technologies/ml-agents.git?path=com.unity.ml-agents#release_21\n</code></pre> <p>You can also edit your project's <code>manifest.json</code> directly and add the following line to the <code>dependencies</code> section:</p> <pre><code>\"com.unity.ml-agents\": \"git+https://github.com/Unity-Technologies/ml-agents.git?path=com.unity.ml-agents#release_21\",\n</code></pre> <p>See Git dependencies for more information. Note that this may take several minutes to resolve the packages the first time that you add it.</p>"},{"location":"com.unity.ml-agents/#local-installation-for-development","title":"Local Installation for Development","text":"<p>Clone the repository and follow the Local Installation for Development directions.</p>"},{"location":"com.unity.ml-agents/#requirements","title":"Requirements","text":"<p>This version of the Unity ML-Agents package is compatible with the following versions of the Unity Editor:</p> <ul> <li>2019.4 and later</li> </ul>"},{"location":"com.unity.ml-agents/#known-limitations","title":"Known Limitations","text":""},{"location":"com.unity.ml-agents/#training","title":"Training","text":"<p>Training is limited to the Unity Editor and Standalone builds on Windows, MacOS, and Linux with the Mono scripting backend. Currently, training does not work with the IL2CPP scripting backend. Your environment will default to inference mode if training is not supported or is not currently running.</p>"},{"location":"com.unity.ml-agents/#inference","title":"Inference","text":"<p>Inference is executed via the Unity Inference Engine.</p> <p>CPU</p> <p>All platforms supported.</p> <p>GPU</p> <p>All platforms supported except:</p> <ul> <li>WebGL and GLES 3/2 on Android / iPhone</li> </ul> <p>NOTE: Mobile platform support includes:</p> <ul> <li>Vulkan for Android</li> <li>Metal for iOS.</li> </ul>"},{"location":"com.unity.ml-agents/#headless-mode","title":"Headless Mode","text":"<p>If you enable Headless mode, you will not be able to collect visual observations from your agents.</p>"},{"location":"com.unity.ml-agents/#rendering-speed-and-synchronization","title":"Rendering Speed and Synchronization","text":"<p>Currently the speed of the game physics can only be increased to 100x real-time. The Academy also moves in time with FixedUpdate() rather than Update(), so game behavior implemented in Update() may be out of sync with the agent decision making. See Execution Order of Event Functions for more information.</p> <p>You can control the frequency of Academy stepping by calling <code>Academy.Instance.DisableAutomaticStepping()</code>, and then calling <code>Academy.Instance.EnvironmentStep()</code></p>"},{"location":"com.unity.ml-agents/#unity-inference-engine-models","title":"Unity Inference Engine Models","text":"<p>Currently, only models created with our trainers are supported for running ML-Agents with a neural network behavior.</p>"},{"location":"com.unity.ml-agents/#helpful-links","title":"Helpful links","text":"<p>If you are new to the Unity ML-Agents package, or have a question after reading the documentation, you can checkout our GitHub Repository, which also includes a number of ways to connect with us including our ML-Agents Forum.</p> <p>In order to improve the developer experience for Unity ML-Agents Toolkit, we have added in-editor analytics. Please refer to \"Information that is passively collected by Unity\" in the Unity Privacy Policy.</p>"},{"location":"ml-agents-envs/","title":"Unity ML-Agents Python Interface","text":"<p>The <code>mlagents_envs</code> Python package is part of the ML-Agents Toolkit. <code>mlagents_envs</code> provides three Python APIs that allows direct interaction with the Unity game engine: - A single agent API (Gym API) - A gym-like multi-agent API (PettingZoo API) - A low-level API (LLAPI)</p> <p>The LLAPI is used by the trainer implementation in <code>mlagents</code>. <code>mlagents_envs</code> can be used independently of <code>mlagents</code> for Python communication.</p>"},{"location":"ml-agents-envs/#installation","title":"Installation","text":"<p>Install the <code>mlagents_envs</code> package with:</p> <pre><code>python -m pip install mlagents_envs==1.0.0\n</code></pre>"},{"location":"ml-agents-envs/#usage-more-information","title":"Usage &amp; More Information","text":"<p>See - Gym API Guide - PettingZoo API Guide - Python API Guide</p> <p>for more information on how to use the API to interact with a Unity environment.</p> <p>For more information on the ML-Agents Toolkit and how to instrument a Unity scene with the ML-Agents SDK, check out the main ML-Agents Toolkit documentation.</p>"},{"location":"ml-agents-envs/#limitations","title":"Limitations","text":"<ul> <li><code>mlagents_envs</code> uses localhost ports to exchange data between Unity and   Python. As such, multiple instances can have their ports collide, leading to   errors. Make sure to use a different port if you are using multiple instances   of <code>UnityEnvironment</code>.</li> <li>Communication between Unity and the Python <code>UnityEnvironment</code> is not secure.</li> <li>On Linux, ports are not released immediately after the communication closes.   As such, you cannot reuse ports right after closing a <code>UnityEnvironment</code>.</li> </ul>"},{"location":"ml-agents/","title":"Unity ML-Agents Trainers","text":"<p>The <code>mlagents</code> Python package is part of the ML-Agents Toolkit. <code>mlagents</code> provides a set of reinforcement and imitation learning algorithms designed to be used with Unity environments. The algorithms interface with the Python API provided by the <code>mlagents_envs</code> package. See here for more information on <code>mlagents_envs</code>.</p> <p>The algorithms can be accessed using the: <code>mlagents-learn</code> access point. See here for more information on using this package.</p>"},{"location":"ml-agents/#installation","title":"Installation","text":"<p>Install the <code>mlagents</code> package with:</p> <pre><code>python -m pip install mlagents==1.0.0\n</code></pre>"},{"location":"ml-agents/#usage-more-information","title":"Usage &amp; More Information","text":"<p>For more information on the ML-Agents Toolkit and how to instrument a Unity scene with the ML-Agents SDK, check out the main ML-Agents Toolkit documentation.</p>"},{"location":"ml-agents/#limitations","title":"Limitations","text":"<ul> <li>Resuming self-play from a checkpoint resets the reported ELO to the default   value.</li> </ul>"},{"location":"doxygen/Readme/","title":"Doxygen files","text":"<p>To generate the API reference as HTML files, run:</p> <pre><code>doxygen dox-ml-agents.conf\n</code></pre>"}]}